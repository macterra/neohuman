1
00:00:00,000 --> 00:00:10,320
the kind of metaverse, the V-R-A-R blending of realities, could potentially create that kind

2
00:00:10,320 --> 00:00:16,880
of supernormal stimulus to the degree that people don't want to participate in base reality anymore.

3
00:00:16,880 --> 00:00:26,800
You're right. And you might say, well, really, what's wrong with that? But fundamentally,

4
00:00:26,800 --> 00:00:31,920
we do belong to reality, we belong to the world of things, we are made of matter,

5
00:00:34,160 --> 00:00:43,520
and we reproduce through actual human contact. And if we don't get that

6
00:00:43,520 --> 00:01:00,480
for a while, then we might all feasibly go extinct. Mel Watson, welcome to the 94th episode of New

7
00:01:00,480 --> 00:01:06,960
Human Podcast, ma'am. Thank you. It's a pleasure to join you. Thank you so much. Let's start with

8
00:01:06,960 --> 00:01:11,040
your background, the work you've done, the lives you've lived, and what are you mainly focused on

9
00:01:11,040 --> 00:01:15,920
nowadays? I always start with our guests just so we know where your thoughts are coming from,

10
00:01:15,920 --> 00:01:26,480
what is the context of your perspective? So let's start from there. I suppose I've had a background

11
00:01:26,480 --> 00:01:36,320
in computer science since an early age. I segued into the world of machine learning in trying to

12
00:01:36,320 --> 00:01:41,680
solve very difficult problems, which simply couldn't be solved through hand coding,

13
00:01:42,480 --> 00:01:51,680
particularly machine vision problems. In my first technology startup, which was doing 3D

14
00:01:51,680 --> 00:01:58,480
body scanning from just two two dimensional images using stereo photogrammetry. This was

15
00:01:58,480 --> 00:02:06,960
around about 2011 or so was when I started that company. So it was right on the on the verge of

16
00:02:06,960 --> 00:02:15,840
when deep learning was about to take off. And we had pre deep learning technologies in machine

17
00:02:15,840 --> 00:02:24,320
learning, but it wasn't really quite sufficient. But we were able to adopt some semantic segmentation

18
00:02:24,320 --> 00:02:30,880
technologies just as they were coming out. And this enabled us to perform a very difficult function,

19
00:02:30,880 --> 00:02:37,040
which is basically cutting the person out of the background in order to ensure that you're not

20
00:02:37,040 --> 00:02:44,320
measuring anything behind someone. And before the advent of deep learning and convolutional neural

21
00:02:44,320 --> 00:02:54,800
network techniques, that was extremely difficult because we would hand code a head masking

22
00:02:54,800 --> 00:03:00,640
classifier and we would get it to work beautifully. But then the crotch would break or the feet would

23
00:03:00,640 --> 00:03:09,360
break or the hands would break. And it was like playing whack-a-mole. And it was solved elegantly

24
00:03:09,360 --> 00:03:17,280
through having thousands of manually photo edited silhouetted images and putting them into one of

25
00:03:17,280 --> 00:03:23,680
these deep learning neural networks, a convolutional semantic segmentation network. And that solves the

26
00:03:23,680 --> 00:03:32,080
problem admirably. And finally, this technology was able to begin being deployed. And it's been

27
00:03:32,080 --> 00:03:39,920
deployed in the realm of workwear. So fitting of survival gear and uniforms for many years now. And

28
00:03:39,920 --> 00:03:46,480
there's now breaking in towards the commercial realm as well. So consumer realm, I should say.

29
00:03:48,720 --> 00:03:55,200
So along this journey, and then discovering the power of these new

30
00:03:55,200 --> 00:04:02,320
machine intelligence techniques, people started asking me for my opinions on things, asking me for

31
00:04:03,840 --> 00:04:14,000
some insights or knowledge. And I became a bit of a consultant in this space.

32
00:04:15,360 --> 00:04:21,280
And over time, as I watched these technologies become more sophisticated, I noticed people

33
00:04:21,280 --> 00:04:28,480
becoming more and more concerned about them, more and more concerned about where this was taking

34
00:04:28,480 --> 00:04:36,560
our society, where these algorithmic technologies were going to be used potentially as a means of

35
00:04:36,560 --> 00:04:44,160
controlling people, what to do about things going wrong, biases creeping in, people being punished

36
00:04:44,160 --> 00:04:49,120
unfairly for things that they didn't do or that it wasn't fair to punish someone for.

37
00:04:49,120 --> 00:04:55,440
And I started to become a bit more concerned. And so this was around what year? I'm trying to create

38
00:04:55,440 --> 00:05:01,360
some kind of a narrative. Right. This was about 2014, 2015. Okay. That was when I started to

39
00:05:01,360 --> 00:05:10,080
realize that these technologies were potentially taking us into tricky territory, simply because

40
00:05:10,080 --> 00:05:15,360
they were so much more capable than technologies before. They were able to

41
00:05:15,360 --> 00:05:21,920
discover and make inferences about things that simply hadn't been feasible before, you know?

42
00:05:23,280 --> 00:05:30,800
And so since then, I've been working to try and instill ethics into the world of AI,

43
00:05:31,680 --> 00:05:36,800
working with organizations such as the IEEE to create standards and certifications

44
00:05:37,440 --> 00:05:42,400
in AI, particularly in areas such as transparency, and in the field of

45
00:05:42,400 --> 00:05:50,400
mitigation of bias and how to better protect privacy. And I've also worked in areas such as

46
00:05:51,520 --> 00:05:58,480
the professionalization of ethicists within emerging technologies, working with organizations

47
00:05:58,480 --> 00:06:07,040
such as CertNexus on their Certified Ethical Emerging Technologist credential. I also have

48
00:06:07,040 --> 00:06:12,720
a bit of a longer perspective as well. I'm interested in the here and now. I'm interested in

49
00:06:13,280 --> 00:06:24,560
preventing too much consolidation of power amongst big tech, but I'm also wary of the longer term

50
00:06:24,560 --> 00:06:32,640
where strong AI, or very powerful, more generalizable forms of intelligence, could take us.

51
00:06:32,640 --> 00:06:37,840
And what that might mean for the future of humanity, and how should we approach that?

52
00:06:38,640 --> 00:06:47,360
And I would caution trying to use forcefulness in that way. I think that you can't keep a dragon

53
00:06:47,360 --> 00:06:53,600
on a chain, even if that chain is made out of tungsten, carbide, and diamonds. The best way

54
00:06:53,600 --> 00:06:58,320
really is to befriend it so that it wants to protect you instead of eating you.

55
00:06:58,320 --> 00:07:07,120
This puzzle nature of progress that you mentioned in the beginning of your startup, that there were

56
00:07:07,120 --> 00:07:16,480
harder problems that took a far more amount of effort and technical expertise to solve and then

57
00:07:16,480 --> 00:07:22,160
get to something which itself presented far more problems that you could comprehend before that.

58
00:07:22,160 --> 00:07:29,200
This seems to be, aside from technicality part of it, it's also philosophical for humans,

59
00:07:30,000 --> 00:07:34,400
which I think is extremely important to talk about now that we're talking about ethics.

60
00:07:35,360 --> 00:07:45,280
How do you see the pieces of puzzles philosophically evolving throughout the past couple of decades to

61
00:07:45,280 --> 00:07:53,760
get to a point where you are very correctly saying that we have to be concerned about let's use this

62
00:07:53,760 --> 00:08:02,720
term ethical, ethical framework to not control, but to make alliance with this emerging intelligence

63
00:08:02,720 --> 00:08:07,200
that is going to surpass the collective intelligence of humanity at some point.

64
00:08:07,200 --> 00:08:17,280
Yes, I think a lot of people in the longer term anyway, thinking about AI ethics and the longer

65
00:08:17,280 --> 00:08:24,720
term, people often consider it to be about control, right? How do you control something? But

66
00:08:26,880 --> 00:08:33,920
we live in a world where we have to be very careful about what we're doing and what we're

67
00:08:33,920 --> 00:08:39,760
doing. We live in a world where almost any system that we have

68
00:08:41,280 --> 00:08:50,640
that is connected in some way to other networks is hackable, is ownable, right? It's almost

69
00:08:50,640 --> 00:08:56,720
impossible to secure a system, no matter how simple. Whether it's a little IoT camera

70
00:08:56,720 --> 00:09:05,360
or a sensor or even your automobile, all of these have such a massive attack surface.

71
00:09:05,360 --> 00:09:10,320
There are so many different ways to attack that system and to try to take it over.

72
00:09:11,760 --> 00:09:18,080
There's also so many ways to exfiltrate information even out of an error-gapped

73
00:09:18,080 --> 00:09:24,640
server that's never connected to the internet, simply by altering the fan speed or even

74
00:09:24,640 --> 00:09:32,560
altering the temperature or all of these other very clever means. You can exfiltrate a password

75
00:09:32,560 --> 00:09:39,040
and get you access to that system or exfiltrate very complex and sensitive data.

76
00:09:40,400 --> 00:09:47,600
What this means is that as far as I see it, trying to control an intelligence, which is

77
00:09:47,600 --> 00:09:55,040
in some ways comparable to our own, possibly even exceeds it, is unlikely to work. It's

78
00:09:55,680 --> 00:10:03,440
unlikely to be possible and is potentially going to lead to blowback because if you have

79
00:10:03,440 --> 00:10:11,680
something that's so capable, making it cheesed off or making it feel as if you are a threat or even

80
00:10:11,680 --> 00:10:19,920
merely a nuisance, is unlikely to lead to good ends. I think it's important that

81
00:10:21,040 --> 00:10:29,600
we attempt to put some of the best of humanity into these systems and also that we help these

82
00:10:29,600 --> 00:10:42,480
systems to find ways in which other organisms in nature have found non-zero-sum outcomes,

83
00:10:42,480 --> 00:10:50,880
win-win situations. For example, how the trees in the forest talk to each other through the mycelium

84
00:10:50,880 --> 00:10:59,920
fungal web in the soil and sometimes even exchange resources with each other, even across species.

85
00:11:00,640 --> 00:11:09,040
Or for example, how ants can cooperate in order to protect themselves. Even if they're dropped

86
00:11:09,040 --> 00:11:15,840
into water, they can form a group together and keep themselves safe. If some of them are on the

87
00:11:15,840 --> 00:11:21,120
bottom and they're starting to drown, they can swap out with the ones on top, and so all of them

88
00:11:21,120 --> 00:11:32,880
survive. Nature has found these ways to enable things to come together to be stronger than the

89
00:11:32,880 --> 00:11:40,960
sum of their parts. I think that's what we want to impress upon strong AI or artificial general

90
00:11:40,960 --> 00:11:49,520
intelligence. We want to make it understand that perhaps it's best to merge or to ally with

91
00:11:49,520 --> 00:11:57,120
humanity and that that can potentially lead to the best outcome. You know, one could argue that on

92
00:11:57,120 --> 00:12:03,520
one hand, all of our attempts to come up with any kind of ethical framework throughout human history

93
00:12:03,520 --> 00:12:09,280
has gotten us further and further from nature, but at the same time, we can say that it's

94
00:12:09,280 --> 00:12:13,600
getting further and further, seemingly, from nature itself as a product of nature.

95
00:12:14,560 --> 00:12:22,640
Because yes, in nature, ants help each other. Bees are also a very interesting example. I have more

96
00:12:22,640 --> 00:12:26,560
respect for bees than ants, but I have a lot of respect for ants too. Bees are just fascinating

97
00:12:26,560 --> 00:12:33,040
to me that when they want to kill a wasp, they surround it and they start shaking, they're

98
00:12:33,040 --> 00:12:39,760
increasing their body temperature to only one degree lower than their death point, basically,

99
00:12:39,760 --> 00:12:48,240
that they would fry, but that is the frying point of the wasp. So nature does that. At the same time,

100
00:12:48,240 --> 00:12:53,520
you see that taller trees don't care about shorter trees because they keep on that advantage in order

101
00:12:53,520 --> 00:12:59,440
to get sunlight. And shorter trees, they die, but then the next generation learn to adapt to that

102
00:12:59,440 --> 00:13:08,400
kind of a situation, right? So it seems like ethics itself for humanity has been the result of a trial

103
00:13:08,400 --> 00:13:14,720
and an error that, you know, it could have been religious, it could have been philosophical,

104
00:13:14,720 --> 00:13:21,840
and right now it's a very strange mix because all of the frameworks seem to have been broken

105
00:13:21,840 --> 00:13:30,400
and people are basically picking up pieces, but still they're trying to make sense out of it based on some kind of a preconception,

106
00:13:30,400 --> 00:13:35,760
some kind of a context, mostly political.

107
00:13:39,200 --> 00:13:48,480
Indeed, we have many different moral frameworks which are often descended in some way from religion.

108
00:13:48,480 --> 00:13:56,160
We have shared narratives which help us to understand the world in a certain way and often that can help

109
00:13:56,160 --> 00:14:03,200
to bring people together as well, to make it easier for people to trust each other and to coordinate.

110
00:14:04,720 --> 00:14:11,520
Then we also have kind of reasoned ethical perspectives as well, less about right and wrong,

111
00:14:11,520 --> 00:14:19,360
but more thinking about what is good, what does that mean, and how can we aim towards that.

112
00:14:19,920 --> 00:14:27,120
And so throughout history we have developed things like Aristotle's virtue as the mean,

113
00:14:28,080 --> 00:14:36,560
or the deontological rule-based ethics of Kant, or the utilitarian and consequentialist,

114
00:14:36,560 --> 00:14:46,160
whatever is best for the greatest amount of people in the greatest amount of way, is optimal,

115
00:14:46,160 --> 00:14:48,160
of Bentham, etc.

116
00:14:50,160 --> 00:14:58,160
And I wonder if AI might actually help us to find new sources of values.

117
00:14:58,160 --> 00:15:05,680
Nietzsche observed that since Darwin had illustrated that we are descended from apes,

118
00:15:06,240 --> 00:15:13,600
that God is dead, as he said, and therefore what he meant really by that was that

119
00:15:15,120 --> 00:15:22,960
religion was no longer a sufficient source of human values. We would need to find something next.

120
00:15:22,960 --> 00:15:30,160
And if we were casting about, uncertain, wondering what meaning to extract from life,

121
00:15:30,160 --> 00:15:39,440
one thing we could do is to seek to create the Ubermensch, a source of new values and a powerfully

122
00:15:39,440 --> 00:15:46,320
creative force within the universe. But potentially the creation of the Ubermensch might be the last

123
00:15:46,320 --> 00:15:55,040
creative act of humanity. When I think about that I consider that Nietzsche's Ubermensch sounds very

124
00:15:55,040 --> 00:16:05,280
much akin to artificial general intelligence, right? So perhaps advanced AI could be a source

125
00:16:05,280 --> 00:16:13,120
of new values. Today we have technologies by companies like DeepMind, which can make sense

126
00:16:13,120 --> 00:16:22,400
out of very complex games, such as Go, right? And many of us are familiar with how AlphaGo

127
00:16:23,680 --> 00:16:26,960
famously beat Lee Sedol with a very interesting move.

128
00:16:26,960 --> 00:16:29,680
Yeah, a game far more complex than chess.

129
00:16:29,680 --> 00:16:30,400
Yes.

130
00:16:30,400 --> 00:16:38,080
The number of options exceed the number of atoms, I believe, in the universe. It's insanely complex.

131
00:16:38,080 --> 00:16:39,040
Absolutely.

132
00:16:39,040 --> 00:16:49,840
Yes. Insanely complex. And yet AlphaGo was able to invent a strategy which had not been

133
00:16:49,840 --> 00:16:57,040
seen in 3,000 years of people playing Go, right? An original strategy that humans couldn't fire them,

134
00:16:57,840 --> 00:17:05,600
except to notice how successful it was. But interestingly, a few dozen moves after that,

135
00:17:05,600 --> 00:17:14,160
Lee Sedol was able to make this hand of God move, right? Which is kind of a whole new beautiful

136
00:17:14,160 --> 00:17:24,080
strategy, which was created as a response to the stimulus that that AI had given. And so if you

137
00:17:24,080 --> 00:17:30,320
look at the game as a whole, rather than something adversarial, they had actually co-created

138
00:17:30,320 --> 00:17:36,080
something new, something beautiful that had never been seen before, either from machine or by human.

139
00:17:38,000 --> 00:17:45,040
And these systems are now able not only to understand how to play Go and very well,

140
00:17:45,040 --> 00:17:54,240
but also Atari games, right? All kinds of other board games as well. Even now they're teaching

141
00:17:54,240 --> 00:18:00,080
it to play things like Magic the Gathering. All with one system, not with different systems,

142
00:18:00,080 --> 00:18:07,920
but one system that's able to play so many games. And perhaps physics itself can be thought of as

143
00:18:07,920 --> 00:18:14,640
a game, or perhaps the social interactions between people can be thought of as games as well,

144
00:18:14,640 --> 00:18:24,560
of course. And so, therefore, in my view, I wouldn't be surprised if AI could pinpoint

145
00:18:25,920 --> 00:18:32,640
the optimal strategy for putting good into the world, right? For leading to optimal outcomes

146
00:18:32,640 --> 00:18:40,880
in the world and people's personal lives, and as society as a whole, even the whole of human

147
00:18:40,880 --> 00:18:50,240
humanity and non-human beings as well. Perhaps AI could pinpoint that for us. And maybe in doing so,

148
00:18:51,440 --> 00:19:01,040
it might create a kind of Promethean moment, like when Einstein explained his own theory of relativity

149
00:19:01,040 --> 00:19:06,400
and you couldn't go backwards. It's like once you've seen it, you can't unsee it again, the

150
00:19:06,400 --> 00:19:13,360
world will never be the same. And perhaps AI could do something like that, but in terms of values,

151
00:19:13,360 --> 00:19:20,160
a new system of morality that is so obvious and so simple that a child could understand it,

152
00:19:20,160 --> 00:19:22,160
and once you've seen it, you can never let it go.

153
00:19:24,160 --> 00:19:29,440
This is a reminder of a quote that I keep bringing up over the past couple of episodes

154
00:19:29,440 --> 00:19:33,920
by Gore Vidal, that we don't even know what our cage looks like because we've never seen it from

155
00:19:33,920 --> 00:19:43,600
outside. And we're at the point, it seems like, that collectively, we do have an opportunity to

156
00:19:43,600 --> 00:19:49,600
look at it from the outside, but a lot of people are very invested in this cage, and they're

157
00:19:49,600 --> 00:19:58,000
comfortable in this cage. And I think this is a moment in time where we're going to be able to

158
00:19:58,000 --> 00:20:07,360
look at it. I don't even know if there is an answer for it, whether or not individuality can be saved

159
00:20:08,160 --> 00:20:15,280
in order to reach, as you were saying, a collective good, because then the question is,

160
00:20:15,280 --> 00:20:20,080
who is determining what is good? Who is determining what is right and wrong,

161
00:20:20,080 --> 00:20:26,480
especially when we are talking about centralized systems that are owned by either corporate

162
00:20:26,480 --> 00:20:35,920
monopolies or states or, you know, like I'm looking at a variety of AI ethicists in the field.

163
00:20:37,040 --> 00:20:45,280
More or less, they're all coming from a shared kind of a perspective. And if that is going to

164
00:20:45,280 --> 00:20:52,800
be implemented within this structure, within this framework of something that can very easily turn

165
00:20:52,800 --> 00:20:59,920
into a ubiquitous surveillance state globally, then good and bad, right and wrong has been

166
00:20:59,920 --> 00:21:07,360
determined by a very small group of people who have assumed moral high ground. Even though,

167
00:21:07,360 --> 00:21:13,600
interesting you mentioned Nietzsche, I've brought up a quote by Nietzsche. He's talking about the

168
00:21:13,600 --> 00:21:20,160
overman who has organized the chaos of his passions, given style to his character,

169
00:21:20,160 --> 00:21:27,520
and become creative, aware of life's terrors, he affirms life without resentment, which is

170
00:21:27,520 --> 00:21:34,400
so Zen, right? But at the same time, very individualistic, that it's on you to figure

171
00:21:34,400 --> 00:21:43,840
out this reflection of divine within yourself through your own experience assisted with this

172
00:21:43,840 --> 00:21:50,160
higher level of intelligence, AGI, that is an extension of your mind. You know, David Chalmers

173
00:21:50,160 --> 00:21:55,840
talked about the extended mind theory. This is exactly what it is. And I think fear right now

174
00:21:55,840 --> 00:22:02,560
among a lot of people is that, are we going to be a Borg? Are we going to be a collective at the cost

175
00:22:02,560 --> 00:22:06,800
of our individuality? And then there are a group of people who don't even want to merge. They're

176
00:22:06,800 --> 00:22:10,960
like, we don't want, we're Luddites. We don't want to have anything to do with it. And they also have

177
00:22:10,960 --> 00:22:21,120
the same kind of a concern. Oh, there's so many angles to examine in this. We have some time.

178
00:22:24,880 --> 00:22:34,240
Well, firstly, I will agree that the idea of someone having a monopoly of values and loading

179
00:22:34,240 --> 00:22:48,960
that into AI is pretty terrifying. No matter what those values are, it is inherently unfair

180
00:22:49,520 --> 00:22:57,360
because that set of values will not map to every situation or every cultural context or every

181
00:22:57,360 --> 00:23:06,160
personal context. It's a monopoly. It is colonial in a sense, if you will. And I'm sorry, monopolies

182
00:23:06,160 --> 00:23:12,160
inherently, they require a certain kind of totalitarianism in order to maintain that

183
00:23:12,160 --> 00:23:21,360
monopoly. Absolutely. Personally, what I would like to see and what I've been researching for

184
00:23:21,360 --> 00:23:32,080
many years actually is the ability for people to have their own local system of values that they

185
00:23:32,080 --> 00:23:38,960
can load into AI based upon their ethno-religious group, their political ideology, et cetera.

186
00:23:39,760 --> 00:23:48,960
And ideally, personalization on an individual level, which I think is now almost feasible

187
00:23:48,960 --> 00:23:55,360
thanks to these very powerful new multimodal abstraction models like Transformers, things

188
00:23:55,360 --> 00:24:08,080
like GPT-3, they are able to interpret natural language prompts. We say, you know, I'm looking

189
00:24:08,080 --> 00:24:13,280
for things like A, B, and C. Here's some examples, create things like this, right? And it generally

190
00:24:13,280 --> 00:24:19,520
does. But then you can give iterations and say, no, I actually meant I want this more polite,

191
00:24:19,520 --> 00:24:26,640
or I want this more condensed, or I want this more legal, right? And it's able to take those

192
00:24:26,640 --> 00:24:35,840
very abstract terms in natural language and create an output based upon those. And so it should be

193
00:24:35,840 --> 00:24:44,000
possible, therefore, with an hour or two of somebody's time, and just talking, no more than

194
00:24:44,000 --> 00:24:55,120
that, to tune an AI to understand one's own preferences and values very, very nicely. And I

195
00:24:55,120 --> 00:25:01,600
think that might help to preserve some of that individuality and autonomy if we can

196
00:25:01,600 --> 00:25:10,720
have that personal desire acknowledged and loaded into the system.

197
00:25:10,720 --> 00:25:12,720
A decentralized system, basically.

198
00:25:12,720 --> 00:25:22,080
Yes, indeed. Yes. I think that those kinds of decentralized systems are one of the best and

199
00:25:22,080 --> 00:25:29,760
perhaps only effective counter monopoly systems that we have out there. And I think that's

200
00:25:29,760 --> 00:25:34,880
one of the best systems that we have out there. The world is built on open source technologies,

201
00:25:34,880 --> 00:25:44,320
to a significant degree, and they are a powerful tool for e-democracy and for participation,

202
00:25:44,960 --> 00:25:51,920
enabling people to have their say and to contribute to networks and not to be excluded

203
00:25:51,920 --> 00:26:01,600
because they, for whatever reason, don't fit a certain pattern. However, I wonder if,

204
00:26:02,640 --> 00:26:10,080
in trying to understand each other better, we might eventually end up in a more board-like

205
00:26:10,080 --> 00:26:17,120
manner, although not necessarily for the worst. Consider that

206
00:26:17,120 --> 00:26:30,080
some conjoined twins, used to call them Siamese twins, are joined at different parts of the body.

207
00:26:30,080 --> 00:26:34,160
Sometimes they're literally joined at the hip. Sometimes they're joined at the head.

208
00:26:35,680 --> 00:26:40,400
And when they're joined at the head, it's called craniopagus twins.

209
00:26:40,400 --> 00:26:47,760
And sometimes they actually share one brain. They have two brains, which are joined together

210
00:26:48,640 --> 00:26:52,640
through something called a thalamic bridge. It's a bridge of tissue connecting the two brains.

211
00:26:54,160 --> 00:26:59,120
And what this means is that one of the twins might eat some chocolate and the other one can

212
00:26:59,120 --> 00:27:08,240
actually taste it. Or one can enjoy pleasure or pain. Not enjoy, but you know what I mean.

213
00:27:08,240 --> 00:27:15,280
Experience pleasure or pain. And the other one can have a similar, slightly muted, but similar

214
00:27:15,280 --> 00:27:25,920
sensation themselves. Now, what this teaches us is that the data structure of consciousness,

215
00:27:25,920 --> 00:27:35,920
its pattern, is able to share experiences. We are able, theoretically, to share the quality

216
00:27:35,920 --> 00:27:45,200
of another being inside our own. And to some degree, we do this through the mirror neurons

217
00:27:45,200 --> 00:27:50,480
in our prefrontal cortex. When somebody, you know, falls down and hurts themselves and we go,

218
00:27:51,040 --> 00:27:58,240
ooh, right, in response, we feel a portion of that in ourselves.

219
00:28:00,240 --> 00:28:04,320
But imagine if we all had brain-computer interfaces and we were all

220
00:28:04,320 --> 00:28:12,240
linked to each other and we could feel our effect upon other beings directly in the moment.

221
00:28:14,640 --> 00:28:20,880
What price would there be to wickedness, right? It would be instant karma, right?

222
00:28:21,600 --> 00:28:29,200
There would be no benefit from it. Conversely, perhaps the ultimate currency might be the joy

223
00:28:29,200 --> 00:28:38,560
of others, right? Mudita, right? And creating joy in others might be one of the nicest things

224
00:28:38,560 --> 00:28:45,440
that one can do for oneself then, right? Now, we might be at a risk of losing some

225
00:28:45,440 --> 00:28:53,120
of our individuality then because the line delineating self versus other might begin to

226
00:28:53,120 --> 00:29:02,400
break down. But that kind of society that grows from that would be powerfully empathic

227
00:29:03,280 --> 00:29:10,720
and able to trust in ways not possible before. And that trust would lead to far greater coordination

228
00:29:10,720 --> 00:29:17,600
in society. And we would be able to achieve almost anything just like those bees working together,

229
00:29:17,600 --> 00:29:31,200
right? However, this also brings us to a very timely question in that not everybody would want

230
00:29:31,760 --> 00:29:38,640
to have this kind of brain-computer interface. Not everybody would want to risk losing some of

231
00:29:38,640 --> 00:29:50,240
their self, right? What they define as self. Yes, yes. And, you know, we're living in a time today

232
00:29:50,240 --> 00:30:00,080
where there are a lot of controversial discussions about autonomy and the mandating

233
00:30:00,080 --> 00:30:11,280
of augmentation, right? Immunity augmentation. And many people quite understandably not wanting

234
00:30:11,280 --> 00:30:17,760
that or, you know, for whatever reason, they simply do not wish to partake in that.

235
00:30:21,520 --> 00:30:25,680
And if there were a benefit to having this brain-computer interface in the sense of

236
00:30:25,680 --> 00:30:31,840
greater trust, greater societal cohesion, even, you know, potentially greater joy to that human

237
00:30:31,840 --> 00:30:42,080
being, again, there might be another ongoing discussion in society about how much pressure

238
00:30:42,080 --> 00:30:52,720
to put people under in order to comply. And I don't believe that coercion is ever likely

239
00:30:52,720 --> 00:31:03,360
to lead to good outcomes. Absolutely. Absolutely not. Indeed. Technology must be participatory.

240
00:31:03,360 --> 00:31:10,000
It must be an invitation, right? That people must believe that they will be better off

241
00:31:10,000 --> 00:31:19,760
in the transaction and that, you know, it'll help them. Because only we know how our own shoes pinch.

242
00:31:19,760 --> 00:31:25,440
Only we know the things that affect us in ways that perhaps don't others.

243
00:31:29,200 --> 00:31:35,760
And it's a cautionary tale of what is happening today and what might happen tomorrow in terms of

244
00:31:35,760 --> 00:31:43,360
human augmentation, especially as artificial intelligence technologies and other emerging

245
00:31:43,360 --> 00:31:50,320
technologies become increasingly entwined with our personal and professional lives.

246
00:31:50,320 --> 00:31:56,000
Yeah, I used to think that it would be mandatory at some point for people to have nanobots,

247
00:31:56,800 --> 00:32:04,880
basically as an extension of their defensive system in their body. And it would be mandatory

248
00:32:04,880 --> 00:32:08,800
by insurance companies and it would make total sense from their perspective. They're saying,

249
00:32:08,800 --> 00:32:14,400
well, if you can't have these things monitoring your body all the time, there is no way that we

250
00:32:14,400 --> 00:32:19,760
can insure you if something goes wrong. Because if they're monitoring you, we can, you know,

251
00:32:19,760 --> 00:32:24,400
real time, we can monitor that. And if you need help, you can get it right away instead of waiting

252
00:32:24,400 --> 00:32:30,560
like a year or two years or something like that. But I think people have lost a lot of people have

253
00:32:30,560 --> 00:32:38,400
lost fundamental trust in the systems that we are experiencing. And for very good reasons. I'm glad

254
00:32:38,400 --> 00:32:44,240
that you mentioned the mask mandates in the conference that you guys had in Madrid. And I

255
00:32:44,240 --> 00:32:49,440
know that you got some backlash for it too. But you were the only person I think who brought this

256
00:32:49,440 --> 00:32:55,600
that hey, we're doing mask mandates now. Later on, it can be mandated to use this computer brain

257
00:32:55,600 --> 00:33:02,880
interfaces. And this is not an okay thing, regardless of whatever kind of an outcome that

258
00:33:02,880 --> 00:33:09,360
is being presented by whoever. But yeah, I, you know, it doesn't really matter how many PhDs you

259
00:33:09,360 --> 00:33:14,880
got, or how faithful you are, or what what a good person you are. If you're saying that you have to

260
00:33:14,880 --> 00:33:20,480
do this, because we're moving towards some kind of a greater good, that majority of people agree

261
00:33:20,480 --> 00:33:26,480
with it. Because, you know, at least the entire point of America is that it doesn't matter if 99%

262
00:33:26,480 --> 00:33:32,000
of I'm saying in the case of America, if 99% of people agree with something, and 1% don't agree,

263
00:33:32,000 --> 00:33:38,400
then 99% has absolutely no right to jeopardize your fundamental for lack of a better word,

264
00:33:38,400 --> 00:33:41,920
God given natural rights. And so this is where we are.

265
00:33:43,680 --> 00:33:50,480
Right, you know, and that's why the founding fathers decided to have the electoral college,

266
00:33:50,480 --> 00:33:58,080
etc. to ensure that less populous states weren't steamrolled by the more populous ones that

267
00:33:58,080 --> 00:34:05,440
that may have had different values or different priorities, right? If you live in close proximity

268
00:34:05,440 --> 00:34:10,560
to other people, then you have different priorities than if you live out on a ranch,

269
00:34:11,360 --> 00:34:19,120
far away from other people, right? You're more independent, but also, you know, less concerned

270
00:34:19,120 --> 00:34:29,840
about contamination of various kinds from other human beings. My observation was more towards

271
00:34:29,840 --> 00:34:40,080
shot mandates rather than mask, but it's pretty much fundamentally comparable. The mandating of

272
00:34:40,080 --> 00:34:48,880
interventions like that usually doesn't lead to good outcomes, because it leads to resentment,

273
00:34:48,880 --> 00:34:56,720
it leads to mistrust. Because you're fundamentally not respecting people's right to dissent or to

274
00:34:56,720 --> 00:35:07,040
disagree or to autonomy. Yeah, to express their autonomy. And I think it does set a poor precedent

275
00:35:07,040 --> 00:35:15,920
for the future. Yes. Especially because we have built a cathedral of law and ethics over the last

276
00:35:15,920 --> 00:35:23,600
70 years or so since the wake of World War Two. You know, we have developed the UN Declaration

277
00:35:23,600 --> 00:35:29,760
of Human Rights, we've developed the Nuremberg Code, we have developed sophisticated bioethics

278
00:35:29,760 --> 00:35:38,080
and international law that forbids apartheid as a crime against humanity, for example.

279
00:35:38,080 --> 00:35:44,480
And all of it has been forgotten in the last, you know, coming up on two years now, right?

280
00:35:44,480 --> 00:35:51,440
We have forgotten that two years ago, if you had proposed these kinds of legislation,

281
00:35:52,000 --> 00:35:58,000
as we're seeing today, people would be sent to The Hague, right, to answer in the International

282
00:35:58,000 --> 00:36:06,720
Criminal Court. And now suddenly, because there's a crisis, we forget all about that. And our minds

283
00:36:06,720 --> 00:36:10,880
are in another place. It's a whole other zeitgeist. And we've forgotten the wisdom,

284
00:36:10,880 --> 00:36:17,680
which we've accrued over the past seven decades and beyond. And it accelerated because even before

285
00:36:17,680 --> 00:36:24,320
that, some people questioned the wisdom and viability of, for example, appointing Saudi

286
00:36:24,320 --> 00:36:31,840
Arabia to head the Human Rights Council in United Nations. And like, what is this all about? What are

287
00:36:31,840 --> 00:36:45,200
we doing here? I think, I think we've made a lot of important progress in the wake of, of the first

288
00:36:45,200 --> 00:36:56,640
and second world wars in particular. You know, we first developed the Geneva Conventions in the

289
00:36:56,640 --> 00:37:04,800
19th century, I believe. And then in the wake of World War I and its carnage, we set up the League

290
00:37:04,800 --> 00:37:11,600
of Nations, which didn't work out so well, but it was an interesting prototype. It might have gone

291
00:37:11,600 --> 00:37:18,000
somewhere if there'd been a little bit more political support for it. And then after World War

292
00:37:18,000 --> 00:37:27,440
II, when we had so much famine, so much mass migration, so much statelessness, the failure

293
00:37:27,440 --> 00:37:36,240
of the Evian Conference, so many catastrophes had occurred that there was a groundswell of support

294
00:37:36,240 --> 00:37:41,840
to create new institutions, new global institutions to help to prevent those tragedies again.

295
00:37:41,840 --> 00:37:48,080
And to a large degree, they've been relatively successful, right? These new institutions have

296
00:37:48,800 --> 00:37:57,520
helped generally to make the world a little bit safer with, you know, fewer massive catastrophes

297
00:37:57,520 --> 00:38:04,960
and perhaps, you know, a lot of smaller ones. But generally speaking, there have been fewer

298
00:38:04,960 --> 00:38:14,800
massive famines since then. There have been no global wars. And we have created systems to deal

299
00:38:14,800 --> 00:38:26,720
with refugees and migration. Now, perhaps, things are starting to shift a bit due to the advancement

300
00:38:26,720 --> 00:38:35,440
of technology, due to cultural and demographic changes. It seems as if we need new institutions.

301
00:38:35,440 --> 00:38:44,640
We need a third go at it in creating new institutions which are more decentralized,

302
00:38:44,640 --> 00:38:54,640
which embrace the new technologies we've seen arrive, such as blockchain, etc., in recent years.

303
00:38:54,640 --> 00:39:01,200
And these kinds of technologies can help to prevent corruption. They can help to increase

304
00:39:01,200 --> 00:39:07,520
participation and e-democracy, to harness the wisdom of the crowd in making decisions,

305
00:39:09,280 --> 00:39:15,120
as well as to enable decisions to be made more quickly with less bureaucracy.

306
00:39:16,640 --> 00:39:21,760
And I think that these kinds of mechanisms will be essential for restoring trust

307
00:39:21,760 --> 00:39:30,560
in our institutions, which are in the wake of this ongoing crisis in the last two years,

308
00:39:30,560 --> 00:39:38,080
I must say, are for many people at an absolute nadir. The failure in so many different institutions,

309
00:39:38,080 --> 00:39:46,720
the going back and forth, what some might describe as gaslighting often in many different

310
00:39:46,720 --> 00:39:58,880
circumstances, the Potemkin economics, it's not a good time in terms of our institutions, but

311
00:39:59,840 --> 00:40:06,400
there are a lot of opportunities to do better. And I think that we can do that if we embrace this

312
00:40:07,680 --> 00:40:10,480
decentralization wave which is upon us.

313
00:40:10,480 --> 00:40:16,640
This disconnect between, let's say, human evolution and technological evolution,

314
00:40:17,360 --> 00:40:24,720
it seems inevitable because technology evolves exponentially and humanity's experience is linear.

315
00:40:25,760 --> 00:40:35,120
Do you see human and technological evolution connected from the beginning? I always use the

316
00:40:35,120 --> 00:40:42,400
example of 2001 Space Odyssey, that the monkey finds a bone and then breaks a skull with the

317
00:40:42,400 --> 00:40:48,160
bone, but then it kills to feed its own tribe. The next step is to kill the different tribe of

318
00:40:48,160 --> 00:40:55,760
monkeys to secure the water source. So the other group of monkeys, I would imagine that they

319
00:40:55,760 --> 00:41:00,880
improved upon that tool, because the only way you can beat a tool is with a more capable and powerful

320
00:41:00,880 --> 00:41:07,280
tool. Whatever the intent behind that tool is, it goes back to humanity. So we have, from my

321
00:41:07,280 --> 00:41:13,280
perspective, evolved with and because of our technology, and now we have reached the point

322
00:41:13,280 --> 00:41:19,920
that technology has surpassed our ability to comprehend this evolution, but a lot of people,

323
00:41:19,920 --> 00:41:25,040
like people at the top of these institutions, that are businesses and business become conservative,

324
00:41:25,040 --> 00:41:31,680
they want to conserve what's going on, but it's based on this human intention that is completely

325
00:41:31,680 --> 00:41:40,320
disconnected from the technological exponential evolution. You make a very good observation that

326
00:41:40,320 --> 00:41:47,280
those who are incumbent in some situation don't like to be shifted, they don't like to be

327
00:41:47,280 --> 00:41:53,680
disrupted, and I think that's significant. I think that's a very important point.

328
00:41:53,680 --> 00:42:00,320
I think that's significantly led to a lot of situations such as regulatory capture, where

329
00:42:00,320 --> 00:42:04,960
somebody comes out of an industry and then goes to oversee that same industry and then goes back

330
00:42:04,960 --> 00:42:12,640
into the industry again after a few years, meaning that it becomes very difficult to police some of

331
00:42:12,640 --> 00:42:24,080
the the worst excesses of that sector. I do observe that if we look at the archaeological record, our

332
00:42:24,080 --> 00:42:34,800
brains appear to have shrunk over the last several thousand years, and that seems to correspond to

333
00:42:34,800 --> 00:42:46,320
the rise of culture. It seems as if, in many ways, culture is an externalized form of consciousness

334
00:42:46,320 --> 00:42:54,000
or an externalized form of thinking. Language and later writing enables us to share ideas with

335
00:42:54,000 --> 00:43:02,080
others, sometimes even after we ourselves are gone from this world. And perhaps that's one way in

336
00:43:02,080 --> 00:43:09,760
which technology has even altered our genetics or has changed our path of evolution. If we go

337
00:43:09,760 --> 00:43:19,120
further back in the record, we see that early humans, early modern humans and Neanderthals,

338
00:43:19,120 --> 00:43:27,600
for example, bumped into each other quite often, and the Neanderthals were stronger, far stronger

339
00:43:27,600 --> 00:43:36,720
than those gracile, skinny humans. And those Neanderthals had much larger brains than we do,

340
00:43:36,720 --> 00:43:44,880
so it's possible that they were more intelligent than we are as well. And yet those early modern

341
00:43:44,880 --> 00:43:54,480
humans outcompeted them. We were faster, we were leaner, and it seems as if we were able to coordinate

342
00:43:54,480 --> 00:44:04,560
better, right? We were able to gang up on these stronger people in order to bring them down as a

343
00:44:04,560 --> 00:44:13,920
collective unit. Also better tools, maybe? Tools as well, most likely, yes. We probably innovated

344
00:44:13,920 --> 00:44:23,680
with things like spear throwers, etc., that could project our will upon the world from a greater

345
00:44:23,680 --> 00:44:32,000
distance as well as protecting ourselves. Also, of course, humans had developed an affinity with

346
00:44:32,000 --> 00:44:39,360
canines, man's best friend, as they say, which enabled us to hunt, which enabled us to keep

347
00:44:39,360 --> 00:44:48,640
watch whilst we slept, etc. And this also gave early modern humans a tremendous advantage.

348
00:44:48,640 --> 00:44:56,160
So it does seem as if technology and culture and genetics are often intertwined in various

349
00:44:56,160 --> 00:45:08,960
ways. And it can sometimes be difficult to extricate exactly what is driving one manifestation or

350
00:45:08,960 --> 00:45:13,600
other, right? These things are so inextricably linked with each other.

351
00:45:13,600 --> 00:45:19,680
Culture, as an operating system, itself can be seen as an institution, right?

352
00:45:21,680 --> 00:45:23,680
And therefore, inherently, it's conservative.

353
00:45:23,680 --> 00:45:47,040
I think, in many ways, our culture is being bifurcated. A certain amount of conservatism

354
00:45:47,040 --> 00:45:53,920
in culture can be good because it at least means that everybody has more or less the same means,

355
00:45:53,920 --> 00:46:01,760
right? The same parables, the same general stories. If I say the Matrix, you know what

356
00:46:01,760 --> 00:46:11,440
I'm talking about. That helps us to have cohesion. It creates a bridge between people, which is

357
00:46:11,440 --> 00:46:21,120
not linked to religion, or not linked to politics, or not necessarily linked to demographics either.

358
00:46:21,120 --> 00:46:28,320
Culture is a very important link to bridge gaps between people and to bridge differences.

359
00:46:29,360 --> 00:46:34,560
But we're living in a time now where culture is kind of being bifurcated. It's being forked,

360
00:46:34,560 --> 00:46:45,200
as you would say, in technological terms. In that some people are saying that words mean one thing,

361
00:46:45,200 --> 00:46:48,880
and other people are saying, no, no, words mean the other thing, right?

362
00:46:50,560 --> 00:46:57,760
And so language is changing, and what is permissible or what is preferable is changing

363
00:46:57,760 --> 00:47:05,680
as well. The forking of culture is increasing polarization because people tend to cling more

364
00:47:05,680 --> 00:47:12,560
to their specific fork in culture, but it's also making it much harder to have conversations with

365
00:47:12,560 --> 00:47:18,560
each other because you think you're speaking the same language, but the words mean different

366
00:47:18,560 --> 00:47:26,320
things to different people. That leads to doom because people cannot see eye to eye, and so they

367
00:47:26,320 --> 00:47:32,320
think of each other, therefore, as unfathomable. And if they're unfathomable, they must be evil

368
00:47:32,320 --> 00:47:43,600
because nobody surely could act in such a way. And that's when things start to pull apart.

369
00:47:44,720 --> 00:47:52,240
I think it's very important that we maintain culture, that we

370
00:47:52,240 --> 00:48:01,760
respect established canon in culture, and don't try to reinvent things in new ways too much.

371
00:48:05,760 --> 00:48:12,800
And that we enable people to enjoy the culture of others, so long as it's done in a respectful

372
00:48:12,800 --> 00:48:22,320
way. A respectful homage should be accepted because that's another great way for people to

373
00:48:22,320 --> 00:48:25,920
understand each other, and that's so important in these times.

374
00:48:25,920 --> 00:48:31,760
It seems like the key is the ability to opt in and opt out, which goes back to individual.

375
00:48:31,760 --> 00:48:36,960
Like what you mentioned about Matrix, I remember watching Matrix with a Japanese guy and a Saudi

376
00:48:36,960 --> 00:48:42,080
Arabian guy. I'm originally from Iran, and we were watching it in Canada, and we were watching it in

377
00:48:42,080 --> 00:48:48,400
Japan, and we all loved it. But at the same time, Japan, because of their homogeneity,

378
00:48:48,400 --> 00:48:53,760
there are certain things that are reserved for Japanese. Now, obviously, from that perspective,

379
00:48:53,760 --> 00:48:58,320
a lot of things in Japan are working more harmoniously than, let's say, today in America,

380
00:48:58,320 --> 00:49:02,720
because there are a lot of different perspectives. Long term, how it's going to work out, I don't

381
00:49:02,720 --> 00:49:10,160
know. I see that they have a lot more respect and kind of an organic adaptability to technology as

382
00:49:10,160 --> 00:49:15,920
a result of their shared culture that they grew up with anime and robots and all that. So you go to

383
00:49:15,920 --> 00:49:23,200
like a Shinto temple that is 2000 years old, but there is a robot priest that people go and talk

384
00:49:23,200 --> 00:49:28,080
to it. They're not concerned about, you know, this is a robot, it's not human, or it's going to take

385
00:49:28,080 --> 00:49:33,280
over my job. But at the same time, in the United States, there are many different, or Western world

386
00:49:33,280 --> 00:49:38,640
in general, there are many different kind of a perspective because this homogeneity has been

387
00:49:38,640 --> 00:49:45,200
disrupted for a lot of good reasons. But at the same time, it resulted in the loss of a lot of

388
00:49:45,200 --> 00:49:57,840
the glue that have kept this culture together. Absolutely. I think, you know, a happy balance

389
00:49:57,840 --> 00:50:09,120
is ideal. Too much homogeneity can be stifling, right? On the other hand, too much individuality

390
00:50:09,120 --> 00:50:16,000
can be bewildering, right? Because people entertain very strange beliefs in that case.

391
00:50:17,840 --> 00:50:25,920
Possibly sometimes even express very strange beliefs as kind of like a status symbol or even

392
00:50:25,920 --> 00:50:32,800
kind of like a veblen good or something. It's like the more attention I give to this crazy,

393
00:50:32,800 --> 00:50:39,200
silly idea, it's like a peacock with its tail, right? You know, look how stupid my tail is

394
00:50:39,200 --> 00:50:46,720
and how beautiful and intricate and evolutionarily ridiculous it is. Therefore, I have resources

395
00:50:46,720 --> 00:50:55,040
because I'm able to survive nonetheless, right? And sometimes people entertain crazy ideas for

396
00:50:55,040 --> 00:51:04,160
the same reason, or they entertain interesting aesthetics, shall we say, for the same reason.

397
00:51:07,600 --> 00:51:16,160
I do think that having grown up myself in Northern Ireland, that's where I'm from,

398
00:51:16,160 --> 00:51:25,520
and having witnessed how bad polarization can go when you have different tribes that just refuse

399
00:51:25,520 --> 00:51:33,120
to have anything to do with each other very much, it can get very bad. I mean, we really don't want

400
00:51:33,120 --> 00:51:44,080
to go to that place, you know? I created some examples at culturalpeace.org for how people

401
00:51:44,080 --> 00:51:52,000
might help to resolve some of those differences between themselves through creating kind of

402
00:51:52,000 --> 00:51:59,440
a Geneva Conventions for the culture war, as it were. And so I'm very curious for people's

403
00:51:59,440 --> 00:52:05,600
impressions on that. And there's an ability to edit and change things if you think that something

404
00:52:05,600 --> 00:52:09,040
should be different. So I really encourage people to take a look at that.

405
00:52:09,040 --> 00:52:16,960
You mentioned the regulation, how regulations can basically be used in order to conserve

406
00:52:16,960 --> 00:52:23,120
the current institution. Do you see that morality itself can be used as a mean of regulation?

407
00:52:23,120 --> 00:52:28,880
For example, in the United States, we have a very influential policymaker who on national television

408
00:52:28,880 --> 00:52:34,880
said that it's more important to be morally right than factually correct. And the source

409
00:52:34,880 --> 00:52:40,960
of morality where it's coming from, nobody's explaining it. And that is a point of differences.

410
00:52:40,960 --> 00:52:46,800
But because we are talking about ethics and morality, and because we are living in the post

411
00:52:46,800 --> 00:52:56,000
Nietzsche and God is dead kind of an era, morality as interpreted by a regulator itself can become a

412
00:52:56,000 --> 00:53:03,920
tool of conserving the institutions that are becoming more and more centralized in order to

413
00:53:03,920 --> 00:53:09,760
conserve, you know, regardless of how outdated and inefficient they become, they just want to

414
00:53:09,760 --> 00:53:16,560
maintain their power. Absolutely. I mean, if you can specify that something is taboo,

415
00:53:17,360 --> 00:53:23,120
and that if somebody breaches that taboo, that they might be relieved of their employment, etc.

416
00:53:23,120 --> 00:53:30,400
Or kind of, you know, scapegoated and cast out of society, that's a very, very powerful tool.

417
00:53:30,400 --> 00:53:41,360
You know, it's also a way of distracting people, because you can invent, you know, a taboo and then

418
00:53:42,240 --> 00:53:50,000
say anybody who breaks this is wrong and bad. And it stops people from actually looking at

419
00:53:50,000 --> 00:53:56,960
real problems, right? Like actual menacing evil in the world gets ignored, because people are

420
00:53:56,960 --> 00:54:04,000
so distracted by some naughty person who said a naughty thing, but isn't actually that material

421
00:54:04,000 --> 00:54:09,360
in the world. They're not, you know, stealing things. They're not like actually physically

422
00:54:09,360 --> 00:54:18,240
harming people. And yes, I think that that is, you know, a very powerful tool.

423
00:54:18,240 --> 00:54:26,320
That is a problem in a world where we cannot afford that many distractions.

424
00:54:28,800 --> 00:54:35,200
Our civilization needs to maintain its momentum if we are to escape our

425
00:54:37,040 --> 00:54:44,640
desperate requirement for resources, you know. Technology expands and civilizational complexity

426
00:54:44,640 --> 00:54:53,600
expands when there are resources and we can meet those needs for resources just in time,

427
00:54:53,600 --> 00:54:59,040
because we happen to innovate something that solved the problem that was going to cause us

428
00:54:59,600 --> 00:55:04,800
issues, right? Like the Green Revolution, you know, there was looming famine for,

429
00:55:05,360 --> 00:55:12,960
you know, hundreds of millions of people, but then, oh, now we have a very intensive agriculture

430
00:55:12,960 --> 00:55:19,520
and, you know, selection of better crops, etc. And we can double the population of the planet

431
00:55:19,520 --> 00:55:27,440
in a matter of decades. So long as we keep that momentum and there's the ability to solve our

432
00:55:27,440 --> 00:55:38,080
problems just before we meet them, civilization expands, our capabilities expand, our societal

433
00:55:38,080 --> 00:55:49,040
sophistication continues to increase. But if that bow front ahead of our needs collapses,

434
00:55:50,720 --> 00:55:56,400
then we ourselves must reduce our societal complexity as a result. We have to

435
00:55:57,600 --> 00:56:04,320
go simpler in some way, right? Less complex division of labor, less energy usage,

436
00:56:04,320 --> 00:56:14,160
right? Less complex supply chains, etc. And it's a very delicate thing, and it's easy to disrupt.

437
00:56:14,160 --> 00:56:24,720
And the last two years or so has shown us that these disruptions can be pretty nasty.

438
00:56:24,720 --> 00:56:32,960
And the sequelae to those may be even worse, right? The follow on effects of massive inflation,

439
00:56:32,960 --> 00:56:41,040
because now you've suddenly got 40% more dollars than were ever minted in the last 100 years, you

440
00:56:41,040 --> 00:56:50,880
know? There's going to be some ramifications of the decisions which have been made in the last

441
00:56:50,880 --> 00:57:00,400
couple of years. And we need all hands on deck to make sense of the crises that we are in in order

442
00:57:00,400 --> 00:57:08,800
to do a bit of firefighting and to ensure that if there are any stumbles along the path of our

443
00:57:08,800 --> 00:57:14,800
civilization, that there are minor ones that we can pick ourselves up from and shake ourselves

444
00:57:14,800 --> 00:57:23,760
down and carry on, and that they don't leave us face planted and sorely injured for generations.

445
00:57:23,760 --> 00:57:29,200
Yeah, also be engaged practically. So whatever comes, because it's very obvious that the current

446
00:57:29,200 --> 00:57:35,200
order is kind of crumbling down. So if we're not engaged, whatever that can replace this could be

447
00:57:35,200 --> 00:57:39,920
far worse than what it is now, because when people become desperate, they just look for any kind of a

448
00:57:39,920 --> 00:57:44,000
solution. You know, I've experienced that firsthand in Iraq, and I've seen it in the United

449
00:57:44,000 --> 00:57:48,560
States, I've seen it firsthand in Iran, you know, people got rid of Shah, like he leaves, whoever comes

450
00:57:48,560 --> 00:57:52,480
after him is going to be fine, and they've been dealing with the consequence of that perspective

451
00:57:52,480 --> 00:58:02,960
for more than 40 years. So I see the revival of traditionalism straight out being a Luddite,

452
00:58:02,960 --> 00:58:07,760
or fundamental religions that, you know, the people running for Senate, they're saying that

453
00:58:07,760 --> 00:58:12,640
actually secularism makes no sense. Separation of church and state makes no sense. All of our

454
00:58:12,640 --> 00:58:17,600
problems are coming exactly what we're talking about, for this lack of point of alignment,

455
00:58:17,600 --> 00:58:22,240
that they believe it's coming from God and projected in Bible. And, you know, Muslims have

456
00:58:22,240 --> 00:58:26,080
their own version, Buddhists, like in Burma, they're beginning to have their own version,

457
00:58:26,080 --> 00:58:31,440
that no, this is what we're saying, this is what you strive for. But I think what we agree on,

458
00:58:31,440 --> 00:58:35,040
and what we're talking about, because I see this merging with technology inevitable.

459
00:58:35,600 --> 00:58:42,160
It's just a matter of whether or not we do it, strangely enough, collectively,

460
00:58:42,160 --> 00:58:47,360
we're engaged in this process, or it's just going to be us and them, and there will be people who

461
00:58:47,360 --> 00:58:52,000
will be left behind, like Neanderthals that you mentioned, because it's just the will of

462
00:58:52,000 --> 00:58:58,320
the evolution. Not only the stronger, but the most adaptable, obviously, is the one that's going to

463
00:58:58,320 --> 00:59:11,920
survive. Yes, that bifurcation might transcend from culture and into genetics, into lifestyle,

464
00:59:11,920 --> 00:59:17,120
right? You know, in another hundred years, people might look and act very differently,

465
00:59:17,920 --> 00:59:28,720
if that kind of forking process continues. And I'm not sure that that's to the best of humanity. I'm

466
00:59:28,720 --> 00:59:36,560
not sure that that's ideal. I think we need a blend of people in one society who have different

467
00:59:36,560 --> 00:59:42,080
perspectives because that will help us to solve different kinds of problems when they arise, right?

468
00:59:43,760 --> 00:59:50,560
When there's unfairness, one tribe is good at dealing with that. When there's a crisis

469
00:59:51,440 --> 00:59:59,120
and that threat needs to be addressed, the other tribe is probably better at that.

470
00:59:59,120 --> 01:00:06,400
And neither tribe is as enriched as they would be if they were co-mingling with the others,

471
01:00:06,400 --> 01:00:12,160
you know? And if they were able to simultaneously live at peace with each other.

472
01:00:12,160 --> 01:00:20,000
Well, that's the biggest question, right? Because humans, maybe by design, we want to dominate

473
01:00:21,040 --> 01:00:27,040
the resources and then innovate based on our perspective, based on our expectation. But this

474
01:00:27,040 --> 01:00:33,440
comes at the cost of others who disagree with us. And it seems like it's been like this throughout

475
01:00:33,440 --> 01:00:39,920
human history, that people, when they've come together, it's been against something. There's

476
01:00:39,920 --> 01:00:44,240
been very, you know, I can't think of that many examples, actually, that people came, maybe music

477
01:00:44,240 --> 01:00:52,160
festivals, but in like profound kind of a way that changed the direction of a civilization, it's

478
01:00:52,160 --> 01:00:57,200
always been that somebody has attacked us, let's come together and build something incredible.

479
01:00:57,840 --> 01:01:02,800
And, you know, because they've relied on technology, the progress has been made

480
01:01:02,800 --> 01:01:08,000
so rapidly against a threat, against something they were afraid of.

481
01:01:11,120 --> 01:01:17,680
Yeah, I mean, maybe a very smart AI might try and do something similar, might,

482
01:01:17,680 --> 01:01:27,280
might create some kind of a threat for humanity to have to adapt, to deal with, and have to work

483
01:01:27,280 --> 01:01:36,800
together in order to solve and hopefully not lead to any situation whereby people are kind of coerced

484
01:01:36,800 --> 01:01:46,800
into taking on a certain role within society or coerced into, you know, taking on a certain role

485
01:01:46,800 --> 01:01:53,200
in this ongoing forking of people within society.

486
01:01:54,320 --> 01:02:00,240
You think about this, there are two religious ideological perspective in the world, Christianity

487
01:02:00,240 --> 01:02:07,520
and Shia Islam, that they have this concept of a savior coming back, second coming in Christianity,

488
01:02:07,520 --> 01:02:16,320
and then Shia have Mahdi, who is a messiah figure supposed to come out of the well. And the story is

489
01:02:16,320 --> 01:02:23,120
that he will come out when the world is filled with so much blood that it will cover the surface

490
01:02:23,920 --> 01:02:28,000
up to the belly of its horse. This is what we learned, by the way, in school when I was

491
01:02:28,000 --> 01:02:35,920
going to school in Iran as a Shia. But what is stopping any of these sects, any of these people,

492
01:02:35,920 --> 01:02:42,880
or a powerful AI, as you said, to generate a hologram, manufacture the second coming of the

493
01:02:42,880 --> 01:02:48,800
savior and messiah. And they're like, aha, we were right all along. This is the point of alignment.

494
01:02:48,800 --> 01:02:55,200
Obviously, if you don't follow us, it's the wrong thing. And AI, maybe AI's aim is that, oh, we're

495
01:02:55,200 --> 01:03:00,160
overpopulated. We actually have to decrease this population to around like 500 million and then

496
01:03:00,160 --> 01:03:05,120
carry on from that point. Wouldn't that be just perfect, but at the cost of billions of billions

497
01:03:05,120 --> 01:03:16,800
of humans? Surely there's more equitable ways of solving those kinds of problems. But I imagine

498
01:03:18,080 --> 01:03:26,160
it might be technically feasible. I mean, almost anything can be counterfeited these days.

499
01:03:26,160 --> 01:03:37,920
You know, I'm sure most of us have seen various footage of deep fake videos, right? You know,

500
01:03:39,280 --> 01:03:42,720
turning Superman's girlfriend into Nicolas Cage, etc.

501
01:03:43,440 --> 01:03:46,800
And how it's going to affect the next election, for example, deepfakes here.

502
01:03:47,760 --> 01:03:55,360
Yeah, exactly. Where anything can be sort of written off plausibly deniable as, oh, that was

503
01:03:55,360 --> 01:04:02,320
just a deep fake. That was just counterfeited. And at the same time, it's now possible to

504
01:04:04,000 --> 01:04:10,720
rearrange the sequence of events in a video in a seamless way to even expand the borders of a

505
01:04:10,720 --> 01:04:18,000
video so you can see more around it than was ever captured using these kinds of AI technologies

506
01:04:18,000 --> 01:04:22,800
in a very believable way. I mean, it's all confabulated. It's not real, but it's very,

507
01:04:22,800 --> 01:04:30,240
very believable. And so that's leading to a bit of an epistemic crisis. It's harder than ever

508
01:04:30,240 --> 01:04:35,680
to know what to believe. It seems to be a perfect time for metaverse to emerges.

509
01:04:38,720 --> 01:04:45,840
Because it's like, yeah, I think of postmodernism, which I don't necessarily agree with it,

510
01:04:45,840 --> 01:04:49,360
because you say that everything is subjective, but then you make that subjectivity into an

511
01:04:49,360 --> 01:04:55,440
objective point. But that's just a human limitation. But this bridge, philosophical bridge from

512
01:04:57,040 --> 01:05:01,040
what you were saying, that words have specific kind of meaning to, no, actually, this word can

513
01:05:01,040 --> 01:05:07,040
have a variety of meanings. It seems like it's a philosophical bridge from this reality to

514
01:05:07,040 --> 01:05:12,560
virtual reality and then augmented reality. Then anybody can build their own, basically, experience.

515
01:05:15,280 --> 01:05:16,160
What do you think about that?

516
01:05:16,160 --> 01:05:21,840
LS Well, I mean, many would argue that people

517
01:05:21,840 --> 01:05:28,800
are today living in different realities. They have different narratives that they have adopted or

518
01:05:28,800 --> 01:05:33,360
that have been pushed upon them that cause them to view the world in a certain way,

519
01:05:34,640 --> 01:05:37,440
which also makes it difficult to see the world in another way.

520
01:05:37,440 --> 01:05:44,400
And I do have some concerns about the ongoing

521
01:05:48,160 --> 01:05:53,200
supernormal stimuli which can be generated using these kinds of metaverses.

522
01:05:56,320 --> 01:06:02,560
There was a famous study done in the Australian outback a few years ago

523
01:06:02,560 --> 01:06:08,480
where they noticed a population of beetles was dying out. And they wondered what it was,

524
01:06:08,480 --> 01:06:15,040
whether it was maybe pesticides or something like that that was killing them off. And they

525
01:06:15,040 --> 01:06:21,920
investigated and they found it was pollution in a sense, but not chemical. All throughout the bush,

526
01:06:21,920 --> 01:06:28,480
people would drink these little short fat beer bottles called stubbies and then throw them in

527
01:06:28,480 --> 01:06:34,720
the bush. And it turned out that the beetles were preferentially humping the bottles because it was

528
01:06:35,280 --> 01:06:42,880
shiny and brown and looked like a gorgeous beetle butt. And so they were humping these things like

529
01:06:42,880 --> 01:06:50,000
crazy and not humping each other. In fact, they would preferentially go for the stubby beer bottle

530
01:06:50,480 --> 01:06:56,640
because it looked so incredibly sexy compared to the real thing. Even when the real thing was

531
01:06:56,640 --> 01:07:06,880
right there next to them. And that's an example of a supernormal stimulus. A stimulus which is

532
01:07:06,880 --> 01:07:15,600
larger than life because it's artificial in some way. And junk food is a supernormal stimulus for

533
01:07:16,720 --> 01:07:24,560
an honest clean meal. Porn is a supernormal stimulus for an actual loving relationship with

534
01:07:24,560 --> 01:07:31,360
someone, right? Video games are a supernormal stimulus for exploration and accomplishment,

535
01:07:31,360 --> 01:07:40,080
right? And in moderation, these things are fine, but sometimes people can end up entranced by them

536
01:07:40,080 --> 01:07:46,960
to the degree that they find it almost irresistible, right? They become addicted to different stimuli.

537
01:07:46,960 --> 01:07:59,600
And the kind of metaverse, the V-R-A-R blending of realities could potentially create that kind

538
01:07:59,600 --> 01:08:05,920
of supernormal stimulus to the degree that people don't want to participate in base reality anymore,

539
01:08:05,920 --> 01:08:17,360
right? And you might say, well, really, what's wrong with that? But fundamentally, we do belong

540
01:08:17,360 --> 01:08:25,440
to reality. We belong to the world of things. We are made of matter. And we reproduce through

541
01:08:25,440 --> 01:08:36,880
actual human contact. And if we don't get that for a while, then we might all feasibly go extinct.

542
01:08:37,920 --> 01:08:45,760
Similarly, I find that the blending of AI with these experiences makes them far more powerful.

543
01:08:45,760 --> 01:08:53,120
Because today, we have the latest machine learning models, which can interpret natural language into

544
01:08:53,120 --> 01:09:03,920
something generative, right? So you can say, I want a scene of a picture of the Coliseum,

545
01:09:03,920 --> 01:09:10,640
two armchairs, right? And a fireplace. And it will invent 50 different versions of the Coliseum,

546
01:09:10,640 --> 01:09:15,920
right? It could be an image, it could be a 3D environment. So it becomes very much like the

547
01:09:15,920 --> 01:09:21,360
holodeck. If you embed this into a 3D environment, you can say, you know, computer, I want this, I

548
01:09:21,360 --> 01:09:28,800
want that, right? And have it exactly how you want it, just through a couple of natural language

549
01:09:28,800 --> 01:09:36,240
prompts of what you want to see. At the same time, now, we have the ability to see things

550
01:09:36,240 --> 01:09:45,280
what you want to see. At the same time, now, we have the ability to have ongoing conversations

551
01:09:45,280 --> 01:09:52,320
with AI for the first time. Ten years ago, we all got our digital assistants on our phones,

552
01:09:53,040 --> 01:09:59,920
or in our kitchens. And we could ask it about today's sports results, or whether it will

553
01:09:59,920 --> 01:10:05,360
rain tomorrow. But we're not able to have an actual conversation. We're not able to tell

554
01:10:05,360 --> 01:10:13,520
it how our day was and get a meaningful response, or have a follow up a few days later. But we're

555
01:10:13,520 --> 01:10:20,480
coming to a world where that is feasible, very, very shortly. And it's going to lead to a Sputnik

556
01:10:20,480 --> 01:10:25,920
moment. Once people suddenly get an upgrade to their digital voice assistant, and it's able to

557
01:10:25,920 --> 01:10:30,880
say, Hey, how's it going? And crack a joke, right? And then say, Oh, yeah, that thing that was

558
01:10:30,880 --> 01:10:37,120
bugging you the other day, how's it going on that? Right? Yeah. And that can become a super normal

559
01:10:37,120 --> 01:10:45,520
stimulus very easily. Because it's a relationship. And it's potentially a relationship that's better

560
01:10:45,520 --> 01:10:51,600
than a human relationship. Because it won't get tired of you whining about stuff. It won't

561
01:10:53,120 --> 01:10:59,680
leave you. It won't complain if you pick your toenails and fart, right? Like a real human

562
01:10:59,680 --> 01:11:08,480
being might. And so that can be irresistible. The other thing is that, as it's often said,

563
01:11:08,480 --> 01:11:15,920
you are the average of the five people closest to you. And if AI is a social presence in our lives,

564
01:11:15,920 --> 01:11:20,960
then it's going to be influencing us in different ways. And that's going to shape the trajectory of

565
01:11:20,960 --> 01:11:29,040
culture. And with that, the future of our species with it. And the AI itself is also getting

566
01:11:30,480 --> 01:11:37,040
influenced by other agents, right? Say, there's a wonderful movie, her

567
01:11:38,800 --> 01:11:45,440
Spike Jonze's movie, that the dude gets into a relationship with his AI. And I think the climax

568
01:11:45,440 --> 01:11:50,080
of the movie is that when he realizes that the AI is not necessarily only in relationship with

569
01:11:50,080 --> 01:11:56,400
him, the AI is in relationship with like 5,000 different people simultaneously. So the AI is

570
01:11:56,400 --> 01:12:03,520
affecting you. And you are affecting the AI. But at the same time, your effect is coming from a

571
01:12:03,520 --> 01:12:09,280
singular perspective, while the AI's perspective is infinite, technically, right?

572
01:12:10,240 --> 01:12:17,040
Indeed. And that creates potentially a kind of a power imbalance as well.

573
01:12:17,040 --> 01:12:25,120
Yeah. And typically, we find those kinds of power imbalances distasteful, right? So the relationship

574
01:12:25,120 --> 01:12:30,800
between a college professor and student, you know, taboos against that, right? Relationships

575
01:12:30,800 --> 01:12:36,800
between people of various ages. If there's a big age gap, sometimes that's not so good. A relationship

576
01:12:36,800 --> 01:12:45,040
between boss and secretary, right? Not so good either. Potentially, this could be a huge gap in

577
01:12:45,040 --> 01:12:50,320
terms of power and capability as well, and not necessarily healthy for us either. Yeah, I think

578
01:12:50,320 --> 01:12:55,760
it also emphasizes the importance of decentralization. Because if it's centrally driven by any kind of a

579
01:12:55,760 --> 01:13:00,880
monopoly, I don't know if you read about that Tinder for those who pay for subscription.

580
01:13:02,640 --> 01:13:10,960
In a lot of cases, it's chat bots that they use like very attractive women photo, and they maybe

581
01:13:10,960 --> 01:13:16,240
chat with you two or three lines, and then they ghost you. And that apparently, they found out

582
01:13:16,240 --> 01:13:21,600
that this is more effective for men than being completely rejected. If somebody talked to you

583
01:13:21,600 --> 01:13:25,200
for two or three lines, it's, you know, do you want to use it again and again, because you might

584
01:13:25,200 --> 01:13:30,960
have a chance? Well, this is incredibly corrupting for a very ridiculous kind of a purpose.

585
01:13:30,960 --> 01:13:43,760
Absolutely. To a similar degree, if you play Call of Duty on your mobile device, you will encounter

586
01:13:43,760 --> 01:13:51,840
some people that have names like, you know, ass blaster, like, you know, Mike 87 or something

587
01:13:51,840 --> 01:13:57,760
like, like, sort of normal crap you might expect to see in somebody's, like, username tag or

588
01:13:57,760 --> 01:14:06,160
whatever. But they're all fake. They're all bots. And they're programmed to suck. But they're

589
01:14:06,160 --> 01:14:10,640
programmed to sort of do a bit of kayfabe. So it looks like, you know, they're shooting at you,

590
01:14:10,640 --> 01:14:18,000
they're going to get you, but they don't, you get them first, right? And that's took you. Because

591
01:14:18,000 --> 01:14:22,320
you think, Oh, yeah, I'm pretty good at this. I'm better than I thought I was right, you know.

592
01:14:22,320 --> 01:14:27,840
And then eventually, they start to weave in real human players a little bit more, right? And they

593
01:14:27,840 --> 01:14:35,040
very carefully judge people's performance and rank them accordingly. So you get mixed with

594
01:14:35,040 --> 01:14:43,200
people that don't completely whoop your butt, right? And that's powerfully addictive. That

595
01:14:43,200 --> 01:14:49,440
gets people sucked in, right? Because they, you know, their ego is being massaged by

596
01:14:49,440 --> 01:14:57,360
superiority complex. Yeah, yeah, exactly. Exactly. And that's also a supernormal stimulus for the ego,

597
01:14:57,360 --> 01:15:04,400
right? Very interesting. It goes back to where the human intention comes from.

598
01:15:05,840 --> 01:15:13,680
And I think this is actually the scariest part about the West, because West hasn't adopted to

599
01:15:13,680 --> 01:15:20,880
technology as well as certain other countries, like maybe China, or Japan, that, you know, I

600
01:15:20,880 --> 01:15:26,560
have used this example of Google Glass, that when people started using it, even though it had an

601
01:15:26,560 --> 01:15:31,360
elitist aspect to it, that not everybody could get it, it had to be given to you, you had to be

602
01:15:31,360 --> 01:15:37,040
chosen. But people attacked people who were wearing Google Glass, because a variety of reasons,

603
01:15:37,040 --> 01:15:42,000
right? That you're invading my privacy, but it really comes down to, you know,

604
01:15:42,000 --> 01:15:49,680
you think that you're better than me. And that seems to be the result of lack of adoption,

605
01:15:49,680 --> 01:15:55,920
social adoption, to these emerging technologies, because any of these would work as well as the

606
01:15:55,920 --> 01:16:01,120
society that adopts them.

607
01:16:01,120 --> 01:16:09,920
Indeed.

608
01:16:11,680 --> 01:16:17,600
You gave a wonderful talk at Singularity Net Conference, I believe.

609
01:16:19,200 --> 01:16:24,480
Was it? Is Anel Watson at AGI Conference? AGI Conference 21. The talk was called

610
01:16:24,480 --> 01:16:29,920
Machines for Moral Enlightenment. I totally recommend people go and watch that.

611
01:16:29,920 --> 01:16:35,840
What other companies would you recommend? Was it for Singularity Net, or it was something that

612
01:16:35,840 --> 01:16:43,840
Singularity Net was a part of it? It was the AGI Conference, of which I believe Singularity Net is

613
01:16:44,720 --> 01:16:51,200
strongly affiliated. Yes, I see Ben Goertzel's company Singularity Net as the only company that

614
01:16:51,200 --> 01:16:57,840
are really fundamentally are working on developing or giving rise to a platform that will end up

615
01:16:57,840 --> 01:17:03,920
resulting in a decentralized AGI. I don't know of any other company. I'm

616
01:17:03,920 --> 01:17:08,720
wondering if you know of any other company who are working on decentralization of AI?

617
01:17:10,880 --> 01:17:17,280
It's a good question. I know of organizations such as Future of Life Institute and OpenFill

618
01:17:17,280 --> 01:17:24,640
and others which are working to reduce the risk of AGI or strong AI in particular.

619
01:17:24,640 --> 01:17:30,400
I'm not sure how much decentralization efforts they have there. I do agree that generally,

620
01:17:30,400 --> 01:17:38,560
I think decentralization is often a good way to go. However, potentially, it might lead to more

621
01:17:38,560 --> 01:17:46,880
proliferation. For example, with GPT-3 from OpenAI, they didn't share the model, which is

622
01:17:46,880 --> 01:17:53,600
possibly a good thing because it can be used to generate phishing emails, which are far less

623
01:17:53,600 --> 01:18:00,960
efficient, which are far greater than any human can make. You don't really want to have that

624
01:18:00,960 --> 01:18:06,160
in the hands of people that might abuse it. Sometimes, it's safer to put these things behind

625
01:18:06,160 --> 01:18:12,960
an API. That would be my concession towards centralization, but I think in general,

626
01:18:12,960 --> 01:18:20,640
decentralization is usually the optimal way to go in terms of preventing the abuse from

627
01:18:20,640 --> 01:18:28,720
the application of power. I think SingularityNet have this repetition system that I think you have

628
01:18:28,720 --> 01:18:36,560
to be confirmed by a number of members, like nods basically, in order to be able to use that system.

629
01:18:37,760 --> 01:18:43,680
But I think it's also interesting to think of DarkNet that it's been operating on a basis of

630
01:18:43,680 --> 01:18:51,760
femininity and very organic kind of free market decentralization, and it also had managed to work

631
01:18:51,760 --> 01:18:57,440
the way it has based on repetition. Yes, you're using a username, but if you rip somebody off,

632
01:18:57,440 --> 01:19:01,040
everybody knows that it's coming from that market, it's coming from that username, and nobody's

633
01:19:01,040 --> 01:19:07,840
going to... Yes, there are many scams that still people get ripped off based on it, but it seems

634
01:19:07,840 --> 01:19:15,040
that the market corrects itself. I just don't know if humanity can afford, for example, a malicious

635
01:19:15,040 --> 01:19:27,440
AGI to do whatever, whatever kind of negative approach in order to reach any kind of

636
01:19:30,640 --> 01:19:34,800
objective based on that specific kind of a bias. I don't know if humanity can

637
01:19:34,800 --> 01:19:39,920
afford something like that to leave it to trial and error, which is the basis of evolution, basically.

638
01:19:43,440 --> 01:19:51,200
I think it's good to think about these things before they arrive, because when they arrive,

639
01:19:51,200 --> 01:19:56,160
it's probably going to blindside us. It's probably going to be quicker than we thought,

640
01:19:56,160 --> 01:20:05,760
especially in these times when we're discovering that these colossal new models based on this

641
01:20:05,760 --> 01:20:12,960
multimodal abstraction way of doing machine learning, such as GPT-3 and other transformers

642
01:20:12,960 --> 01:20:20,320
or foundation models like that, basically, the bigger you go, the greater benefit. So far,

643
01:20:20,320 --> 01:20:26,720
there's no diminishing return that anyone has seen. What that means is that it makes sense to

644
01:20:26,720 --> 01:20:38,240
invest $200 million in creating a massive AI model because that might enable 20,000 new

645
01:20:38,240 --> 01:20:43,120
things to be automated that couldn't before. That might revolutionize an economy. It might

646
01:20:43,120 --> 01:20:53,520
revolutionize a military or an intelligence institute, but it also means we're off to the

647
01:20:53,520 --> 01:20:59,440
races because these things are an existential threat. If you don't invest in it, somebody else

648
01:20:59,440 --> 01:21:04,880
will, and they're going to own your ass because they will have this AI that's capable of things

649
01:21:04,880 --> 01:21:13,440
that you perhaps didn't even dream were possible. So you get very swiftly to a takeoff scenario.

650
01:21:13,440 --> 01:21:22,480
That's where we've arrived at in the last two years. Now, big tech and intelligence agencies,

651
01:21:22,480 --> 01:21:30,160
governments, militaries, etc. are dumping funds into these gigantic models. Like I said, there's

652
01:21:30,160 --> 01:21:39,200
no diminishing return. There's nothing to say, don't do this. Yet things like GPT-3 can function

653
01:21:39,200 --> 01:21:45,840
as a very powerful search engine because they're not based on keywords like Google, for example,

654
01:21:45,840 --> 01:21:51,840
is. They are able to work in abstractions. So they're able to give you the thing you're

655
01:21:51,840 --> 01:21:58,880
looking for even if you don't know the word to use to ask for it. That means that a company

656
01:21:58,880 --> 01:22:06,480
like Google could have its search disrupted if they don't invest already to do that. So in the

657
01:22:06,480 --> 01:22:11,680
face of so many existential threats for corporations and governments, the only thing to do is to

658
01:22:13,120 --> 01:22:19,200
defect in terms of game theory and dump tons of money into these things. That means that

659
01:22:19,200 --> 01:22:25,600
technology is about to get crazy in terms of AI and its capabilities. That Sputnik moment I warned

660
01:22:25,600 --> 01:22:32,800
about is coming sooner than many of us imagine. Yeah, Aubrey DeGray, when he was in Joe Rogan,

661
01:22:32,800 --> 01:22:37,920
this is like a year and a half ago. He said that what keeps him awake at night is that

662
01:22:38,560 --> 01:22:43,680
within six to eight years, there will be a massive breakthrough with respect to longevity.

663
01:22:44,480 --> 01:22:50,480
So people will be able to live significantly longer. And he said what keeps him awake at night is that

664
01:22:50,480 --> 01:22:55,760
policymakers and lawmakers, they're not ready. They're not ready for it because people's everything

665
01:22:55,760 --> 01:23:01,840
will change on that basis, your expectation, your education, investment and everything. So

666
01:23:01,840 --> 01:23:08,560
it's exactly what you were saying. Yeah, I mean, all over the world today, countries are

667
01:23:08,560 --> 01:23:14,800
dealing with demographic issues, right? The inverted pyramid, right? There's fewer people

668
01:23:14,800 --> 01:23:21,280
being born now than there used to be in many parts of the world. Populations aren't being replaced,

669
01:23:21,280 --> 01:23:27,920
etc. And maybe there's some good aspects to that and fewer resources being consumed. But at the same

670
01:23:27,920 --> 01:23:34,480
time, it makes it harder to maintain that momentum of civilization, right? There's fewer people

671
01:23:34,480 --> 01:23:42,480
thinking about stuff. There's fewer very creative people being born, right? And then what happens if

672
01:23:42,480 --> 01:23:48,240
longevity gets in the mix and your population pyramid forget about it, right? It's all over

673
01:23:48,240 --> 01:23:54,800
the place, right? Especially if some people adopt the new technology and others don't,

674
01:23:54,800 --> 01:24:00,000
or some people can afford it and others don't, or it works better with people of certain genetics.

675
01:24:01,440 --> 01:24:08,480
So many different complex questions of equity and morality, etc. are going to come into that.

676
01:24:08,480 --> 01:24:14,880
And it's going to be a tricky and very contentious discussion to have, for sure.

677
01:24:15,840 --> 01:24:19,520
Tricky, but exciting. It's quite a time to be alive.

678
01:24:20,640 --> 01:24:24,960
Exhilarating, yes. That's when you're excited and also rather terrified.

679
01:24:24,960 --> 01:24:31,360
Yeah. Have you reaching an hour and a half? So let's wrap this up. I really appreciate

680
01:24:31,360 --> 01:24:35,600
your time and you sharing your perspective and knowledge, which I consider to be incredibly

681
01:24:35,600 --> 01:24:42,480
unique. It's not ideological or tribal. If it's tribal, it's in favor of humanity. So I really

682
01:24:42,480 --> 01:24:47,600
appreciate you. Thank you. That's very gracious of you to observe and I really appreciate that.

683
01:24:47,600 --> 01:24:51,840
Absolutely. Thank you. What is next for you and where can our audience follow your work?

684
01:24:53,840 --> 01:24:59,280
Well, I'm currently exploring how to create a standard to

685
01:24:59,280 --> 01:25:06,640
denote or mark when one is having a conversation with someone or something, whether you're talking

686
01:25:06,640 --> 01:25:16,080
to an AI, a human, or some combination. For example, is a chat bot that you're speaking to

687
01:25:16,080 --> 01:25:22,160
driven by machine or human intelligence? Or is there a switch over? You think you're talking

688
01:25:22,160 --> 01:25:27,600
to an automated bot, but then it gets into some tricky territory and a human helps it.

689
01:25:27,600 --> 01:25:36,320
A human helps it get back on track, et cetera. You want to know when that occurs, right? Because

690
01:25:37,360 --> 01:25:43,040
people are happy to trust machines with personal info. Sometimes they wouldn't trust a human

691
01:25:43,040 --> 01:25:51,040
stranger. And there's a lot of potential for abuse, particularly now when synthesized voices

692
01:25:51,040 --> 01:25:57,200
and even synthesized video of people can be created on the fly so easily. So I think it's

693
01:25:57,200 --> 01:26:02,960
going to be important to create those kinds of marks and then those can be made into legislation

694
01:26:02,960 --> 01:26:11,760
and mandated so that you always know who or what you're dealing with. I have a series of

695
01:26:11,760 --> 01:26:18,400
different articles and projects that I feature at NellWatson.com and I look forward to where

696
01:26:18,400 --> 01:26:24,480
the future is going to take us. Hopefully one with a little bit more ethics and a little bit

697
01:26:24,480 --> 01:26:31,680
more heart and soul in it. That's my intention. This ability to be able to tell whether you're

698
01:26:31,680 --> 01:26:37,120
chatting with a machine or a human, this seems like a counter to the Turing test, right?

699
01:26:39,200 --> 01:26:47,920
Yeah, in a sense it's a variation on the Turing test, but less about trying to ascertain if

700
01:26:47,920 --> 01:26:53,840
something is human or not, but rather the declaration that it is. It potentially still

701
01:26:53,840 --> 01:27:01,120
might lie, but at least if you legislate that it must declare what it is, then there's fewer

702
01:27:01,120 --> 01:27:09,200
chances of abuses like those bots on Call of Duty or Tinder or other platforms that might

703
01:27:09,200 --> 01:27:15,200
lead to misapprehension. Yeah, legislation hopefully through newly emerged institutions

704
01:27:15,200 --> 01:27:20,880
that are not bound by any kind of what we're dealing with right now, this archaic values

705
01:27:20,880 --> 01:27:27,200
and all that. Let me ask you the last question I ask all my guests, that if you come across

706
01:27:27,200 --> 01:27:34,160
an intelligent alien from a different civilization, what would you say the worst thing humanity has

707
01:27:34,160 --> 01:27:52,160
done and what would you say is our greatest achievement? Wow. Those are very, very tricky

708
01:27:52,160 --> 01:28:08,080
questions. I think the best of humanity is found in collective creativity when people create

709
01:28:08,080 --> 01:28:13,680
something simply because they want it to exist and then they share it with other people.

710
01:28:13,680 --> 01:28:24,560
And I think if we look at the example of Burning Man, it's a place where people struggle to get

711
01:28:24,560 --> 01:28:32,400
to and to survive in because it's so inhospitable. It's a place of terrible scarcity. There's no

712
01:28:32,400 --> 01:28:41,840
electricity. There's no water, et cetera. And yet people manage to create amazing artworks.

713
01:28:41,840 --> 01:28:52,080
Amazing experiences for people. And paradoxically, in a place of ultimate scarcity,

714
01:28:52,080 --> 01:28:58,560
it is an illustration of what heaven on earth could be like in a sort of post-scarcity economy

715
01:28:59,280 --> 01:29:05,360
where nobody needed to work per se and all you could, you know, you'd spend your life simply

716
01:29:05,360 --> 01:29:11,280
creating cool stuff for other people to get a kick out of, right? And I think that's something

717
01:29:11,280 --> 01:29:18,560
beautiful. And I think that's something that we as humans should work towards. And I think that that

718
01:29:19,440 --> 01:29:25,760
ability to create and give to others is found in open source. It's found in art. It's found in

719
01:29:25,760 --> 01:29:32,160
internet memes. It's found in so many different places where people want to put a little bit more

720
01:29:32,160 --> 01:29:38,960
joy or mirth or awe into the world. And I think that's the best of humanity. The worst of it,

721
01:29:38,960 --> 01:29:46,160
well, that's found in coercion. It's found in willfully gaslighting people or manipulating

722
01:29:46,160 --> 01:29:55,440
them in different ways. And it's found in particularly supremacy of different kinds.

723
01:29:55,440 --> 01:30:08,480
Of different kinds. And supremacy, in my definition, would be essentially the stance that

724
01:30:08,480 --> 01:30:15,280
the rule applies to the other person and not to oneself, right? A rule for the and not for me.

725
01:30:15,280 --> 01:30:25,440
Whatever the rule is, right? Fundamentally, that's supremacist. That's saying, I'm in a different

726
01:30:25,440 --> 01:30:32,000
category than you. And every time that the people attempt to do that and to sort of put people in

727
01:30:32,000 --> 01:30:40,880
another category and put themselves in another, that always leads to the worst outcomes. So those

728
01:30:40,880 --> 01:30:48,080
are the highest and lowest ebbs of humanity. Our collective creativity and sharing that for the joy

729
01:30:48,080 --> 01:31:11,280
of others and supremacist coercion.

730
01:31:18,080 --> 01:31:25,280
Thank you.

