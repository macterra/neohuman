1
00:00:00,000 --> 00:00:03,340
The way I put this is at Vanity Trump's privacy.

2
00:00:03,340 --> 00:00:14,840
Hello, and welcome to the 11th episode of Neohuman Podcast.

3
00:00:14,840 --> 00:00:20,740
I'm Agha Bahari, at Agologist on Twitter and Instagram, and you can follow the show on

4
00:00:20,740 --> 00:00:25,360
liveonlimbo.com, iTunes, and soon on YouTube.

5
00:00:25,360 --> 00:00:30,840
With me today is Kevin Kelly. Kevin is a founding executive editor of Wired Magazine and a former

6
00:00:30,840 --> 00:00:35,000
editor publisher of the Whole Earth Review. Welcome to Neohuman Podcast, Kevin.

7
00:00:35,000 --> 00:00:36,880
It's my pleasure to be here.

8
00:00:36,880 --> 00:00:40,420
Let's start with a little bit of your background, the works you've done, and what you're working

9
00:00:40,420 --> 00:00:41,920
on now these days.

10
00:00:41,920 --> 00:00:52,240
It's a long story. I'm a pretty old guy. Let me just start from the current. I just finished

11
00:00:52,240 --> 00:00:59,720
a book called The Inevitable, which is about the next 20 to 30 years, primarily in the

12
00:00:59,720 --> 00:01:08,760
digital technology space. I kind of outline what I think are going to be the inevitable

13
00:01:08,760 --> 00:01:15,200
changes, such as things as artificial intelligence and virtual reality, which are coming whether

14
00:01:15,200 --> 00:01:17,120
we want them or not.

15
00:01:17,120 --> 00:01:23,800
My take on these is that we should embrace these fully as a way to steer them and manage

16
00:01:23,800 --> 00:01:31,560
them. We can't prohibit them. We can't stop them. We can only put our arms around, immerse

17
00:01:31,560 --> 00:01:38,320
ourselves fully in them, and with use, try and steer them.

18
00:01:38,320 --> 00:01:43,320
What inspired you to write this book now? I was actually going to ask you about the book,

19
00:01:43,320 --> 00:01:48,280
but what tools do you use to see into the future, and what are the 12 technological forces

20
00:01:48,280 --> 00:01:53,760
that you think will shape our future? Let's begin with why you saw the necessity to write

21
00:01:53,760 --> 00:01:55,880
this book now at this time.

22
00:01:55,880 --> 00:02:07,280
The origins of the book, truthfully, started on my blog, where I began writing what seemed

23
00:02:07,280 --> 00:02:13,040
to me to be very obvious things that I think were difficult for other people to see, and

24
00:02:13,040 --> 00:02:20,200
an example would be the fact that the Internet is the world's largest copy machine. Anything

25
00:02:20,200 --> 00:02:24,480
that touches the Internet is going to be copied, no matter what it is. If it can be copied,

26
00:02:24,480 --> 00:02:30,520
it will be copied when it touches it, and yet people were trying to stop the copying,

27
00:02:30,520 --> 00:02:40,640
and so my understanding, my realization was that you can't stop the copying. Copying is

28
00:02:40,640 --> 00:02:47,360
just an inherent, inevitable part of the Internet, and anyone who's trying to prohibit copying

29
00:02:47,360 --> 00:02:56,360
or outlaw it or prevent it with copy protection stuff is just working against the grain, and

30
00:02:56,360 --> 00:03:01,120
what you want to do is you want to work with the grain. You want to work with the fact

31
00:03:01,120 --> 00:03:04,680
that the copies are going to be promiscuous, and they're going to be ubiquitous, and you

32
00:03:04,680 --> 00:03:09,560
have to kind of understand that, and you have to embrace the idea that the copies are ubiquitous.

33
00:03:09,560 --> 00:03:15,320
It also means that they become worthless, too, and so you have to therefore have an

34
00:03:15,320 --> 00:03:20,240
economy, your own business, based on some things that can't be copied, and those become

35
00:03:20,240 --> 00:03:25,240
the valuable. That's the kind of insight that I was writing about on my blog, and there

36
00:03:25,240 --> 00:03:28,960
are others, and I just felt that it would really be beneficial for people to kind of

37
00:03:28,960 --> 00:03:36,720
understand this, that we would, as a society as a whole, benefit from accepting these technologies,

38
00:03:36,720 --> 00:03:41,840
and even, individually, if you were in business, you would also benefit. The origins of the

39
00:03:41,840 --> 00:03:48,440
book was trying to convey some basic notions of how technology works like that.

40
00:03:48,440 --> 00:03:53,320
Mm-hmm. What are the 12 technological forces, in your opinion?

41
00:03:53,320 --> 00:04:01,360
Yeah. In the book, I kind of gather these forces and give them a verb. They're actually

42
00:04:01,360 --> 00:04:13,680
technically a gerund. They're a constant motion. It ends ING, so there's screening, filtering,

43
00:04:13,680 --> 00:04:23,120
accessing, cognifying, things like that. I don't think we have time in this podcast

44
00:04:23,120 --> 00:04:32,040
to go through all 12 of them, and they're kind of larger umbrellas for other, they're

45
00:04:32,040 --> 00:04:38,720
not just 12 trends. There's 12 categories that I put these trends into, and they're

46
00:04:38,720 --> 00:04:42,320
somewhat arbitrary in the sense that they're a convenient way to organize things, but they're

47
00:04:42,320 --> 00:04:50,520
not really distinct trends. In fact, there's a great interdependency of them. Sharing is

48
00:04:50,520 --> 00:05:02,680
one of them. To share and to interact, interacting is another. Each of those encourage and empower

49
00:05:02,680 --> 00:05:08,720
the other. There's a deep interdependence of these forces, and so they're not even really

50
00:05:08,720 --> 00:05:14,080
that distinct. The things I cover, though, I cover artificial

51
00:05:14,080 --> 00:05:22,640
intelligence, and I cover what's coming in, the increase in interaction and interacting

52
00:05:22,640 --> 00:05:29,440
in virtual reality, and I cover tracking, which is the quantified self and the surveillance

53
00:05:29,440 --> 00:05:36,040
that we're going to be enduring. I would say the same thing about tracking, as I would

54
00:05:36,040 --> 00:05:42,480
say it by copying, which is that the Internet wants to track stuff. If you're touching the

55
00:05:42,480 --> 00:05:46,400
Internet, you're going to be tracked. If you're going into VR, we'll be tracked to an extent

56
00:05:46,400 --> 00:05:52,680
that has not been possible before. Everything that you do in VR will be tracked.

57
00:05:52,680 --> 00:05:59,920
We can't stop tracking. People who are thinking that we're going to make laws or whatever

58
00:05:59,920 --> 00:06:05,200
to stop tracking are wrong. We're going to have more tracking. The thing to do is to

59
00:06:05,200 --> 00:06:13,640
work with tracking and to make the tracking work for us, make it civil, make it domesticated,

60
00:06:13,640 --> 00:06:17,400
make it symmetrical. There's many things that we have to do, but we're not going to stop

61
00:06:17,400 --> 00:06:22,720
the fact that 50 years from now, 30 years, whatever, most of our lives are going to be

62
00:06:22,720 --> 00:06:29,960
tracked in some capacity. How do we make that livable? How do we make that work? How do

63
00:06:29,960 --> 00:06:33,720
we steer that? How do we make that kind of tracking? It's going to be tracked, but what

64
00:06:33,720 --> 00:06:40,240
kind of tracking is not inevitable? That's the kinds of things that I cover.

65
00:06:40,240 --> 00:06:47,800
Excellent. You mentioned to adapt the change and change with it in a way. What comes to

66
00:06:47,800 --> 00:06:55,040
mind is that general public has always made the argument for privacy, that all these technologies

67
00:06:55,040 --> 00:07:00,000
are intruding our privacy, and that government is making an argument. We saw a very good

68
00:07:00,000 --> 00:07:09,840
example with Apple versus FBI case, the argument for security. How do you think these two will

69
00:07:09,840 --> 00:07:14,520
come maybe to some sort of a mutual understanding with the tech side of things and grow with

70
00:07:14,520 --> 00:07:19,520
it in the upcoming future? I don't think they come to a common understanding. I think this

71
00:07:19,520 --> 00:07:27,160
tension will be with us forever. One of the important things to understand is that there's

72
00:07:27,160 --> 00:07:32,120
a really, there's a big linkage between two dimensions. The one dimension is this sort

73
00:07:32,120 --> 00:07:41,240
of, we may call it the privacy. On one end of this dimension on this axis is privacy.

74
00:07:41,240 --> 00:07:46,840
On the other one would be transparency. In one sense, you are sharing information about

75
00:07:46,840 --> 00:07:52,680
yourself and that would be transparent. The other one is you share nothing. At the extreme

76
00:07:52,680 --> 00:08:01,560
end, you are the Unabomber, Ted Konzinski, living in a shack somewhere off the grid.

77
00:08:01,560 --> 00:08:07,440
You have no friends. You communicate with nobody. You're a loner. You're a hermit. Nobody

78
00:08:07,440 --> 00:08:13,540
knows anything about you. At the other end, you have somebody who is living in everything

79
00:08:13,540 --> 00:08:21,600
about them is their sharing, their thoughts, their oversharing, whatever it is. That's

80
00:08:21,600 --> 00:08:30,720
one dimension. But there's another axis that's actually linked to that. That linkage is personalization.

81
00:08:30,720 --> 00:08:38,680
At one end, you, at the extreme end, everything about you is personalized. Everything, you

82
00:08:38,680 --> 00:08:48,440
get customized, everything. Your clothes are customized. Your food is customized. Your

83
00:08:48,440 --> 00:08:56,880
medical attention is all personalized just to you. At the other end of that axis is generic.

84
00:08:56,880 --> 00:09:00,960
Nothing is personalized. You just get, you're just a number. You're treated just like an

85
00:09:00,960 --> 00:09:05,520
average person. Everything is, average is generic. Here, the thing is, the thing is,

86
00:09:05,520 --> 00:09:14,360
those two axis, the transparency and the personalization are linked so that the only way that you can

87
00:09:14,360 --> 00:09:20,560
get something personalized is that you have to reveal something about yourself. I want

88
00:09:20,560 --> 00:09:24,640
my friends to treat me as individual, and therefore, I have to open up and reveal myself

89
00:09:24,640 --> 00:09:28,960
to my friends. The only way that a company can give you some kind of personalized service

90
00:09:28,960 --> 00:09:35,280
is if they know something about you. Those two things are linked so that if we want to

91
00:09:35,280 --> 00:09:42,200
have maximum personalization, we have to have maximum transparency. If we don't want any

92
00:09:42,200 --> 00:09:49,920
kind of transparency, then the price is going to be, you can be treated as generic number,

93
00:09:49,920 --> 00:09:55,000
not as an individual. The surprise has, and so there's a little slider that you can go

94
00:09:55,000 --> 00:10:02,480
back and forth. I think we should have that choice. Here's the thing, and technology gives

95
00:10:02,480 --> 00:10:07,720
us those kind of choices. Whenever the surprise has been in the last 10 years, is that when

96
00:10:07,720 --> 00:10:13,880
people are given the choice to push the slider, they are pushing it to the personalized transparent

97
00:10:13,880 --> 00:10:24,440
side. The way I put this is that vanity trumps privacy. People would much, in other words,

98
00:10:24,440 --> 00:10:32,040
are saying, treat me as an individual, personalized. I am unique. I am special. I'm not a number.

99
00:10:32,040 --> 00:10:39,280
The cost of that is being transparent and then people say, fine. The other end of like,

100
00:10:39,280 --> 00:10:43,240
I don't want anybody to know anything about me and just treat me as a number. Not so many

101
00:10:43,240 --> 00:10:54,840
people are there. As we go into the future, the demands and the requirements of personalization

102
00:10:54,840 --> 00:10:59,600
will require and encourage people to be transparent.

103
00:10:59,600 --> 00:11:04,680
Do you think there will be aspect of, because Internet is coming out of the machines into

104
00:11:04,680 --> 00:11:09,880
the world that we call the real world as we are experiencing, right? With virtual reality

105
00:11:09,880 --> 00:11:16,800
and augmented reality. Do you think there will be aspect of it that will be an equivalent

106
00:11:16,800 --> 00:11:23,480
of deep web and dark net that people can do many different things, a lot of them illegal

107
00:11:23,480 --> 00:11:25,800
and at the same time stay anonymous?

108
00:11:25,800 --> 00:11:35,520
Yeah. I think anonymity has to remain as an option, but it should be discouraged as much

109
00:11:35,520 --> 00:11:43,520
as possible. You should keep it to the smallest amount. I liken it to a rare earth element

110
00:11:43,520 --> 00:11:49,440
if you have the periodic table of all the elements. There are some elements that are

111
00:11:49,440 --> 00:11:58,120
required by our cells biologically, so cadmium, arsenic. These are elements that we require

112
00:11:58,120 --> 00:12:04,080
trace amounts that are necessary, almost like vitamins, that are necessary to have our cells

113
00:12:04,080 --> 00:12:11,360
working. While they are necessary in very low amounts, in any significant amount, they're

114
00:12:11,360 --> 00:12:22,200
toxic. The same thing with anonymity. We have to allow that as a possibility for the very

115
00:12:22,200 --> 00:12:27,160
rare political persecution whistleblowers, but you want to keep it down to an absolute

116
00:12:27,160 --> 00:12:33,840
minimum. If you have a lot of rare earth metals in your body, you die. If you have a lot of

117
00:12:33,840 --> 00:12:42,680
anonymity on your website or your firm, you die. It's toxic. The uses of it are very,

118
00:12:42,680 --> 00:12:49,440
very rare, are very special. We have to have that option, but to use it on a daily basis

119
00:12:49,440 --> 00:13:03,360
is toxic. If you look at all the places where there's trolling, harassment, all this kind

120
00:13:03,360 --> 00:13:07,240
of stuff, it all comes from places where people are anonymous or pseudo-anonymous because

121
00:13:07,240 --> 00:13:15,480
it brings out the worst in people. We do need to have an option, but it should be really

122
00:13:15,480 --> 00:13:27,920
kept and worked hard to keep it to the exception, to the rare earth element level of 0.00%.

123
00:13:27,920 --> 00:13:32,360
It should be low. If there's a lot of anonymity, it's going to be toxic.

124
00:13:32,360 --> 00:13:37,360
Even without being anonymous, I think the best example would be the Microsoft bot that they

125
00:13:37,360 --> 00:13:42,440
put on Twitter and in 24 hours, it became into a monster basically.

126
00:13:42,440 --> 00:13:47,000
That's because Twitter is often anonymous. It has nothing to do with the bot. It has

127
00:13:47,000 --> 00:13:55,520
to do with the fact that Twitter allows anonymous accounts. It's the anonymity of Twitter that

128
00:13:55,520 --> 00:14:03,640
caused it, not the bots. You don't see actually what happened in China. China did the same

129
00:14:03,640 --> 00:14:08,480
thing. They had a bot that was programming, but because China requires you to have your

130
00:14:08,480 --> 00:14:15,200
identity, a verified identity, there wasn't that same problem.

131
00:14:15,200 --> 00:14:19,360
You're suggesting a system that has to verify your identity and it has to be a singular

132
00:14:19,360 --> 00:14:23,840
identity because now it's not. I can be something in this world and I can be a completely different

133
00:14:23,840 --> 00:14:30,520
person in let's say second life, but the world of second life is becoming more and more real.

134
00:14:30,520 --> 00:14:36,640
Will there be VR companies or places that you can go to as anonymous? Yes, but I can

135
00:14:36,640 --> 00:14:52,440
tell you right now that a lot of those places will be like 4chan. They won't be a very pleasant

136
00:14:52,440 --> 00:15:01,560
place to be. That's all I can say. They won't be a pleasant place to be. There's also pseudo

137
00:15:01,560 --> 00:15:07,240
anonymity, which is where you have a consistent, you don't have your real name, but you have

138
00:15:07,240 --> 00:15:18,200
a consistent identity that can be linked back if needed with some effort. Boar people try

139
00:15:18,200 --> 00:15:26,480
out different identities that can be linked back. Again, I think that we have to have

140
00:15:26,480 --> 00:15:35,760
that option, but if it reaches any sizable degree, it becomes toxic.

141
00:15:35,760 --> 00:15:43,320
To control those elements, essentially we need to regulate them again. What's the elements?

142
00:15:43,320 --> 00:15:51,200
You give a very good example that if you go to a virtual reality company and want to sign

143
00:15:51,200 --> 00:15:56,440
up with them, you definitely need to have an identity, but for that virtual reality

144
00:15:56,440 --> 00:16:01,600
company to be required to collect your identity, they need to be regulated by a bigger body

145
00:16:01,600 --> 00:16:10,640
of governance. I would imagine. No, I'm saying that. There's no government

146
00:16:10,640 --> 00:16:15,520
entity that requires Facebook to try and verify your identity. They're doing it for business

147
00:16:15,520 --> 00:16:25,360
reasons, is what I'm saying. Facebook, even now, requires that you be a real person. There's

148
00:16:25,360 --> 00:16:32,160
no government regulation to that. That's their decision. The same thing with the VR. It's

149
00:16:32,160 --> 00:16:35,880
I'm saying good business practice is going to dictate. If you want to have a place that

150
00:16:35,880 --> 00:16:42,440
people are going to be comfortable with, you are going to require identities. I think there

151
00:16:42,440 --> 00:16:48,720
will be, over time, just like Facebook's biggest asset, I believe, is their verified

152
00:16:48,720 --> 00:16:57,720
identity. Other companies will either try to recreate that or many have many ride on

153
00:16:57,720 --> 00:17:03,440
top of Facebook's verified identity. That becomes an extremely valuable thing because

154
00:17:03,440 --> 00:17:09,360
they realize that if they want to have a place that people are comfortable with, they can't

155
00:17:09,360 --> 00:17:13,240
have high anonymity. It's just toxic.

156
00:17:13,240 --> 00:17:19,800
Well, you would agree that there will be alternative words. Let's say alternative social medias

157
00:17:19,800 --> 00:17:25,960
that are based on anonymity and encryption. Because one side of it is that people like

158
00:17:25,960 --> 00:17:30,600
Edward Snowden are warning us that government is monitoring you and using your information

159
00:17:30,600 --> 00:17:34,760
for bad. Therefore, these are the tools that you have to say anonymous, to say encrypted

160
00:17:34,760 --> 00:17:39,120
and all of that. But at the same time, you're making a very good point that more anonymity

161
00:17:39,120 --> 00:17:44,560
will create more shadowy places that wrong kind of characters can.

162
00:17:44,560 --> 00:17:55,440
Yeah, encryption should not be confused with anonymity. A lot of the information that, say,

163
00:17:55,440 --> 00:18:01,640
flows back and forth between my computer and Facebook can be encrypted. Don't confuse encryption

164
00:18:01,640 --> 00:18:06,040
with anonymity. Those are two different things. They have some overlap in some cases, but

165
00:18:06,040 --> 00:18:12,000
they can be completely separate. My credit cards are encrypted, but they're

166
00:18:12,000 --> 00:18:23,640
not anonymous. First of all, we do want encryption everywhere. We're only talking about anonymity

167
00:18:23,640 --> 00:18:28,000
here, which is not the same as encryption. So don't confuse those two things.

168
00:18:28,000 --> 00:18:33,280
What do you think about Bitcoin? I think Bitcoin is really interesting. The thing about Bitcoin,

169
00:18:33,280 --> 00:18:38,360
of course, is that it's only one of a hundred different cryptocurrencies. The thing about

170
00:18:38,360 --> 00:18:42,120
the cryptocurrencies is that you can kind of engineer them in any way. Bitcoin happens

171
00:18:42,120 --> 00:18:50,480
to have anonymity engineered into that, but it doesn't have to be. You can have a version

172
00:18:50,480 --> 00:18:55,520
of a cryptocurrency that is not anonymous. I think that the anonymity of Bitcoins is

173
00:18:55,520 --> 00:19:00,080
completely distracting to its really fundamental breakthrough, which is the blockchain. So the

174
00:19:00,080 --> 00:19:09,920
blockchain is the real core thing. The anonymity is actually harmful to it in the long term,

175
00:19:09,920 --> 00:19:13,760
but the blockchain is the really brilliant technological innovation, which is a kind

176
00:19:13,760 --> 00:19:19,000
of a distributed accounting system. It's distributed trust. It's a way of distributing trust.

177
00:19:19,000 --> 00:19:24,120
I think actually in the long term that many governments will actually mandate blockchain

178
00:19:24,120 --> 00:19:29,920
technology without the anonymity in order to make a completely accountable economy where

179
00:19:29,920 --> 00:19:35,960
every single transaction is accounted for. Do you think the advancement of modern technology,

180
00:19:35,960 --> 00:19:43,200
it's been advancing quite crazily after the turn of the century, it will essentially and

181
00:19:43,200 --> 00:19:48,120
ultimately forces all the other aspects of society, economical, political, everything

182
00:19:48,120 --> 00:19:52,640
to change with it. So we need a new political system. We need a new economic system.

183
00:19:52,640 --> 00:20:00,280
Yeah, I think it's, you cannot use distributed peer to peer open source technology and not

184
00:20:00,280 --> 00:20:04,000
have it affect your politics eventually.

185
00:20:04,000 --> 00:20:10,400
You dropped out of college, if I'm correct, and backpacked around Asia taking photographs.

186
00:20:10,400 --> 00:20:14,400
Would you talk a little about the experience and also the experience of learning from life

187
00:20:14,400 --> 00:20:18,280
versus academic institutions?

188
00:20:18,280 --> 00:20:25,400
Yeah, I mean, I was sort of grew, came to age in the kind of the arrow of the hippies

189
00:20:25,400 --> 00:20:30,800
and I read the whole earth catalog, which said that you should invent your life. And

190
00:20:30,800 --> 00:20:38,400
I took that to heart. I was in high school, I kind of was an overachiever. I doubled up

191
00:20:38,400 --> 00:20:46,200
in math and science every year. And the problem with college, at least when I was there, my

192
00:20:46,200 --> 00:20:52,000
first year, it was grade 13, it was more the same sitting in a classroom and I just, you

193
00:20:52,000 --> 00:20:57,960
know, if they had had things like a gap year, or internships or something, but I needed

194
00:20:57,960 --> 00:21:06,000
to do something at that point. And the only alternative I had was basically to do it myself.

195
00:21:06,000 --> 00:21:14,040
So I dropped out and went to Asia as a photographer, which was a passion of mine at that point.

196
00:21:14,040 --> 00:21:20,560
And by the way, this was at a time when very few people had cameras. I tried to describe

197
00:21:20,560 --> 00:21:27,680
this to my kids where a family would have a little brownie camera and they would have

198
00:21:27,680 --> 00:21:33,280
a roll of film in it and they would get it developed. It would have like 24 exposures

199
00:21:33,280 --> 00:21:38,160
and they would get it developed and it would come back and there would be pictures of Easter,

200
00:21:38,160 --> 00:21:45,120
4th of July, and Halloween on the same roll. I remember those days.

201
00:21:45,120 --> 00:21:49,960
24 pictures for a year. Okay. So that, you know, the average family might have taken

202
00:21:49,960 --> 00:22:00,480
24 pictures in a year. And so I had a camera and I filled my backpack with 500 rolls of

203
00:22:00,480 --> 00:22:09,920
film, which was, you know, been saving up for forever to buy, almost no clothes. And

204
00:22:09,920 --> 00:22:16,240
I would take, I would shoot two rolls a day. That was 70 pictures and I would come back

205
00:22:16,240 --> 00:22:21,640
and I would tell people that I shot two rolls of film and their eyes would bug out. That

206
00:22:21,640 --> 00:22:29,160
would just be unbelievable. 70 pictures in one day. What are you shooting?

207
00:22:29,160 --> 00:22:36,760
So I was often the only person with a camera within 100 miles in different parts of Asia.

208
00:22:36,760 --> 00:22:46,280
So that's what I did and what I got from that was I experienced a whole continent basically

209
00:22:46,280 --> 00:22:55,800
that was in transformation. But when I arrived in places like Afghanistan or Nepal, I had

210
00:22:55,800 --> 00:23:00,720
arrived in a time machine. I had gotten into a time machine and I traveled back to the

211
00:23:00,720 --> 00:23:10,000
1500s. I landed there and it was in every respect, the 1500s. The, I mean, their lives

212
00:23:10,000 --> 00:23:18,320
had not changed the feudal mindset, the feudal culture. There was no metal. There was, I

213
00:23:18,320 --> 00:23:22,720
mean, every, there was no electricity. There was no TV. There was no radio. I mean, there

214
00:23:22,720 --> 00:23:31,280
was, it was 1500s. And so I had the experience of living in the past in a real way in these

215
00:23:31,280 --> 00:23:38,760
cultures. And so when I, when came out of that into, you know, the capital cities and

216
00:23:38,760 --> 00:23:46,080
back to the US, I could see with my own eyes and witness it with my own being what technology

217
00:23:46,080 --> 00:23:52,200
was doing, what progress was, the fact that the progress was real and what it was like

218
00:23:52,200 --> 00:23:57,880
to not have technology because I lived in that time when there wasn't these things.

219
00:23:57,880 --> 00:24:03,520
So I got a very good sense of the benefits of progress and technology.

220
00:24:03,520 --> 00:24:07,280
Did you go to Iran too? I'm originally from Iran. That's why I'm curious.

221
00:24:07,280 --> 00:24:08,280
I lived in Iran.

222
00:24:08,280 --> 00:24:09,280
Oh yeah, which city?

223
00:24:09,280 --> 00:24:10,280
Tehran.

224
00:24:10,280 --> 00:24:11,280
This is in 70.

225
00:24:11,280 --> 00:24:19,200
79, 79 during the Khomeini Revolution. I lived in Medan Fadosi down the carpet bazaar.

226
00:24:19,200 --> 00:24:20,200
Wow.

227
00:24:20,200 --> 00:24:27,080
Fair the, all the chanting and the demonstrations, death to Americans was happening. And I would,

228
00:24:27,080 --> 00:24:32,000
I would actually lived above a carpet shop and I would come down and hang out with the

229
00:24:32,000 --> 00:24:36,840
people. And when the cameras weren't on, they were talking to me and the newest American

230
00:24:36,840 --> 00:24:40,880
and they say, we love Americans. We just hate your government. And I thought, oh my gosh,

231
00:24:40,880 --> 00:24:43,680
I wish Americans were that sophisticated.

232
00:24:43,680 --> 00:24:47,760
So I was learning Farsi, which I've also, I was going to school learning Farsi, which

233
00:24:47,760 --> 00:24:54,760
I've mostly forgotten, but I would have probably still be in Iran if I hadn't been kicked out

234
00:24:54,760 --> 00:25:00,640
because I loved it. And I unfortunately saw very little of it because the day I arrived,

235
00:25:00,640 --> 00:25:05,800
the day I arrived in Iran, I came here from Afghanistan, there was a day that they had

236
00:25:05,800 --> 00:25:10,720
the helicopter shooting the students from the helicopters. And I was like, travel was

237
00:25:10,720 --> 00:25:18,000
really hard. You know, it was, it was anyway, so I never got to Persepolis and we got to

238
00:25:18,000 --> 00:25:20,000
Esfahan someday.

239
00:25:20,000 --> 00:25:23,720
Yeah, you were there at the very key moment.

240
00:25:23,720 --> 00:25:29,520
And yes, yeah, it was, it was, yeah, it was a bad time for Iran. It really fell.

241
00:25:29,520 --> 00:25:33,600
You know, unfortunately, it's very sad what's, what's been happening and what's still happening

242
00:25:33,600 --> 00:25:34,600
to that country.

243
00:25:34,600 --> 00:25:35,600
Yeah.

244
00:25:35,600 --> 00:25:36,600
You mentioned.

245
00:25:36,600 --> 00:25:37,600
Someday I'll return.

246
00:25:37,600 --> 00:25:39,720
Yeah. Yeah. That would be awesome.

247
00:25:39,720 --> 00:25:44,920
I'll tell you, I have a lot of fans in Iran. I'm in touch with a lot of Iranians and the

248
00:25:44,920 --> 00:25:48,720
people who are into technology and they go to Sharif University. All those people are

249
00:25:48,720 --> 00:25:49,720
big fans.

250
00:25:49,720 --> 00:25:55,160
Yeah, yeah, I know. And I occasionally get emails from them. And, and I, you know, would

251
00:25:55,160 --> 00:25:59,840
very much like to go back and visit. In fact, I was just talking to a friend of mine who

252
00:25:59,840 --> 00:26:06,040
Tyler McNeven, who was for the past six years has been, we're trying to work with the government

253
00:26:06,040 --> 00:26:13,680
to do a project. He calls Iran Iran, which he wants to run the length of Iran and make

254
00:26:13,680 --> 00:26:19,280
a film about it, just meeting the people on the way. And several times it came very close

255
00:26:19,280 --> 00:26:23,560
to giving. Actually, they started one time. They gave him permits. He started, he ran

256
00:26:23,560 --> 00:26:29,520
part of it. And then they just kind of, I don't know, something happened. And so, so

257
00:26:29,520 --> 00:26:35,400
he hasn't been able to complete it. But we were just talking just a couple of weeks ago

258
00:26:35,400 --> 00:26:39,120
about, well, maybe we'd just go back as tourists at this point.

259
00:26:39,120 --> 00:26:43,120
Yeah, it's politics against all the good ideas.

260
00:26:43,120 --> 00:26:50,840
Yeah, yeah. No, I mean, again, I loved Iran. I loved the Iranians. I just, it was just

261
00:26:50,840 --> 00:26:57,120
really great. But I haven't seen enough of it, of the country to really claim the day.

262
00:26:57,120 --> 00:26:59,320
Yeah, hopefully one day.

263
00:26:59,320 --> 00:27:00,320
Yeah.

264
00:27:00,320 --> 00:27:05,080
Speaking of politics, how do you see the politics in the US affecting the growth of technology?

265
00:27:05,080 --> 00:27:14,040
Well, you know, the rise of Trumpism is a real step backwards in that most of the jobs

266
00:27:14,040 --> 00:27:19,280
that causing the anger for people are not, they haven't been taken by China. They've

267
00:27:19,280 --> 00:27:27,400
been taken by technology. I mean, it's automation. Outsourcing is just the first step in robots.

268
00:27:27,400 --> 00:27:33,040
So whatever goes to China will go into robots. In fact, China itself, 1 million people work

269
00:27:33,040 --> 00:27:38,960
for Foxconn, which makes the iPhones, they've committed even three or four years ago to

270
00:27:38,960 --> 00:27:47,520
buy 1 million robots. So they're going to automate, China is beginning to automate their

271
00:27:47,520 --> 00:27:56,520
own work. And so these things are inevitable. Automation, AI robots are inevitable, and they're

272
00:27:56,520 --> 00:28:05,040
going to cause even more friction, conflict, as some of the tasks that we currently do,

273
00:28:05,040 --> 00:28:09,080
or many of the tasks that we currently do, whether we're manual or white color, are

274
00:28:09,080 --> 00:28:19,240
going to go to the bots. And so I think this is, I think some of the backlash is just beginning.

275
00:28:19,240 --> 00:28:24,120
There's going to be more of it. But I think my message would be that this is nothing to

276
00:28:24,120 --> 00:28:30,000
do with other countries because it's happening there too. This is about automation and we

277
00:28:30,000 --> 00:28:37,880
can't stop it. We can't prohibit it. We have to embrace it. We have to, so in the future,

278
00:28:37,880 --> 00:28:43,320
in 20 years from now, people will be paid with how well you work with an AI robot. That

279
00:28:43,320 --> 00:28:50,080
will be the skill. You're going to work with them, not against them, but understanding,

280
00:28:50,080 --> 00:28:59,480
being able to talk, to converse well, partner with AI robot is going to be the very valuable

281
00:28:59,480 --> 00:29:03,000
skill. And that's sort of what we should be teaching.

282
00:29:03,000 --> 00:29:06,880
Interesting. How would you categorize that skill? Like if you want to call someone a

283
00:29:06,880 --> 00:29:11,600
doctor or an engineer, what would be the title of that person? Or this will be around the

284
00:29:11,600 --> 00:29:12,600
fields.

285
00:29:12,600 --> 00:29:16,880
It'll be around the fields. But in fact, doctoring is one of the things that was just going

286
00:29:16,880 --> 00:29:24,280
that direction. So the Watson, which is being retrained after playing Jeopardy, is being

287
00:29:24,280 --> 00:29:34,400
trained as a diagnostic doctor. And their market is not people in Africa with a smartphone

288
00:29:34,400 --> 00:29:39,120
accessing that because you could do that and that would be better than no doctor. But it

289
00:29:39,120 --> 00:29:43,720
actually requires a certain, to get the most out of it, requires a certain amount of intelligence

290
00:29:43,720 --> 00:29:50,480
to working with the person. So it's like the patient doesn't quite know how to make the

291
00:29:50,480 --> 00:29:56,960
best use of the AI, but the doctor does. And so it's the doctor plus the AI that are going

292
00:29:56,960 --> 00:30:02,560
to be the best diagnosis. It's not just the AI itself, not the doctor by itself, it's

293
00:30:02,560 --> 00:30:03,960
the doctor plus AI.

294
00:30:03,960 --> 00:30:12,080
So doctors who learn, it's like learning how to search or learning, being a power user

295
00:30:12,080 --> 00:30:19,680
for Google. They'll be power users, AI power users who really know how the AI thinks, how

296
00:30:19,680 --> 00:30:26,400
to best optimize its knowledge and its use. And they know people too. So they're kind

297
00:30:26,400 --> 00:30:31,560
of like the interface. And so that will certainly be for several decades, that will be the,

298
00:30:31,560 --> 00:30:36,360
I think, the pattern, which is you have AI plus humans as being the world's expert.

299
00:30:36,360 --> 00:30:43,240
It's also true by the way right now in chess. When Gary Kasparov, the world chess champion

300
00:30:43,240 --> 00:30:51,560
lost to Deep Blue, the IBM Deep Blue, he complained afterwards saying, look, you know, Deep Blue

301
00:30:51,560 --> 00:30:59,680
had a record of every single chess play ever. All chess moves. He said, if I had access

302
00:30:59,680 --> 00:31:05,560
to that same database while I was playing, I would have beat Deep Blue. So he made a

303
00:31:05,560 --> 00:31:10,960
new chess league called the Freestyle Chess League. And there you could play, humans could

304
00:31:10,960 --> 00:31:21,760
play with AI database. And so now it's freestyle because you can play any, it's like kind of

305
00:31:21,760 --> 00:31:28,960
freestyle martial arts. You can play any method you want. You can play as a single, solitary

306
00:31:28,960 --> 00:31:34,640
human chess champ. You could play as an AI or you can play as what they call centaurs

307
00:31:34,640 --> 00:31:41,800
and AI plus human. And in the past couple of years, the world's best chess player on

308
00:31:41,800 --> 00:31:49,600
the planet is not an AI. It's not a human. It's a centaur. It's an AI plus human. And

309
00:31:49,600 --> 00:31:54,200
that's just showing you that that partnership is really the way that we're going to go at

310
00:31:54,200 --> 00:31:55,800
least for the first couple of decades.

311
00:31:55,800 --> 00:31:59,280
Oh, I totally agree. And that makes a lot of sense because I was going to say that access

312
00:31:59,280 --> 00:32:03,960
to database is one thing, but processing all that information to the speed that AI is.

313
00:32:03,960 --> 00:32:08,520
But yeah, that makes a lot of sense. Speaking of AI, some prominent figures in the world

314
00:32:08,520 --> 00:32:14,640
of tech expressed their concern about negative possibilities that can be caused by the rise

315
00:32:14,640 --> 00:32:19,360
of artificial intelligence. Where do you stand on pros and cons of artificial intelligence?

316
00:32:19,360 --> 00:32:27,040
Yeah, yeah. I mean, it's certainly, we have to consider that possibility. And I think

317
00:32:27,040 --> 00:32:33,720
even though I think that possibility is very, very low and very unlikely for a number of

318
00:32:33,720 --> 00:32:40,720
reasons, I think that the solution to that problem is something we should be doing anyway.

319
00:32:40,720 --> 00:32:51,320
So I endorse the solution, which is that we want to train our AIs to be, to have values,

320
00:32:51,320 --> 00:32:56,000
to be ethical and moral. And that's what's happening right now. There's Google has a

321
00:32:56,000 --> 00:33:00,440
group. They're trying to teach the ethics to the self-driving cars because you have

322
00:33:00,440 --> 00:33:06,200
to, you have to program all that kind of stuff in beforehand. So like, who, if there's an

323
00:33:06,200 --> 00:33:13,240
accident with pedestrian, who should get priorities? Should it be the passenger or the pedestrians?

324
00:33:13,240 --> 00:33:20,560
You have to actually decide that right now. And so those are ethical questions. And

325
00:33:20,560 --> 00:33:27,800
I was going to ask you about ethics and morality. Then first of all, do you think it's objective

326
00:33:27,800 --> 00:33:36,560
and how we would translate from organic analog human thought process to a digital machine

327
00:33:36,560 --> 00:33:48,120
process? Yeah, I think we humans overestimate the uniqueness or the distinctiveness. And

328
00:33:48,120 --> 00:33:54,080
that's that's the humans. And we're not that much. Most of what we can do, including our

329
00:33:54,080 --> 00:33:59,240
emotions can be programmed. And that's been the sort of never ending surprise. We keep

330
00:33:59,240 --> 00:34:04,160
being surprise, surprise, surprise, surprise. And we should realize by now that things that

331
00:34:04,160 --> 00:34:07,680
we thought only humans can do, obviously machines can do. And then once they machines

332
00:34:07,680 --> 00:34:15,200
do, we say, of course, machines can do that. That's just machine learning. Well, yeah,

333
00:34:15,200 --> 00:34:20,680
so emotions will be programmed into machines. It's not that difficult, and they'll be very

334
00:34:20,680 --> 00:34:27,360
valuable to have emotions in machines. Our animals, our pets have emotions. And so it's

335
00:34:27,360 --> 00:34:34,520
not, it's not just us. And so most of the things that animals can do, it's just kind

336
00:34:34,520 --> 00:34:41,920
of programming will program into machines. And so ethics turns out to be, it turns out

337
00:34:41,920 --> 00:34:46,800
that actually humans are not very good at ethics. We're very inconsistent. We're very

338
00:34:46,800 --> 00:34:52,720
opportunistic, we're very situationalist, relativistic. But what's going to happen as

339
00:34:52,720 --> 00:34:56,640
we try to program them into machines, we're going to realize this and actually will help

340
00:34:56,640 --> 00:35:01,440
us to become better, just like just like having children makes you a better person. We're

341
00:35:01,440 --> 00:35:10,680
going to try to teach our mind children, our technological children, ethics, we will we

342
00:35:10,680 --> 00:35:15,920
will divide we will realize and we will make ourselves better ethically, because we'll

343
00:35:15,920 --> 00:35:20,240
see the inconsistencies and we'll try to overcome them.

344
00:35:20,240 --> 00:35:24,680
Kevin has a new book coming. It's called the inevitable understanding the 12 technological

345
00:35:24,680 --> 00:35:28,480
forces that will shape our future. I think it's coming in June.

346
00:35:28,480 --> 00:35:33,360
That's right. June 7 is the pub date. It's available on Amazon for preorder right now.

347
00:35:33,360 --> 00:35:37,640
Excellent. We talked a little about future. I want to ask you about another future is

348
00:35:37,640 --> 00:35:44,080
Ray Kurzweil. I know you respect him, but you disagree with his perhaps optimism about

349
00:35:44,080 --> 00:35:48,520
the singularity. Do you think the singularity is near?

350
00:35:48,520 --> 00:35:54,640
So yeah, I've known Ray for a while and I respect him and actually I'm in total agreement

351
00:35:54,640 --> 00:36:01,080
with his optimism, except I disagree with his idea about the singularity. So I'm optimistic

352
00:36:01,080 --> 00:36:08,680
without the singularity. The singularity just to recap, I think there's two versions, at

353
00:36:08,680 --> 00:36:13,640
least two versions of it, maybe more. I would kind of reduce them right now in a kind of

354
00:36:13,640 --> 00:36:18,920
oversimplification as being a strong singularity and a weak one. The strong one is the one

355
00:36:18,920 --> 00:36:25,560
that Ray believes and it's that intelligence explosion where you make an AI and AI gets

356
00:36:25,560 --> 00:36:35,840
so smart that it can design an AI that's smarter than itself and then that AI will then design

357
00:36:35,840 --> 00:36:41,240
an AI that's smarter than itself and each time it does it, it may do it a little quicker.

358
00:36:41,240 --> 00:36:45,880
So from outside, it looks like all of a sudden once you make an AI that's smarter than humans,

359
00:36:45,880 --> 00:36:53,360
it suddenly instantly is God. It's infinite wisdom and knowledge and the first thing it

360
00:36:53,360 --> 00:37:01,520
does is it makes us immortal. It figures out how to make us immortal and once it's immortal,

361
00:37:01,520 --> 00:37:10,440
then maybe it can even bring back the dead. So therefore, all these good things happen

362
00:37:10,440 --> 00:37:15,800
once you have the first AI that's smarter than humans and so if you can live to that

363
00:37:15,800 --> 00:37:21,040
point, whatever year that is, then you'll live forever. Ray believes that and that's

364
00:37:21,040 --> 00:37:26,720
why he's taking 250 pills a day because he wants to live until that moment so that he

365
00:37:26,720 --> 00:37:33,080
can actually resurrect his father. I don't even know where to begin to say all the reasons

366
00:37:33,080 --> 00:37:40,800
why I think that's not going to happen. There's two couple things. One is he just says, well,

367
00:37:40,800 --> 00:37:47,600
we're on exponential growth of computation and therefore the singularity is near. Well,

368
00:37:47,600 --> 00:37:54,880
there's the thing. Any point on an exponential curve is near the singularity. That's the

369
00:37:54,880 --> 00:38:01,160
whole thing. I mean, it's like the singularity was near 200 years ago if we're on a singularity

370
00:38:01,160 --> 00:38:09,320
curve and it will be 200 years from now. It's always by definition the singularity is always

371
00:38:09,320 --> 00:38:21,120
near on an exponential curve. Second thing is that I think this idea that an AI will

372
00:38:21,120 --> 00:38:28,280
both either can design itself smarter or make us immortal is what I call the fallacy of

373
00:38:28,280 --> 00:38:35,480
think-ism, which is that by thinking about things, you can solve problems. So the idea

374
00:38:35,480 --> 00:38:40,360
is that if you had an AI that's really smart, it would read all the medical literature in

375
00:38:40,360 --> 00:38:45,160
the world and then cure cancer. That's not how you cure cancer. You can't cure cancer

376
00:38:45,160 --> 00:38:50,720
by thinking about it. We don't have enough information. You have to perform experiments.

377
00:38:50,720 --> 00:38:56,280
There's just so much we don't know that we're not going to get out by thinking about it,

378
00:38:56,280 --> 00:39:01,680
no matter how smart we are. We have to actually do experiments which take biological time

379
00:39:01,680 --> 00:39:07,760
and most of them fail. And no matter how smart you are, you have to spend a lot of time

380
00:39:07,760 --> 00:39:13,520
doing experiments on biological subjects like humans, which just take times. Even simulations

381
00:39:13,520 --> 00:39:21,120
aren't enough to answer those questions because you have to ground-proof assimilation. So

382
00:39:21,120 --> 00:39:27,320
this idea that AI's can do these things, including making themselves more just by thinking

383
00:39:27,320 --> 00:39:32,760
about it, is just a total fantasy. You mentioned the examination through experimentation.

384
00:39:32,760 --> 00:39:38,440
Isn't the part of the argument that the part of examination also will be done by AI and

385
00:39:38,440 --> 00:39:45,800
robotics, nanobots, for example, in our bloodstream and between our neurons. So on both sides,

386
00:39:45,800 --> 00:39:53,560
there will be machines and nanobots and AI doing the work. My thinking is that why would

387
00:39:53,560 --> 00:39:59,600
they want to do something that we can survive because they will have very different values

388
00:39:59,600 --> 00:40:02,520
based on the needs that they will have?

389
00:40:02,520 --> 00:40:13,520
So I think, as I said back, I think we can put our values into the AI's we make. I think

390
00:40:13,520 --> 00:40:19,680
the whole point of cultural evolution is that it's not necessarily Darwinian. There's every

391
00:40:19,680 --> 00:40:27,080
reason. One of the things we realize as humans in our culture is that the more species, the

392
00:40:27,080 --> 00:40:32,800
more variety diversity is a good thing for the economy, for ourselves. So we want to

393
00:40:32,800 --> 00:40:37,840
have as many different ways of thinking. And one of the things, going back to this whole

394
00:40:37,840 --> 00:40:42,000
thing of AI, and this is another point, is that we tend to think of intelligence as a

395
00:40:42,000 --> 00:40:48,800
single dimension. It's not a single dimension. Our own intelligence is a whole suite, a spectrum

396
00:40:48,800 --> 00:40:59,040
of probably about 100 different types of thinking from spatial deduction, rationalization, emotional

397
00:40:59,040 --> 00:41:05,360
intelligence. I mean, there's hundreds of them. We don't have a general purpose intelligence

398
00:41:05,360 --> 00:41:12,600
at all. If we have a very, very specific kind of collective of intelligence suited to our

399
00:41:12,600 --> 00:41:21,440
survival evolutionarily. First of all, the only way you can make this kind of intelligence

400
00:41:21,440 --> 00:41:26,080
is to have the same substrate, a biological substrate. But secondly, there's not much

401
00:41:26,080 --> 00:41:32,320
reason to make this suite, this collective of intelligences, artificially. What we want

402
00:41:32,320 --> 00:41:36,360
and what we're doing is making other kinds of thinking. So your calculator is smarter

403
00:41:36,360 --> 00:41:41,640
than you are in arithmetic, your GPS is smarter than you are in spatial navigation, Google

404
00:41:41,640 --> 00:41:49,200
is smarter than you are in total recar. And the self-driving car will drive in an inhuman

405
00:41:49,200 --> 00:41:52,280
way. That's the whole point. That's why we want it driving. We don't want it to drive

406
00:41:52,280 --> 00:41:58,560
like a human. And so what we'll be doing is making different species of thinking. They'll

407
00:41:58,560 --> 00:42:03,240
think differently. The AIs that we're going to make are going to think differently than

408
00:42:03,240 --> 00:42:09,720
humans. That's their main value, because we can get them to help us, work with us to

409
00:42:09,720 --> 00:42:15,520
solve problems that we can't. So these are going to be different species. And the AIs

410
00:42:15,520 --> 00:42:21,320
won't be able to think like us. Our thinking will be different. And there's every reason

411
00:42:21,320 --> 00:42:31,240
to work with us. This idea that the AIs need to kill us is just a crazy, insane Hollywood

412
00:42:31,240 --> 00:42:37,200
trope. It's a cliche. It doesn't make any sense at all. I can't really think of it.

413
00:42:37,200 --> 00:42:43,160
In fact, I think that would be even hard to do, even if it was your job to program this.

414
00:42:43,160 --> 00:42:44,280
I don't think you could do it.

415
00:42:44,280 --> 00:42:48,640
Isn't it interesting that humans find that scenario interesting enough?

416
00:42:48,640 --> 00:42:55,840
Yeah, right. And it's kind of the only vision of the AI future is they take over and terminate

417
00:42:55,840 --> 00:43:04,040
us. And I think it's because this is so cinematic. I mean, it's just a cheap cinematic device.

418
00:43:04,040 --> 00:43:13,480
And the idea that they would find is useful. But more importantly, it's not going to be

419
00:43:13,480 --> 00:43:18,160
that separate from us. And this is the other important thing. So that was the strong version.

420
00:43:18,160 --> 00:43:24,080
I think the weak version of the singularity is that we are making a planetary super-organism

421
00:43:24,080 --> 00:43:34,360
of something, that we are weaving ourselves 7 billion people plus 15 quintillion computers

422
00:43:34,360 --> 00:43:40,200
and smartphones together into one thing that's always on, that there is an intelligence at

423
00:43:40,200 --> 00:43:43,200
this higher level that we can't understand.

424
00:43:43,200 --> 00:43:48,000
So the whole point of the singularity is you can't see beyond it. The insight was that

425
00:43:48,000 --> 00:43:53,640
you have something that was the original idea of the singularity is that there's a horizon

426
00:43:53,640 --> 00:43:59,960
beyond which you cannot see at all. And that you can't even imagine. And I think that there's

427
00:43:59,960 --> 00:44:06,720
a soft version of that where we have a planetary intelligence that we as neurons sort of can't

428
00:44:06,720 --> 00:44:12,520
really understand. And that seems to me to be very, very likely. Again, we are then part

429
00:44:12,520 --> 00:44:18,480
of the thing. So really not this sense of us against them. It's like, no, we are them.

430
00:44:18,480 --> 00:44:26,640
It is us. We're part of it. We may not understand it when it does stuff, but that's us. And

431
00:44:26,640 --> 00:44:33,840
so I think that's a second reason. So on that kind of singularity, I'm saying, yeah, I think

432
00:44:33,840 --> 00:44:38,680
there could be this soft singularity where the future and what happens is really hard

433
00:44:38,680 --> 00:44:45,600
for us to see because we have this thing that has not existed before and it has its own

434
00:44:45,600 --> 00:44:46,600
thoughts.

435
00:44:46,600 --> 00:44:48,720
Do you have a frame for that or we just don't know?

436
00:44:48,720 --> 00:44:54,880
Yes, it's now. We've already begun. I mean, we're building it now. And I'm trying to take

437
00:44:54,880 --> 00:45:00,440
this idea seriously. It's not just the metaphor. I think, no, we really do have a machine that's

438
00:45:00,440 --> 00:45:08,760
I think the things like the flash crash in the world finance system is an example of

439
00:45:08,760 --> 00:45:16,800
kind of like a synchronized thought or something that happens globally. We're not even sure

440
00:45:16,800 --> 00:45:21,640
why it happened. And we will never know. And I think those are the kinds of things that

441
00:45:21,640 --> 00:45:22,640
we'll see more of.

442
00:45:22,640 --> 00:45:28,720
Yeah, I agree. Things are changing and changing fast at the planetary level. So a lot of the

443
00:45:28,720 --> 00:45:34,520
impossible things that we have encountered like Wikipedia are because we have a technology

444
00:45:34,520 --> 00:45:42,000
to allow a billion people to collaborate in real time. That's never happened before.

445
00:45:42,000 --> 00:45:48,280
Most of the kind of cool things are happening, social media, all these other things are due

446
00:45:48,280 --> 00:45:55,680
to the fact that we now have this technology to allow collaboration, cooperation in real

447
00:45:55,680 --> 00:45:58,600
time at the planetary scale.

448
00:45:58,600 --> 00:46:03,600
Speaking of cool things, the cover story of the May issue of Wired Magazine is about

449
00:46:03,600 --> 00:46:10,240
this mysterious unicorn startup, Magic Leap, who as stated on the cover of this month,

450
00:46:10,240 --> 00:46:15,000
Wired are on the quest to create a new kind of reality. Now you spent some time with them

451
00:46:15,000 --> 00:46:16,000
and wrote it.

452
00:46:16,000 --> 00:46:17,000
And by the way, I wrote the article.

453
00:46:17,000 --> 00:46:20,480
Yes, I was exactly going to say that you spent some time and wrote that extensive piece

454
00:46:20,480 --> 00:46:25,480
about them. Would you explain to our listeners what exactly Magic Leap is and what did they

455
00:46:25,480 --> 00:46:31,800
do and why are they so important that without a better version of even being available to

456
00:46:31,800 --> 00:46:34,680
the developers, they raise more than a billion dollars?

457
00:46:34,680 --> 00:46:44,680
Yeah, I can't explain the last one. I mean, why people have given them so much money. So

458
00:46:44,680 --> 00:46:50,840
I did get to see the current version of their technology and I saw all the other VR companies

459
00:46:50,840 --> 00:46:59,080
as well. And a lot of the demos and most of the content that was available say at the

460
00:46:59,080 --> 00:47:09,480
first of the year. And Magic Leap, there's two distinct brands, two different categories

461
00:47:09,480 --> 00:47:16,360
of virtual reality. There's the VR, which is you put on the headset, the goggles and

462
00:47:16,360 --> 00:47:24,440
you are immersed into a total world. So everything you see is this synthetically created world.

463
00:47:24,440 --> 00:47:29,560
And then there's what they call MR or AR, Mixed Reality or Augmented Reality. There's

464
00:47:29,560 --> 00:47:34,800
some transitions, MR or AR. And that is where you were kind of a transparent spectacle like

465
00:47:34,800 --> 00:47:40,400
a pair of glasses and you see your room or even outside or your office. And then the

466
00:47:40,400 --> 00:47:49,320
virtual is inserted into or on top of or within the real world that you see. So you can see

467
00:47:49,320 --> 00:47:55,400
virtual objects or you can see virtual people or you can see virtual screens.

468
00:47:55,400 --> 00:48:02,280
So MR and AR are the same thing. MR and AR are basically the same thing. And if you

469
00:48:02,280 --> 00:48:07,760
can do MR or AR, you can do VR because all you have to do is turn the lights in the room

470
00:48:07,760 --> 00:48:14,440
down and fill, not just an object, but you can fill everything in. So technically, VR

471
00:48:14,440 --> 00:48:22,480
is a subset of MR or AR, we'll call it MR. So VR is a subset of MR, meaning that again,

472
00:48:22,480 --> 00:48:32,240
if your technology can do MR really, really well, you can easily do VR too. So technically,

473
00:48:32,240 --> 00:48:41,160
MR is more difficult to do. And magically it's doing MR, which is this augmented mixed

474
00:48:41,160 --> 00:48:47,760
reality where you have a pair of visors or spectacles or glasses, which are mostly kind

475
00:48:47,760 --> 00:48:54,880
of clear. And you can see everything around you, but you have this virtual aspect to it.

476
00:48:54,880 --> 00:49:06,280
And the thing about magic leaf stuff is that it really does appear as if it's there. Now

477
00:49:06,280 --> 00:49:14,960
the it, the image is there's no pixels that you can detect. It's pretty stationary, but

478
00:49:14,960 --> 00:49:24,440
the lighting is off. And let me say this is that to really pass for that as to be really

479
00:49:24,440 --> 00:49:30,320
real, the virtual object would have to be lit with the same light that's in the room

480
00:49:30,320 --> 00:49:36,720
that you're in. And that would require another level of computation of detecting and measuring

481
00:49:36,720 --> 00:49:42,360
all the light in the room as you move around and reproducing that entire lighting onto

482
00:49:42,360 --> 00:49:47,720
the virtual object. That does not happen right now. So what that means, and there's a couple

483
00:49:47,720 --> 00:49:54,680
other glitches or things to overcome, what that means is that what you see today in the

484
00:49:54,680 --> 00:50:00,640
MR is a artificial looking, it's an artificial thing. It doesn't look real, but it looks

485
00:50:00,640 --> 00:50:07,520
really present. So its presence is absolutely real. It's like having, you know, so there's

486
00:50:07,520 --> 00:50:13,760
often there's a fantasy thing, you know, it's a baby elephant or it's a little robot thing,

487
00:50:13,760 --> 00:50:18,800
or it's Mickey Mouse or it's a person, but they don't look entirely real in that room

488
00:50:18,800 --> 00:50:25,960
because the lighting is off among other things. But their presence is absolutely real. You're

489
00:50:25,960 --> 00:50:32,000
totally convinced. And that's the same thing with VR is even in the cartoon world, you

490
00:50:32,000 --> 00:50:37,920
know that it can't really be real world, but you really feel present that presence is real.

491
00:50:37,920 --> 00:50:45,720
And so that real presence is sort of the key because what's happening with VR and MR is

492
00:50:45,720 --> 00:50:51,680
that what you're getting is we're moving from internet of information to internet of experiences.

493
00:50:51,680 --> 00:50:58,640
So the experiences are real. They're incredibly vivid. They feel you feel them rather than

494
00:50:58,640 --> 00:51:04,880
know them. And there's a huge difference there between knowing things like information and

495
00:51:04,880 --> 00:51:11,360
feeling them. And that shift is going to be very, very important. I think VR the most

496
00:51:11,360 --> 00:51:15,720
important, the most interesting things in VR are not other landscapes, not other objects,

497
00:51:15,720 --> 00:51:20,000
but other people. And I think VR is going to be the most social of all the social media

498
00:51:20,000 --> 00:51:21,000
that we have.

499
00:51:21,000 --> 00:51:23,000
Is there any audio aspect to what they do as well?

500
00:51:23,000 --> 00:51:27,800
Oh, yeah, absolutely. So there's actually 3D or, you know, by neuro or audio. So it's

501
00:51:27,800 --> 00:51:36,720
not stereo, it's beyond stereo. Because to do true 3D audio, the audio has to change

502
00:51:36,720 --> 00:51:41,400
depending on where you are as you move around the room. So it's not just that you have a

503
00:51:41,400 --> 00:51:48,880
sense of three dimensions. It's a sense that you have like four dimensions. It's X. So

504
00:51:48,880 --> 00:51:49,880
it's changing.

505
00:51:49,880 --> 00:51:50,880
Motion tracking, basically.

506
00:51:50,880 --> 00:51:58,720
Yeah, right. Motion tracking. So they have, there's bionary recording, which is a different

507
00:51:58,720 --> 00:52:04,160
way of recording things so that you actually have a full, true three-dimensional recording

508
00:52:04,160 --> 00:52:08,480
that you can actually navigate through. And that's a large part of it. And it turns out

509
00:52:08,480 --> 00:52:15,480
that actually your hands, the tactile, having hands touching, moving your whole body, having

510
00:52:15,480 --> 00:52:25,080
vibrations come back is 50% or more than 50% of the total sensation or this total illusion

511
00:52:25,080 --> 00:52:30,200
of being there. And that that is a huge part in a lot of the, some of the new arcades,

512
00:52:30,200 --> 00:52:36,040
the VR arcades. Take advantage of that by, you know, giving you a vest that you wear

513
00:52:36,040 --> 00:52:44,320
in gloves and other things where you can actually feel your whole body is there. And then that

514
00:52:44,320 --> 00:52:50,320
transportation is incredible. You really are somewhere different.

515
00:52:50,320 --> 00:52:55,440
How can someone write for Wired Magazine?

516
00:52:55,440 --> 00:53:03,440
I think the general path, and this is true not just for Wired, is with small stuff, start

517
00:53:03,440 --> 00:53:11,080
writing small things. So I think generally most of the writers who've come to Wired have

518
00:53:11,080 --> 00:53:17,360
been writing somewhere else first. So they either have a blog, they have an audience,

519
00:53:17,360 --> 00:53:23,840
or they're writing little things, probably the digital side first. They're doing little

520
00:53:23,840 --> 00:53:28,800
short things. And they are noticed and they move on to longer things that seems to be

521
00:53:28,800 --> 00:53:31,160
the general pattern.

522
00:53:31,160 --> 00:53:36,360
We've been talking to Kevin Kelly, his upcoming book, Inevitable, understanding the 12 technological

523
00:53:36,360 --> 00:53:40,560
forces that will shape our future. I'm going to ask you the final question that I ask all

524
00:53:40,560 --> 00:53:46,320
my guests. And that's, if you come across an intelligent alien from a different civilization,

525
00:53:46,320 --> 00:53:49,960
what would you say as the worst thing that humanity has done? And what would you say

526
00:53:49,960 --> 00:53:54,480
as humanity's greatest achievement?

527
00:53:54,480 --> 00:54:04,200
I think the worst thing that we've done would have to be the enslavement and the dehumanization

528
00:54:04,200 --> 00:54:18,800
of races of people who we declared were not really us. So that kind of completely artificial

529
00:54:18,800 --> 00:54:28,440
arbitrary separation. And I would say, depending on the circumstances, but the best thing we

530
00:54:28,440 --> 00:54:33,440
would have done would have been meeting an alien in another planet. I mean, that's going

531
00:54:33,440 --> 00:54:39,400
to be an achievement that would be quite remarkable. I don't think we're going to meet them on

532
00:54:39,400 --> 00:54:45,000
our own planet. I think there's just going to be a restriction for AIs not to interfere

533
00:54:45,000 --> 00:54:51,720
with us, so they're not going to reveal themselves. But I think our ability to make contact with

534
00:54:51,720 --> 00:55:20,720
another one will be our greatest achievement to that point.

