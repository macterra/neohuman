1
00:00:00,000 --> 00:00:01,600
Why aren't they talking about AI?

2
00:00:01,600 --> 00:00:05,480
It just doesn't matter enough to the constituents, right?

3
00:00:05,480 --> 00:00:08,840
Why are they talking about, name your topic, sir.

4
00:00:08,840 --> 00:00:09,920
Name your topic.

5
00:00:09,920 --> 00:00:11,760
I mean, the most arbitrary things

6
00:00:11,760 --> 00:00:13,840
become giant friction points

7
00:00:13,840 --> 00:00:16,080
and then they get in all the speeches.

8
00:00:16,080 --> 00:00:18,080
Is it because of their inherent meaning

9
00:00:18,080 --> 00:00:20,040
and some platonic reality?

10
00:00:20,040 --> 00:00:21,240
No, no, no.

11
00:00:21,240 --> 00:00:23,480
It's simply because they resonate with the constituency.

12
00:00:23,480 --> 00:00:25,880
So we can expect AI to land

13
00:00:25,960 --> 00:00:30,720
on the sort of political speech table

14
00:00:30,720 --> 00:00:33,840
when it matters to the people who are being spoken to.

15
00:00:41,720 --> 00:00:45,160
Hello and welcome to the 80th episode of New Human Podcast.

16
00:00:45,160 --> 00:00:50,120
I'm a Gabbahari etiologist on Instagram and Twitter

17
00:00:50,120 --> 00:00:52,120
and you can follow the show on liveinlumbo.com,

18
00:00:52,120 --> 00:00:55,440
iTunes, YouTube, BitChute, and at some point on Spotify.

19
00:00:55,440 --> 00:00:58,120
And today with me, I have Daniel Affigiello.

20
00:00:58,120 --> 00:01:00,320
Welcome to New Human Podcast, Daniel.

21
00:01:00,320 --> 00:01:01,600
Hey, glad to be here, brother.

22
00:01:01,600 --> 00:01:03,920
Yeah, let's start with your background,

23
00:01:03,920 --> 00:01:05,600
the work you've done, the lives you've lived

24
00:01:05,600 --> 00:01:08,160
and what are you mainly focused on now these days?

25
00:01:08,160 --> 00:01:13,160
Yeah, so I began sort of not in the computer science space.

26
00:01:13,840 --> 00:01:15,960
So right now I'm the founder and head of research

27
00:01:15,960 --> 00:01:18,440
at a company called Emerge Artificial Intelligence Research

28
00:01:18,440 --> 00:01:20,280
that I founded some three years ago

29
00:01:20,280 --> 00:01:22,240
when I sold my last business.

30
00:01:22,240 --> 00:01:24,200
So at Emerge, our work is really

31
00:01:24,240 --> 00:01:27,760
mapping the return on investment of AI in the private sector.

32
00:01:27,760 --> 00:01:30,240
So we look at banking and financial services,

33
00:01:30,240 --> 00:01:31,920
we look at pharmaceutical,

34
00:01:31,920 --> 00:01:33,920
we look at the range of AI use cases

35
00:01:33,920 --> 00:01:36,160
and which of them are actually delivering value

36
00:01:36,160 --> 00:01:39,280
and we provide research products and data and advisory,

37
00:01:39,280 --> 00:01:41,120
kind of like a Forrester or a Gardner,

38
00:01:41,120 --> 00:01:44,120
to big companies who want to actually see a return

39
00:01:44,120 --> 00:01:45,200
from artificial intelligence.

40
00:01:45,200 --> 00:01:47,720
We do a good deal of work in the public sector as well,

41
00:01:47,720 --> 00:01:49,760
mostly in defense and security,

42
00:01:49,760 --> 00:01:52,160
financial crime, surveillance, things along those lines,

43
00:01:52,160 --> 00:01:54,080
but primarily in the private sector,

44
00:01:54,960 --> 00:01:56,720
it's going to be financial services, life sciences.

45
00:01:56,720 --> 00:01:59,000
So market research is the name of the game,

46
00:01:59,000 --> 00:02:00,840
but you want me to get into the old school stuff.

47
00:02:00,840 --> 00:02:03,240
Yeah, it's interesting to have context

48
00:02:03,240 --> 00:02:06,680
where the perspective of each individual comes from.

49
00:02:06,680 --> 00:02:09,240
I mean, I want to talk about Emerge,

50
00:02:09,240 --> 00:02:10,560
I want to talk about AI,

51
00:02:10,560 --> 00:02:13,560
but it's also interesting to know who is representing

52
00:02:13,560 --> 00:02:16,480
and expressing these ideas about AI.

53
00:02:16,480 --> 00:02:18,040
For sure.

54
00:02:18,040 --> 00:02:19,920
Well, I'll give you, I guess, a little bit of backdrop.

55
00:02:19,920 --> 00:02:23,160
So I'm not exactly from a tech background at all,

56
00:02:23,320 --> 00:02:25,560
I'm from a 4,000 person town in Rhode Island

57
00:02:25,560 --> 00:02:26,880
called Wakefield, Rhode Island.

58
00:02:26,880 --> 00:02:28,040
You've never heard of it before,

59
00:02:28,040 --> 00:02:29,760
and you will from this point until you're deaf,

60
00:02:29,760 --> 00:02:31,280
never hear of it again.

61
00:02:31,280 --> 00:02:33,400
It's a very small place.

62
00:02:33,400 --> 00:02:34,920
It's a quaint place, a beautiful place,

63
00:02:34,920 --> 00:02:36,720
but it's not a place I could stay,

64
00:02:36,720 --> 00:02:38,600
given my objectives in life.

65
00:02:38,600 --> 00:02:40,440
So that's where I'm from,

66
00:02:40,440 --> 00:02:42,680
and the way that I paid for college,

67
00:02:42,680 --> 00:02:44,600
and ultimately for Ivy League graduate school

68
00:02:44,600 --> 00:02:45,960
at the University of Pennsylvania,

69
00:02:45,960 --> 00:02:47,720
where I studied cognitive science,

70
00:02:47,720 --> 00:02:49,920
and my focus was on skill development

71
00:02:49,920 --> 00:02:51,120
and skill acquisition.

72
00:02:51,160 --> 00:02:53,920
So the cognitive science of acquiring skills quickly.

73
00:02:53,920 --> 00:02:55,040
The reason I went in for that

74
00:02:55,040 --> 00:02:56,560
is because I was in combat sports.

75
00:02:56,560 --> 00:02:57,680
So the way I was paying the bills

76
00:02:57,680 --> 00:02:58,880
was running a martial arts gym.

77
00:02:58,880 --> 00:03:00,360
So I was training fighters.

78
00:03:00,360 --> 00:03:02,120
We had some fitness classes,

79
00:03:02,120 --> 00:03:04,800
but it was mostly mixed martial arts and Brazilian jiu-jitsu.

80
00:03:04,800 --> 00:03:06,480
So I have a black belt in Brazilian jiu-jitsu.

81
00:03:06,480 --> 00:03:08,400
I've trained extensively, given seminars

82
00:03:08,400 --> 00:03:11,040
around the United States and even in Brazil,

83
00:03:11,040 --> 00:03:12,920
and won a national tournament,

84
00:03:12,920 --> 00:03:14,880
and did a lot of competing in my day.

85
00:03:14,880 --> 00:03:16,640
And so that's how I was kind of paying the bills.

86
00:03:16,640 --> 00:03:18,960
And so I was really interested in what does it look like

87
00:03:18,960 --> 00:03:21,880
to improve faster from an athlete perspective,

88
00:03:21,880 --> 00:03:24,120
training my own fighters, training myself.

89
00:03:24,120 --> 00:03:25,880
And so that's what I went to graduate school for.

90
00:03:25,880 --> 00:03:28,720
And when I was there, this is back in 2011,

91
00:03:28,720 --> 00:03:30,200
I was getting capped on the shoulder that,

92
00:03:30,200 --> 00:03:32,640
hey, Dan, all this human learning stuff,

93
00:03:32,640 --> 00:03:34,200
all this neural stuff,

94
00:03:34,200 --> 00:03:36,080
they're kind of doing that with computers.

95
00:03:36,080 --> 00:03:38,080
And this is the really early days of ImageNet,

96
00:03:38,080 --> 00:03:40,240
when ImageNet started becoming exciting.

97
00:03:40,240 --> 00:03:41,680
This is also the very early days

98
00:03:41,680 --> 00:03:43,960
of when they were applying natural language processing

99
00:03:43,960 --> 00:03:44,800
to Twitter data.

100
00:03:44,800 --> 00:03:46,480
There was something called Google Zeitgeist

101
00:03:46,480 --> 00:03:48,960
that UPenn was involved with at the time,

102
00:03:48,960 --> 00:03:52,360
deriving sentiment from different factors

103
00:03:52,360 --> 00:03:54,160
in the social media ether,

104
00:03:54,160 --> 00:03:56,640
and kind of determining how that correlated

105
00:03:56,640 --> 00:03:57,640
to different cities,

106
00:03:57,640 --> 00:03:59,200
and how weather affected different cities,

107
00:03:59,200 --> 00:04:01,160
and the moods of people, and the sicknesses of people,

108
00:04:01,160 --> 00:04:02,160
and things along those lines.

109
00:04:02,160 --> 00:04:06,160
So when I got out of grad school with the psych degree,

110
00:04:06,160 --> 00:04:08,480
I kind of became convinced this AI stuff,

111
00:04:08,480 --> 00:04:09,480
looking at the near term,

112
00:04:09,480 --> 00:04:11,400
and then also reading people like Nick Bostrom

113
00:04:11,400 --> 00:04:14,080
and Ben Goertzel, and learning about the long term.

114
00:04:14,080 --> 00:04:16,080
I was lucky enough to interview Ben Goertzel

115
00:04:16,320 --> 00:04:18,160
and Bostrom pretty much right out of grad school.

116
00:04:18,160 --> 00:04:21,200
I talked to all the major players in the AGI space.

117
00:04:21,200 --> 00:04:22,200
I became very convinced

118
00:04:22,200 --> 00:04:23,560
that in both the near and the long term,

119
00:04:23,560 --> 00:04:25,920
this was an exceedingly important technology,

120
00:04:25,920 --> 00:04:28,200
and that my understanding from a conceptual level

121
00:04:28,200 --> 00:04:29,680
in the cognitive sciences,

122
00:04:29,680 --> 00:04:31,520
I should just be able to transfer that over.

123
00:04:31,520 --> 00:04:34,360
So I got right into really studying this stuff,

124
00:04:34,360 --> 00:04:36,240
and then talking to the leading figures there,

125
00:04:36,240 --> 00:04:38,440
and eventually started a company.

126
00:04:38,440 --> 00:04:39,280
Very cool.

127
00:04:39,280 --> 00:04:41,520
I mean, you must have started with interest in the mind,

128
00:04:41,520 --> 00:04:43,120
getting into martial art,

129
00:04:43,120 --> 00:04:45,520
the kind of martial art that you got into.

130
00:04:45,520 --> 00:04:48,320
It's not that, hey, let's just go out and kick some ass.

131
00:04:48,320 --> 00:04:50,680
It's more about finding yourself, right?

132
00:04:50,680 --> 00:04:53,600
Through that art of battle.

133
00:04:54,520 --> 00:04:56,200
Yeah, there's something to be said of that.

134
00:04:56,200 --> 00:04:58,840
I don't personally overly spiritualize martial arts.

135
00:04:58,840 --> 00:05:00,480
I think some people will play it up.

136
00:05:00,480 --> 00:05:02,040
They'll be like, oh yes, I'm a martial artist,

137
00:05:02,040 --> 00:05:04,080
and so you should presume that I am wise.

138
00:05:04,080 --> 00:05:05,040
I actually don't do that.

139
00:05:05,040 --> 00:05:06,040
I think if I am wise,

140
00:05:06,040 --> 00:05:07,960
it's because I've read Montaigne and Plutarch,

141
00:05:07,960 --> 00:05:10,800
more than because I've choked men unconscious.

142
00:05:10,800 --> 00:05:12,640
But I've done both,

143
00:05:12,640 --> 00:05:16,000
and I can say that certainly part of the fascination

144
00:05:16,000 --> 00:05:19,440
of the mind is from beginning with martial arts,

145
00:05:19,440 --> 00:05:21,360
and being able to see how different people learn,

146
00:05:21,360 --> 00:05:22,920
and also see how I learned, right?

147
00:05:22,920 --> 00:05:25,120
To go from, you know, in Rhode Island,

148
00:05:25,120 --> 00:05:26,480
there's not that many good training partners,

149
00:05:26,480 --> 00:05:29,720
so to win national tournaments is actually pretty difficult.

150
00:05:29,720 --> 00:05:32,920
And I had to figure out how to go about drilling,

151
00:05:32,920 --> 00:05:34,680
figure out what kind of training regimens

152
00:05:34,680 --> 00:05:35,520
to build for myself.

153
00:05:35,520 --> 00:05:37,400
And it wasn't just what kind of weights to lift.

154
00:05:37,400 --> 00:05:38,880
In fact, I wasn't doing much of that.

155
00:05:38,880 --> 00:05:40,960
It was what kind of skills do I want to acquire?

156
00:05:40,960 --> 00:05:43,200
And so that forced me to think about,

157
00:05:43,200 --> 00:05:45,160
well, how does this thing upstairs actually work?

158
00:05:45,160 --> 00:05:47,160
And how can I learn as quickly as possible

159
00:05:47,160 --> 00:05:49,960
to really perform and also have my competitors

160
00:05:49,960 --> 00:05:52,320
do very well in tournaments too?

161
00:05:52,320 --> 00:05:54,040
So yeah, it was less spiritual,

162
00:05:54,040 --> 00:05:56,400
but it certainly was psychological in martial arts,

163
00:05:56,400 --> 00:05:59,520
and that's what got me to UPenn and ultimately into AI.

164
00:05:59,520 --> 00:06:01,000
Yeah, in addition to spirituality,

165
00:06:01,000 --> 00:06:04,160
it's also that you had choked people unconscious,

166
00:06:04,160 --> 00:06:06,880
but I assume that you were also choked unconscious yourself.

167
00:06:06,880 --> 00:06:10,240
You were placed on the other side of the equation.

168
00:06:10,240 --> 00:06:11,680
Sure, sure did.

169
00:06:11,680 --> 00:06:13,160
I mean, everybody weighs more than me.

170
00:06:13,160 --> 00:06:14,200
I'm a pretty small fella,

171
00:06:14,200 --> 00:06:15,800
so I did a lot of competitions

172
00:06:15,800 --> 00:06:17,280
against very, very big opponents.

173
00:06:17,280 --> 00:06:18,440
In fact, if you go on Google,

174
00:06:18,440 --> 00:06:21,000
you type in Dan Fijella versus the Giant,

175
00:06:21,000 --> 00:06:23,320
you'll see me in a jujitsu match

176
00:06:23,320 --> 00:06:26,600
against a guy that weighs 80 pounds, 100 pounds more than me

177
00:06:26,600 --> 00:06:27,920
who's actually a UFC fighter.

178
00:06:27,920 --> 00:06:28,760
His name is Pat Walsh.

179
00:06:28,760 --> 00:06:31,640
So this guy fought in the cage with other 220 pound men.

180
00:06:31,640 --> 00:06:33,800
I walk around at about a buck 25.

181
00:06:33,800 --> 00:06:38,280
So definitely, I didn't beat all those guys.

182
00:06:38,280 --> 00:06:39,120
That was a good match.

183
00:06:39,120 --> 00:06:40,120
I'll fortunately beat that guy.

184
00:06:40,960 --> 00:06:44,920
But yeah, I certainly had my ears boxed pretty good.

185
00:06:44,920 --> 00:06:48,080
You can see my cauliflower ear is pretty rough.

186
00:06:48,080 --> 00:06:50,480
And I've certainly, you know,

187
00:06:50,480 --> 00:06:52,400
I'm gonna have knee problems when I'm older.

188
00:06:52,400 --> 00:06:53,240
That's just a fact.

189
00:06:53,240 --> 00:06:54,680
I'm gonna have shoulder problems when I'm older.

190
00:06:54,680 --> 00:06:56,040
That's just a fact.

191
00:06:56,040 --> 00:06:58,160
But right now, it doesn't keep me from doing market research

192
00:06:58,160 --> 00:06:59,520
and what I love, so, you know.

193
00:06:59,520 --> 00:07:02,640
Yeah, it just seems that that experience of humility

194
00:07:02,640 --> 00:07:04,560
and empathy that you create

195
00:07:04,560 --> 00:07:07,000
when you're on the other side of something like that

196
00:07:07,000 --> 00:07:09,280
is also helpful when you're working on,

197
00:07:09,320 --> 00:07:11,160
let's say, artificial intelligence,

198
00:07:11,160 --> 00:07:14,880
which is a big difference from what is being written down

199
00:07:14,880 --> 00:07:16,640
on paper and in academia

200
00:07:16,640 --> 00:07:19,600
than what it's being implemented within the society,

201
00:07:19,600 --> 00:07:22,040
especially the society right now today

202
00:07:22,040 --> 00:07:23,760
that a lot of people are freaked out

203
00:07:23,760 --> 00:07:26,360
by just the name artificial intelligence

204
00:07:26,360 --> 00:07:29,440
and in a lot of aspect for right reasons, right?

205
00:07:29,440 --> 00:07:31,960
So I'm trying to get into,

206
00:07:31,960 --> 00:07:34,240
and we've talked about this a lot in this podcast,

207
00:07:34,240 --> 00:07:36,240
that what you put on the paper,

208
00:07:36,240 --> 00:07:39,200
it might seem awesome and sounds great,

209
00:07:40,120 --> 00:07:42,840
but there's a big difference between what sounds good

210
00:07:42,840 --> 00:07:44,400
and sounds sensible on the paper

211
00:07:44,400 --> 00:07:47,200
than when it's being implemented within the society

212
00:07:47,200 --> 00:07:49,640
when a lot of people have a problem

213
00:07:49,640 --> 00:07:52,080
even accepting the existence of such a thing

214
00:07:52,080 --> 00:07:54,240
that is going to compete with human intelligence

215
00:07:54,240 --> 00:07:56,800
and human, basically the concept of humanity.

216
00:07:58,080 --> 00:08:00,360
Yeah, I think, you know, in the near term,

217
00:08:00,360 --> 00:08:02,480
there's the job and economic considerations

218
00:08:02,480 --> 00:08:04,760
that are real.

219
00:08:04,760 --> 00:08:06,960
I don't think we're gonna see the wave of automation

220
00:08:07,000 --> 00:08:09,280
across the country in the very, very near term,

221
00:08:09,280 --> 00:08:11,480
but I do foresee in the next 10 years,

222
00:08:11,480 --> 00:08:15,680
we did a very substantial poll in 2015 of AI researchers

223
00:08:15,680 --> 00:08:17,960
and by a wide margin,

224
00:08:17,960 --> 00:08:21,480
the most significant and consistent AI risk

225
00:08:21,480 --> 00:08:24,280
in the next 20 years, this is five years ago,

226
00:08:24,280 --> 00:08:26,120
was the economic and job impact.

227
00:08:26,120 --> 00:08:29,000
So I think that seems like a reasonable fear,

228
00:08:29,000 --> 00:08:30,840
but I think to your point,

229
00:08:30,840 --> 00:08:32,800
there is the actual species dominant stuff

230
00:08:32,800 --> 00:08:34,640
if we look 50 years out, 100 years out

231
00:08:34,640 --> 00:08:36,840
that I think is considerable as well.

232
00:08:37,680 --> 00:08:38,520
So yeah, you're right.

233
00:08:38,520 --> 00:08:40,400
I mean, AI in theory and AI in practice,

234
00:08:40,400 --> 00:08:42,680
it's a lot less predictable when it's off the paper.

235
00:08:42,680 --> 00:08:46,360
Social adoption is a big part of any kind of technology

236
00:08:46,360 --> 00:08:47,880
that would progress, right?

237
00:08:47,880 --> 00:08:52,880
So you can recommend certain things to a business

238
00:08:53,200 --> 00:08:57,000
and it might seem all good, a certain kind of a technology,

239
00:08:57,000 --> 00:09:00,000
but when the society is rejecting that kind of technology,

240
00:09:00,000 --> 00:09:02,880
I bring up Google Glass, for example, as an example,

241
00:09:02,880 --> 00:09:04,440
that it was a very practical thing,

242
00:09:04,440 --> 00:09:08,640
well-designed and great, but people didn't like it.

243
00:09:08,640 --> 00:09:09,960
Some because they couldn't get it,

244
00:09:09,960 --> 00:09:12,040
some because they thought, oh, you're taking this,

245
00:09:12,040 --> 00:09:14,000
it jeopardizes my privacy

246
00:09:14,000 --> 00:09:15,120
and you think you're better than me

247
00:09:15,120 --> 00:09:16,720
and all those things, you know?

248
00:09:17,920 --> 00:09:20,520
Yeah, we did a very early interview in 2012

249
00:09:20,520 --> 00:09:23,680
with the head of Engadget at the time.

250
00:09:23,680 --> 00:09:26,120
Engadget is a big consumer tech blog

251
00:09:26,120 --> 00:09:27,680
who talked about his experience

252
00:09:27,680 --> 00:09:29,440
of wearing Google Glass in the subway

253
00:09:29,440 --> 00:09:31,200
and walking his dog and things like that

254
00:09:31,200 --> 00:09:34,040
and just how it wasn't really gonna fit

255
00:09:34,040 --> 00:09:36,480
into the normal mesh of society and the public.

256
00:09:36,480 --> 00:09:38,400
There was in San Francisco?

257
00:09:38,400 --> 00:09:41,320
I forget where Tim is based, Tim Stevens.

258
00:09:41,320 --> 00:09:43,080
This is a great many years ago,

259
00:09:43,080 --> 00:09:44,800
but I suspect it was the Bay Area.

260
00:09:44,800 --> 00:09:46,840
Yeah, because people were getting attacked.

261
00:09:46,840 --> 00:09:49,560
You know, there were videos taken by those,

262
00:09:49,560 --> 00:09:50,720
you know, people who were wearing it

263
00:09:50,720 --> 00:09:52,200
in bars and restaurants

264
00:09:52,200 --> 00:09:53,360
and people were getting attacked

265
00:09:53,360 --> 00:09:54,680
just because of wearing the glass.

266
00:09:54,680 --> 00:09:57,400
Part of it was the argument for privacy,

267
00:09:57,400 --> 00:09:58,840
but the other part was that, hey,

268
00:09:58,840 --> 00:10:01,200
this is not even something we can't buy from the store.

269
00:10:01,200 --> 00:10:03,320
This has to be given to you.

270
00:10:03,320 --> 00:10:06,000
Yeah, so it was like lording over people,

271
00:10:06,000 --> 00:10:07,720
kind of like, ah, I've been granted

272
00:10:07,720 --> 00:10:11,080
this recording intelligence layer

273
00:10:11,080 --> 00:10:11,920
and you don't have it.

274
00:10:11,920 --> 00:10:14,240
There's a certain kind of elitism

275
00:10:14,240 --> 00:10:16,760
that it seems like we can't get away with that.

276
00:10:16,760 --> 00:10:21,080
You know, genetic engineering is another field

277
00:10:21,080 --> 00:10:22,840
that will create this kind of elitism

278
00:10:22,840 --> 00:10:25,160
that, hey, if you can't afford it,

279
00:10:25,160 --> 00:10:28,320
not only financially, but also,

280
00:10:28,320 --> 00:10:30,120
you know, there's a big difference between China,

281
00:10:30,120 --> 00:10:31,360
let's say, and the United States,

282
00:10:31,360 --> 00:10:32,840
and I wanna talk to you about that too,

283
00:10:32,840 --> 00:10:35,080
because you mentioned you're talking to

284
00:10:35,080 --> 00:10:37,520
and you're advising private companies,

285
00:10:37,520 --> 00:10:39,760
but I also see on your website that you've spoken to,

286
00:10:39,760 --> 00:10:42,960
for example, United Nations, Harvard University.

287
00:10:42,960 --> 00:10:46,320
What was, oh, you talked to United Nations,

288
00:10:46,320 --> 00:10:49,000
as I see, about deepfakes, is that right?

289
00:10:49,000 --> 00:10:52,400
Yeah, so we actually, United Nations,

290
00:10:52,400 --> 00:10:54,640
World Bank, Interpol, the OECD,

291
00:10:54,640 --> 00:10:55,680
I mean, we work with-

292
00:10:55,680 --> 00:10:57,680
So beyond the private sector also.

293
00:10:57,680 --> 00:10:59,200
Yeah, these are the most substantial

294
00:10:59,200 --> 00:11:00,960
intergovernmental organizations in the world

295
00:11:01,160 --> 00:11:02,840
who are, luckily, at this point,

296
00:11:02,840 --> 00:11:05,800
at least allocating a certain amount of attention

297
00:11:05,800 --> 00:11:07,320
to artificial intelligence,

298
00:11:07,320 --> 00:11:08,520
and there are certainly initiatives

299
00:11:08,520 --> 00:11:09,960
that I really believe in there.

300
00:11:09,960 --> 00:11:13,040
And not all of those organizations

301
00:11:13,040 --> 00:11:14,880
are our clients, formally, some are.

302
00:11:16,240 --> 00:11:18,480
We've done substantial projects with the World Bank,

303
00:11:18,480 --> 00:11:20,640
for example, but some are just speaking engagements,

304
00:11:20,640 --> 00:11:22,800
because I happen to think it's a very valuable topic

305
00:11:22,800 --> 00:11:24,600
that should be at UN headquarters,

306
00:11:24,600 --> 00:11:26,080
and we were called upon to talk about it.

307
00:11:26,080 --> 00:11:28,840
So deepfakes is a good example of that,

308
00:11:28,840 --> 00:11:30,920
where I'm pretty close with the folks

309
00:11:31,920 --> 00:11:33,720
that run the artificial intelligence and robotics wing

310
00:11:33,720 --> 00:11:35,080
of what is called Unicrit,

311
00:11:35,080 --> 00:11:37,960
which is the crime and justice wing of the United Nations.

312
00:11:37,960 --> 00:11:40,200
I really respect those guys, I think they do a great job,

313
00:11:40,200 --> 00:11:42,840
and they have a thorough understanding of the space.

314
00:11:42,840 --> 00:11:45,800
Not everybody in the intergovernmental domain does.

315
00:11:45,800 --> 00:11:48,880
And they had called us in to essentially create a deepfake

316
00:11:48,880 --> 00:11:51,480
of the head of Unicrit, so a woman by the name of Bettina,

317
00:11:51,480 --> 00:11:54,600
who was the director of that wing of the United Nations.

318
00:11:54,600 --> 00:11:56,080
So we took a video of her,

319
00:11:56,080 --> 00:11:58,480
and then we made her say a bunch of things she never said,

320
00:11:58,480 --> 00:12:01,680
and then also extrapolated that into

321
00:12:01,680 --> 00:12:04,120
how almost anything can be programmatically generated,

322
00:12:04,120 --> 00:12:05,440
and how much better and better

323
00:12:05,440 --> 00:12:07,640
that programmatic generation is becoming,

324
00:12:07,640 --> 00:12:09,560
and what some of the considerations might be.

325
00:12:09,560 --> 00:12:11,880
My personal focus is on the long-term of that.

326
00:12:11,880 --> 00:12:14,200
Yes, political influence, I think, is something.

327
00:12:14,200 --> 00:12:15,520
Media and truth is something.

328
00:12:15,520 --> 00:12:18,800
I think the actual leap in the human experience,

329
00:12:18,800 --> 00:12:19,880
when we can push a button

330
00:12:19,880 --> 00:12:22,680
and have what we wish before our eyes,

331
00:12:22,680 --> 00:12:24,640
the conjuring of what we wish before our eyes,

332
00:12:24,640 --> 00:12:26,880
I think is a much grander transition

333
00:12:26,920 --> 00:12:29,880
than is spoken about in the purely political space.

334
00:12:29,880 --> 00:12:33,560
So I tried to create that stretch for UN leadership there.

335
00:12:33,560 --> 00:12:36,920
So democratization of information is the big picture,

336
00:12:36,920 --> 00:12:37,760
is what you're talking about,

337
00:12:37,760 --> 00:12:39,320
because it's not only deepfakes,

338
00:12:39,320 --> 00:12:41,600
but also, let's say, 3D printing,

339
00:12:41,600 --> 00:12:43,840
that now we have to deal with the very reality

340
00:12:43,840 --> 00:12:45,760
that within a couple of years,

341
00:12:45,760 --> 00:12:49,280
some parts of a gun can be very viably printed

342
00:12:49,280 --> 00:12:51,480
inside of someone's bedroom,

343
00:12:51,480 --> 00:12:54,120
and there's nothing anybody can do about it.

344
00:12:54,120 --> 00:12:56,400
Yeah, my personal expertise

345
00:12:56,400 --> 00:12:57,880
is not in 3D printing.

346
00:12:57,880 --> 00:12:59,760
We've done a couple bits of coverage

347
00:12:59,760 --> 00:13:02,560
on the intersection of what is called

348
00:13:02,560 --> 00:13:03,960
additive manufacturing,

349
00:13:03,960 --> 00:13:06,840
sort of the broad umbrella of 3D printing, and AI,

350
00:13:06,840 --> 00:13:08,520
so where those two mesh.

351
00:13:08,520 --> 00:13:11,360
But making projections around that tech,

352
00:13:11,360 --> 00:13:12,920
I have much less confidence than I do

353
00:13:12,920 --> 00:13:14,920
in, let's say, artificial intelligence

354
00:13:14,920 --> 00:13:16,600
in almost any industry.

355
00:13:16,600 --> 00:13:18,120
But yeah, to your point,

356
00:13:18,120 --> 00:13:19,520
that's part of the transition.

357
00:13:19,520 --> 00:13:22,440
I think what I'm articulating explicitly here

358
00:13:22,480 --> 00:13:26,960
is a broader dynamic of a kind of going in,

359
00:13:26,960 --> 00:13:30,840
so the majority, the bulk of our experience

360
00:13:30,840 --> 00:13:32,600
transitions into virtual spaces.

361
00:13:32,600 --> 00:13:34,880
So you and I are kind of doing that right now.

362
00:13:35,920 --> 00:13:37,800
I do that on my phone a good deal.

363
00:13:37,800 --> 00:13:40,120
We're all on a screen a really good percentage of the time,

364
00:13:40,120 --> 00:13:42,320
and if you look at human life

365
00:13:42,320 --> 00:13:45,360
and human life on screen time as a graph here,

366
00:13:45,360 --> 00:13:49,400
I mean, you don't have to be a rocket scientist.

367
00:13:49,440 --> 00:13:52,440
And I think that when we can call forth

368
00:13:52,440 --> 00:13:54,120
the kinds of experiences we want,

369
00:13:54,120 --> 00:13:56,520
whether it's a relaxing background to do our work,

370
00:13:56,520 --> 00:14:00,760
whether it's a movie that's created just for us, right?

371
00:14:00,760 --> 00:14:02,280
I wanna learn about the French Revolution,

372
00:14:02,280 --> 00:14:05,080
but I wanna learn about it through the eyes of St. Just,

373
00:14:05,080 --> 00:14:06,720
and I don't really care that much

374
00:14:06,720 --> 00:14:08,840
about the post-revolutionary Bonaparte stuff.

375
00:14:08,840 --> 00:14:11,680
I just wanna go right up until St. Just death, that's it,

376
00:14:11,680 --> 00:14:12,760
and I wanna look through his eyes.

377
00:14:12,760 --> 00:14:14,680
When a movie can be played

378
00:14:14,680 --> 00:14:16,440
per my preferences in creation

379
00:14:16,440 --> 00:14:17,800
and based on my past learning

380
00:14:18,080 --> 00:14:19,760
based on my current responsiveness, right?

381
00:14:19,760 --> 00:14:21,560
Am I attentive or is it boring me

382
00:14:21,560 --> 00:14:24,240
being able to have that generated in real time?

383
00:14:24,240 --> 00:14:26,040
I think that might be 15 years off,

384
00:14:26,040 --> 00:14:27,920
but I think that we get to a point

385
00:14:27,920 --> 00:14:31,280
where the compellingness of virtual ecosystems

386
00:14:31,280 --> 00:14:35,120
will be so amazing in terms of fulfilling drives

387
00:14:35,120 --> 00:14:37,880
for curiosity, creativity, for love,

388
00:14:37,880 --> 00:14:40,640
eventually with haptics that might involve sex.

389
00:14:40,640 --> 00:14:42,240
Lord knows how that's gonna evolve.

390
00:14:42,240 --> 00:14:43,680
That's not exactly my focus area,

391
00:14:43,680 --> 00:14:45,920
but I do think there's gonna be transitions there too,

392
00:14:45,920 --> 00:14:48,480
and I think that as the body becomes a husk

393
00:14:48,480 --> 00:14:51,880
and as most of what we wish in its highest form,

394
00:14:51,880 --> 00:14:53,720
right now, if you ask me, Dan,

395
00:14:53,720 --> 00:14:55,400
what is the most refreshing, rewarding,

396
00:14:55,400 --> 00:14:56,640
lovely thing that you could do,

397
00:14:56,640 --> 00:14:59,360
I would tell you it is walking in nature,

398
00:14:59,360 --> 00:15:02,720
reading actual papyrus in a really, really good book,

399
00:15:04,720 --> 00:15:06,080
like not Kindle, I mean like paper,

400
00:15:06,080 --> 00:15:08,760
I don't have ancient texts in my house,

401
00:15:08,760 --> 00:15:12,880
but I'm not, even if I was-

402
00:15:12,920 --> 00:15:14,960
I was gonna ask, you can't read papyrus?

403
00:15:15,840 --> 00:15:20,080
Even if I was exorbitantly wealthy,

404
00:15:20,080 --> 00:15:23,760
I would not become such a profligate

405
00:15:23,760 --> 00:15:25,880
in my collecting of texts like that,

406
00:15:25,880 --> 00:15:27,840
but I like me a good old book,

407
00:15:27,840 --> 00:15:31,520
and I like to breathe in the pine,

408
00:15:31,520 --> 00:15:35,200
and I like to see the sunset and those sorts of things,

409
00:15:35,200 --> 00:15:39,560
but I am not above the idea that within 15 years,

410
00:15:39,560 --> 00:15:42,240
a programmatically generated permutation of that,

411
00:15:42,240 --> 00:15:43,760
even the wind through my hair

412
00:15:43,760 --> 00:15:45,600
in a programmatically generated sense

413
00:15:45,600 --> 00:15:48,960
might be astronomically more refreshing,

414
00:15:48,960 --> 00:15:50,520
because it would be hyper-calibrated

415
00:15:50,520 --> 00:15:53,000
to all the very best experiences of that in the past,

416
00:15:53,000 --> 00:15:55,840
and that is in fact the transition I'm talking about.

417
00:15:55,840 --> 00:16:00,200
I wrote an article called Lotus Eaters and World Eaters,

418
00:16:00,200 --> 00:16:01,960
so Lotus Eaters and World Eaters.

419
00:16:01,960 --> 00:16:04,480
If Googled, that article would come up,

420
00:16:04,480 --> 00:16:06,080
and this is about the bigger transition.

421
00:16:06,080 --> 00:16:08,920
So for me, I think democratization of information

422
00:16:08,920 --> 00:16:10,400
is a very big part of it.

423
00:16:10,400 --> 00:16:13,040
I think that going in is the dynamic

424
00:16:13,040 --> 00:16:15,160
that I consider to be astronomically disruptive,

425
00:16:15,160 --> 00:16:16,920
and I think essentially a borderline

426
00:16:16,920 --> 00:16:18,360
no one is talking about.

427
00:16:18,360 --> 00:16:19,200
That's very interesting,

428
00:16:19,200 --> 00:16:21,760
because recently we've been talking about

429
00:16:21,760 --> 00:16:24,800
when people are talking about AI, artificial intelligence,

430
00:16:24,800 --> 00:16:27,120
and I like personally to think of that A

431
00:16:27,120 --> 00:16:28,960
as augmented intelligence as well,

432
00:16:28,960 --> 00:16:33,840
that you don't really have to reverse engineer

433
00:16:33,840 --> 00:16:36,520
and rebuild the entire brain or an intelligence.

434
00:16:36,520 --> 00:16:38,600
You can use it as a mesh net, basically,

435
00:16:38,600 --> 00:16:40,600
or exoskeleton, right?

436
00:16:40,600 --> 00:16:43,640
But the argument that I'm hearing is that AI

437
00:16:43,640 --> 00:16:47,520
is focused on the thinking mind, on the analytical mind,

438
00:16:47,520 --> 00:16:49,880
on the left brain, basically.

439
00:16:49,880 --> 00:16:53,200
The right brain and instincts

440
00:16:53,200 --> 00:16:56,480
and things we cannot really describe by words,

441
00:16:56,480 --> 00:16:58,360
that is something that we can't even begin

442
00:16:58,360 --> 00:17:01,480
to understand ourselves, let alone trying to replicate

443
00:17:01,480 --> 00:17:04,680
that with any kind of a machine intelligence.

444
00:17:04,680 --> 00:17:07,880
But that is a huge part of what you're talking about,

445
00:17:07,880 --> 00:17:12,280
to merging, basically, within blurring this line

446
00:17:12,280 --> 00:17:14,000
of what is real and what is not real

447
00:17:14,000 --> 00:17:16,600
further and further in the coming years.

448
00:17:16,600 --> 00:17:21,600
Yeah, and I would say, so I may not describe it similarly.

449
00:17:22,640 --> 00:17:24,400
I think it's an interesting distinction,

450
00:17:24,400 --> 00:17:25,400
the left and right brain.

451
00:17:25,400 --> 00:17:29,840
I would say that certainly at an emotive level,

452
00:17:29,840 --> 00:17:31,760
I don't think we're able to replicate,

453
00:17:31,760 --> 00:17:32,720
to the best of our knowledge,

454
00:17:32,720 --> 00:17:34,920
we're not able to replicate sentience in AI,

455
00:17:34,920 --> 00:17:37,200
although we did do a pretty extensive poll

456
00:17:37,200 --> 00:17:38,920
of artificial intelligence researchers

457
00:17:38,920 --> 00:17:40,680
around when sentience might be possible,

458
00:17:40,680 --> 00:17:42,040
something like 2060.

459
00:17:42,040 --> 00:17:43,160
Who knows if they're right, right?

460
00:17:43,160 --> 00:17:44,840
Nobody knows if anybody's right.

461
00:17:44,840 --> 00:17:46,440
But I'm curious about the topic.

462
00:17:46,440 --> 00:17:48,640
Consciousness, I think, is a morally worthy thing.

463
00:17:48,640 --> 00:17:50,240
It might be the morally worthy thing,

464
00:17:50,240 --> 00:17:52,880
but that's its own conversational threat.

465
00:17:52,880 --> 00:17:54,600
But right and left brain, I think,

466
00:17:54,600 --> 00:17:56,640
it's tough to say where AI actually sits,

467
00:17:56,640 --> 00:17:59,640
because, for example, if we look at

468
00:17:59,640 --> 00:18:01,320
the programmatically generated stuff,

469
00:18:01,320 --> 00:18:03,680
in the future, let's say 10 years from now,

470
00:18:03,680 --> 00:18:07,440
if you are a designer of logos, like company logos,

471
00:18:07,440 --> 00:18:10,160
you will not just take 15 hours

472
00:18:10,160 --> 00:18:11,880
to craft one really good logo

473
00:18:11,880 --> 00:18:13,200
and then take another 15 hours

474
00:18:13,200 --> 00:18:14,920
to craft another really good logo.

475
00:18:14,920 --> 00:18:19,920
You may begin with simply a verbalized

476
00:18:20,800 --> 00:18:24,880
or set of checkboxes, set of configurable features,

477
00:18:24,880 --> 00:18:27,600
like what are the values we want to convey,

478
00:18:27,600 --> 00:18:29,520
what are the values we stand for as a company,

479
00:18:29,520 --> 00:18:30,960
what kind of color palettes are we thinking

480
00:18:30,960 --> 00:18:32,080
might be good, whatever,

481
00:18:32,080 --> 00:18:36,160
and have a programmatically generated plethora of logos

482
00:18:36,160 --> 00:18:37,840
that are already stacked and ranked

483
00:18:37,840 --> 00:18:39,600
based on what we would presume

484
00:18:39,600 --> 00:18:41,720
humans would respond to most favorably

485
00:18:41,720 --> 00:18:43,520
based on the values that we had set before.

486
00:18:43,520 --> 00:18:45,400
Now, we would need a really big data set

487
00:18:45,400 --> 00:18:47,360
of people looking at different logos

488
00:18:47,360 --> 00:18:48,560
and correlating them with values,

489
00:18:48,560 --> 00:18:49,720
and we're not there yet.

490
00:18:49,720 --> 00:18:52,960
But if nothing else, we could start with a plethora,

491
00:18:52,960 --> 00:18:55,240
and we could take individual permutations,

492
00:18:55,240 --> 00:18:57,240
determine some tweaks, determine some adjustments,

493
00:18:57,240 --> 00:19:00,000
and create another plethora off of that one permutation,

494
00:19:00,040 --> 00:19:02,720
and in so doing, generate just generally

495
00:19:02,720 --> 00:19:05,560
better and more ideas, better and more ideas.

496
00:19:05,560 --> 00:19:07,800
Same with architecture, same with fashion,

497
00:19:07,800 --> 00:19:10,320
same with textiles, same with name your damn

498
00:19:10,320 --> 00:19:12,320
creative domain, creating beats.

499
00:19:12,320 --> 00:19:14,120
You want to create music for the elevator,

500
00:19:14,120 --> 00:19:15,560
you want to create rap music.

501
00:19:15,560 --> 00:19:17,200
I mean, being able to just have

502
00:19:17,200 --> 00:19:18,600
programmatically generated beats

503
00:19:18,600 --> 00:19:19,760
that we know are going to hit,

504
00:19:19,760 --> 00:19:20,960
that we know are going to respond,

505
00:19:20,960 --> 00:19:22,240
that people are going to respond well to.

506
00:19:22,240 --> 00:19:24,000
We might train it off of Spotify data

507
00:19:24,000 --> 00:19:26,000
in terms of what's most popular right now.

508
00:19:26,000 --> 00:19:28,760
We might train it off of whatever we want to do.

509
00:19:28,760 --> 00:19:31,400
This sort of extension of what you might refer to

510
00:19:31,400 --> 00:19:34,120
as the left brain will eventually become the norm

511
00:19:34,120 --> 00:19:35,480
in a great many creative fields.

512
00:19:35,480 --> 00:19:36,920
Now, is that going to be 10 years?

513
00:19:36,920 --> 00:19:37,840
Is that going to be 15 years?

514
00:19:37,840 --> 00:19:40,080
I can't exactly tell you, but I will say

515
00:19:40,080 --> 00:19:41,960
there was a time where as a graphic designer,

516
00:19:41,960 --> 00:19:43,600
you didn't need to know how to use a computer.

517
00:19:43,600 --> 00:19:46,680
And let me tell you, my good man, times have changed.

518
00:19:46,680 --> 00:19:48,080
And let me tell you, my good man,

519
00:19:48,080 --> 00:19:50,480
in 15 years, times will change again.

520
00:19:50,480 --> 00:19:53,560
And this kind of technology will be part of that change.

521
00:19:54,920 --> 00:19:57,360
I think a good point of distinction,

522
00:19:57,360 --> 00:20:00,000
you mentioned with music, for example.

523
00:20:00,920 --> 00:20:03,440
What constitutes better

524
00:20:03,440 --> 00:20:06,520
is the context that already exists, right?

525
00:20:06,520 --> 00:20:09,760
That is determined based on us, humans,

526
00:20:09,760 --> 00:20:12,880
who are, we are very limited in our experience.

527
00:20:12,880 --> 00:20:14,320
It's linear.

528
00:20:14,320 --> 00:20:17,840
And the argument can be made that for AI

529
00:20:17,840 --> 00:20:20,600
to reaches its true potential,

530
00:20:20,600 --> 00:20:23,680
it should not rely on humanity whatsoever.

531
00:20:23,680 --> 00:20:28,280
So after that point, the argument would become,

532
00:20:28,280 --> 00:20:31,080
would the AI, artificial intelligence,

533
00:20:31,080 --> 00:20:35,480
be able to create a beat or a piece of music

534
00:20:35,480 --> 00:20:38,840
that has absolutely no root within the context

535
00:20:38,840 --> 00:20:41,160
that is created by human civilization

536
00:20:41,160 --> 00:20:42,920
and is something that is introducing

537
00:20:42,920 --> 00:20:45,280
a new context altogether?

538
00:20:45,280 --> 00:20:46,440
Does it make sense?

539
00:20:46,440 --> 00:20:48,000
Yes, it does.

540
00:20:48,000 --> 00:20:49,760
And I don't know if you wanted to go

541
00:20:49,760 --> 00:20:51,040
this far down the rabbit hole,

542
00:20:51,040 --> 00:20:53,720
but there isn't a depth that you could go

543
00:20:53,720 --> 00:20:54,720
that I will not follow you.

544
00:20:54,720 --> 00:20:55,960
Oh, amazing.

545
00:20:55,960 --> 00:20:57,800
I'm mostly interested in rabbit holes

546
00:20:57,800 --> 00:21:00,080
because academic sides, I'm like,

547
00:21:00,080 --> 00:21:02,000
everything is written on Google.

548
00:21:02,000 --> 00:21:04,560
There are so many people who can nail those things better,

549
00:21:04,560 --> 00:21:07,280
but I think it's very important to be imaginative

550
00:21:07,280 --> 00:21:09,280
and consider many different options

551
00:21:09,280 --> 00:21:11,720
that we have to deal with in the coming years.

552
00:21:11,720 --> 00:21:13,960
I think these are interesting topics.

553
00:21:13,960 --> 00:21:15,800
Again, they aren't things that I'd be called upon

554
00:21:15,800 --> 00:21:18,160
to speak about at Interpol or at Citibank,

555
00:21:18,920 --> 00:21:21,200
but they are the long ball consequences

556
00:21:21,200 --> 00:21:23,320
that I think, as you pointed out,

557
00:21:23,320 --> 00:21:25,200
it's important to kind of splay those open

558
00:21:25,200 --> 00:21:26,320
and see what does this mean?

559
00:21:26,320 --> 00:21:27,320
Where does this go?

560
00:21:27,320 --> 00:21:28,800
To your point about music,

561
00:21:28,800 --> 00:21:30,600
I think the question here would be

562
00:21:30,600 --> 00:21:32,440
what would be the purpose of music?

563
00:21:32,440 --> 00:21:34,400
If the purpose is enjoyment

564
00:21:34,400 --> 00:21:36,440
and we are the only sentient things that enjoys,

565
00:21:36,440 --> 00:21:40,040
then I suspect you're right.

566
00:21:40,040 --> 00:21:42,240
We may be able to start from scratch

567
00:21:42,240 --> 00:21:45,280
and create beats and permutations of sound

568
00:21:45,280 --> 00:21:49,320
that really don't resemble any known musical genre

569
00:21:49,320 --> 00:21:52,400
that might be extremely appealing to some people.

570
00:21:52,400 --> 00:21:54,720
Very rare that music would be appealing to everybody,

571
00:21:54,720 --> 00:21:57,080
but that might be doable.

572
00:21:57,080 --> 00:22:00,640
Now, if AI itself could become sentient in some sense,

573
00:22:00,640 --> 00:22:02,200
maybe it would create,

574
00:22:02,200 --> 00:22:04,360
if artificial general intelligence

575
00:22:04,360 --> 00:22:06,520
were to create art for itself, two things.

576
00:22:06,520 --> 00:22:09,520
A, it's extremely unlikely we would be able to understand it,

577
00:22:09,520 --> 00:22:11,920
and B, it's extremely unlikely it would have a form

578
00:22:11,920 --> 00:22:12,880
that we now recognize.

579
00:22:12,880 --> 00:22:14,600
It's not gonna be carving marble.

580
00:22:14,600 --> 00:22:16,560
It's not gonna be fricking oil paints.

581
00:22:16,560 --> 00:22:19,200
It's gonna be something astronomically more complicated,

582
00:22:19,200 --> 00:22:20,080
and from the viewpoint

583
00:22:20,080 --> 00:22:22,560
of something astronomically more intelligent,

584
00:22:22,560 --> 00:22:24,560
it'll be astronomically more beautiful.

585
00:22:24,560 --> 00:22:26,160
The language of such an entity,

586
00:22:26,160 --> 00:22:27,160
we might think about it,

587
00:22:27,160 --> 00:22:29,000
like you and I have an alphabet.

588
00:22:29,000 --> 00:22:31,000
We're speaking English right now.

589
00:22:31,000 --> 00:22:33,000
You may speak other languages, I don't.

590
00:22:33,000 --> 00:22:34,960
I pretend to have a fistful of French words,

591
00:22:34,960 --> 00:22:37,120
but I go no farther.

592
00:22:37,120 --> 00:22:39,160
I'm an uncultured American.

593
00:22:40,400 --> 00:22:42,320
History, I'm pretty strong, but languages.

594
00:22:43,160 --> 00:22:46,240
Let's presume we're talking about the English alphabet.

595
00:22:46,240 --> 00:22:48,920
We've got a couple dozen letters to work with here,

596
00:22:50,040 --> 00:22:53,400
and we can only talk at this current bandwidth.

597
00:22:53,400 --> 00:22:56,840
An AGI might have a number of symbols

598
00:22:56,840 --> 00:22:58,040
that it might communicate with,

599
00:22:58,040 --> 00:22:59,960
either itself or other AGI's.

600
00:22:59,960 --> 00:23:04,200
We might imagine a cube of a million pixels this way,

601
00:23:04,200 --> 00:23:05,360
and a million pixels this way,

602
00:23:05,360 --> 00:23:07,640
and a million pixels this way,

603
00:23:07,640 --> 00:23:09,240
and each of those pixels can have

604
00:23:09,240 --> 00:23:11,560
one of two trillion different permutations

605
00:23:11,560 --> 00:23:13,480
of what we want to call it,

606
00:23:13,480 --> 00:23:15,520
like color number, what have you,

607
00:23:15,520 --> 00:23:18,640
and that any combination of all of that

608
00:23:18,640 --> 00:23:20,320
pushed through is like a letter.

609
00:23:21,400 --> 00:23:25,280
And so we couldn't possibly imagine

610
00:23:25,280 --> 00:23:27,680
the full extent of what expression would be

611
00:23:27,680 --> 00:23:29,120
for something beyond us,

612
00:23:29,120 --> 00:23:32,520
no better than a caterpillar could imagine our expression,

613
00:23:32,520 --> 00:23:34,680
could imagine Shakespeare's sonnets,

614
00:23:34,680 --> 00:23:38,440
could imagine great architecture,

615
00:23:38,440 --> 00:23:40,400
Corinthian columns.

616
00:23:40,400 --> 00:23:42,160
Caterpillars ain't gonna cut that mustard,

617
00:23:42,160 --> 00:23:43,400
and we ain't gonna cut the mustard

618
00:23:43,400 --> 00:23:45,240
of what's beyond us either.

619
00:23:45,240 --> 00:23:48,600
Yeah, we are creating a new and different kind of a species

620
00:23:48,600 --> 00:23:51,200
than humanity, and I think this is very important

621
00:23:51,200 --> 00:23:54,000
for people to understand that this is not a phase

622
00:23:54,000 --> 00:23:56,960
or a trend that's just gonna go away,

623
00:23:56,960 --> 00:23:58,960
and in 10 years, people will be in charge

624
00:23:58,960 --> 00:24:00,240
of everything again.

625
00:24:00,240 --> 00:24:02,760
Like, it's just not gonna happen.

626
00:24:02,760 --> 00:24:04,200
Yeah, I think a lot of people do operate

627
00:24:04,200 --> 00:24:06,200
under that assumption, though.

628
00:24:06,200 --> 00:24:09,600
I think it is really seen as a very short-term transition.

629
00:24:09,640 --> 00:24:11,840
Is it also shared within governments and UN

630
00:24:11,840 --> 00:24:13,880
and those places that you talk at?

631
00:24:13,880 --> 00:24:16,120
Yeah, I mean, in the broad sweep,

632
00:24:16,120 --> 00:24:19,640
so my unabashed long-term objective here

633
00:24:19,640 --> 00:24:21,680
is to bring what I refer to as

634
00:24:21,680 --> 00:24:24,480
the trajectory of intelligence conversation

635
00:24:24,480 --> 00:24:28,680
into these organizations which I have a relationship with.

636
00:24:28,680 --> 00:24:30,680
However, I'm not here to impose that

637
00:24:30,680 --> 00:24:33,080
when it's not a time that they're interested about it, right?

638
00:24:33,080 --> 00:24:34,520
I have certain people within these orgs

639
00:24:34,520 --> 00:24:36,880
that I can talk about all manner of topics,

640
00:24:37,680 --> 00:24:40,160
but for many, the reason I'm called on

641
00:24:40,160 --> 00:24:43,480
is because we study what's possible and what's working,

642
00:24:43,480 --> 00:24:45,120
so they're gonna have an academic,

643
00:24:45,120 --> 00:24:47,760
they might have an expert in computer vision,

644
00:24:47,760 --> 00:24:49,960
but they need somebody who's gonna say,

645
00:24:49,960 --> 00:24:51,640
well, here's where it's impacting this industry,

646
00:24:51,640 --> 00:24:53,240
here's where it's impacting the industry,

647
00:24:53,240 --> 00:24:54,560
here's what makes it hard to adopt,

648
00:24:54,560 --> 00:24:56,840
here's what makes it easy to adopt in defense,

649
00:24:56,840 --> 00:24:58,880
here's what, they need a reality check

650
00:24:58,880 --> 00:25:00,320
on where it's impacting the world,

651
00:25:00,320 --> 00:25:02,960
and that's why I step into the World Bank

652
00:25:02,960 --> 00:25:04,880
to enter poll, et cetera.

653
00:25:04,880 --> 00:25:06,720
So I'm not gonna force feed them that stuff,

654
00:25:06,720 --> 00:25:08,120
but it is my unabashed goal

655
00:25:08,120 --> 00:25:10,000
to eventually be able to introduce these themes,

656
00:25:10,000 --> 00:25:11,160
and we have a new series,

657
00:25:11,160 --> 00:25:13,480
we have a podcast called the AI in Business podcast,

658
00:25:13,480 --> 00:25:15,920
and we have a Saturday series called AI Futures,

659
00:25:15,920 --> 00:25:17,240
where we're kind of stretching this,

660
00:25:17,240 --> 00:25:19,560
we're taking people from reputable organizations,

661
00:25:19,560 --> 00:25:20,600
like let's say Berkeley,

662
00:25:20,600 --> 00:25:23,440
Stuart Russell was our first guest on this podcast,

663
00:25:23,440 --> 00:25:27,840
the OECD, folks from the Future of Humanity Institute

664
00:25:27,840 --> 00:25:30,480
at Oxford, and we're having them kind of stretch

665
00:25:30,480 --> 00:25:31,600
the credible viewpoints

666
00:25:31,600 --> 00:25:33,760
into where is this ultimately taking us?

667
00:25:33,760 --> 00:25:35,920
Because for me, I think we do need to consider those things,

668
00:25:36,240 --> 00:25:39,040
to your point, right now in government and in business,

669
00:25:39,040 --> 00:25:41,800
it is absolutely not on the radar.

670
00:25:41,800 --> 00:25:43,640
It's not even fiction,

671
00:25:43,640 --> 00:25:46,720
it's literally not on the table in any way.

672
00:25:46,720 --> 00:25:48,880
It's not even digestible, it's almost invisible,

673
00:25:48,880 --> 00:25:50,280
even if it were to be articulated,

674
00:25:50,280 --> 00:25:53,760
it would not even land on the table, it doesn't exist.

675
00:25:53,760 --> 00:25:55,280
And it's shared in politics.

676
00:25:55,280 --> 00:25:57,000
I mean, you see how important AI is,

677
00:25:57,000 --> 00:25:59,000
and nobody's really talking about it.

678
00:25:59,000 --> 00:26:01,000
Nobody's talking about the long-term, no.

679
00:26:01,000 --> 00:26:04,800
Even short-term, you see like neither Trump or Biden

680
00:26:04,840 --> 00:26:06,160
are really talking about AI,

681
00:26:06,160 --> 00:26:08,480
but AI is going to have huge impacts

682
00:26:08,480 --> 00:26:11,240
on every single human being's lives on this planet.

683
00:26:11,240 --> 00:26:14,080
And I think, so what is it to be a politician?

684
00:26:15,120 --> 00:26:17,400
Just winning a popularity contest.

685
00:26:19,240 --> 00:26:21,120
In Emerson's words,

686
00:26:21,120 --> 00:26:23,360
to eat dust before the men

687
00:26:23,360 --> 00:26:25,080
who actually stand behind the throne,

688
00:26:25,080 --> 00:26:26,680
something akin to this.

689
00:26:26,680 --> 00:26:29,680
Now, I don't know, I'm not disparaging politicians,

690
00:26:29,680 --> 00:26:30,960
not even in the slightest.

691
00:26:30,960 --> 00:26:33,080
In fact, I do respect the profession

692
00:26:33,080 --> 00:26:36,680
despite the gritty realities they're in.

693
00:26:39,200 --> 00:26:40,800
But to be a politician, like you said,

694
00:26:40,800 --> 00:26:42,000
yes, popularity contest.

695
00:26:42,000 --> 00:26:44,040
So why aren't they talking about AI?

696
00:26:44,040 --> 00:26:47,960
It just doesn't matter enough to the constituents, right?

697
00:26:47,960 --> 00:26:51,320
Why are they talking about name your topic, sir?

698
00:26:51,320 --> 00:26:52,400
Name your topic.

699
00:26:52,400 --> 00:26:54,240
I mean, the most arbitrary things

700
00:26:54,240 --> 00:26:56,320
become giant friction points,

701
00:26:56,320 --> 00:26:58,560
and then they get in all the speeches.

702
00:26:58,560 --> 00:27:00,560
Is it because of their inherent meaning

703
00:27:00,560 --> 00:27:02,520
and some platonic reality?

704
00:27:02,520 --> 00:27:03,720
No, no, no.

705
00:27:03,720 --> 00:27:05,920
It's simply because they resonate with the constituency.

706
00:27:05,920 --> 00:27:08,320
So we can expect AI to land

707
00:27:08,320 --> 00:27:13,080
on the sort of political speech table

708
00:27:13,080 --> 00:27:16,440
when it matters to the people who are being spoken to.

709
00:27:16,440 --> 00:27:18,720
Yeah, the disruption of AI and technology

710
00:27:18,720 --> 00:27:22,080
basically are not in any shape or form

711
00:27:22,080 --> 00:27:23,960
determined by the political will

712
00:27:23,960 --> 00:27:25,800
or social will to accept them though.

713
00:27:25,800 --> 00:27:28,280
That's the thing, like you're talking about deep fakes.

714
00:27:28,280 --> 00:27:32,280
Deep fakes will come and disrupt future political campaigns

715
00:27:32,720 --> 00:27:35,520
from the perspective of constituents and the politicians

716
00:27:35,520 --> 00:27:37,200
and all of them altogether

717
00:27:37,200 --> 00:27:40,200
without anybody agreeing or disagreeing with them.

718
00:27:40,200 --> 00:27:44,520
It does not require agreement of people

719
00:27:44,520 --> 00:27:46,840
to go in that certain kind of a speed

720
00:27:46,840 --> 00:27:48,600
that technology is evolving, right?

721
00:27:48,600 --> 00:27:52,200
So it's important to at least acknowledge

722
00:27:52,200 --> 00:27:57,200
the huge disruption that we haven't seen anything of it yet.

723
00:27:57,280 --> 00:27:58,840
Because what we are experiencing right now,

724
00:27:58,880 --> 00:28:03,360
is this seemingly division that exists globally, really.

725
00:28:03,360 --> 00:28:05,640
A lot of people can say that this is because people

726
00:28:05,640 --> 00:28:08,320
finally have channels of expressions

727
00:28:08,320 --> 00:28:11,240
in the form of social media that now they can be heard.

728
00:28:12,200 --> 00:28:13,280
When you say division, by the way,

729
00:28:13,280 --> 00:28:15,960
I just want to follow you, what do you mean?

730
00:28:15,960 --> 00:28:17,480
So division, for example,

731
00:28:17,480 --> 00:28:19,480
what is being portrayed by the media

732
00:28:19,480 --> 00:28:22,080
and what is being portrayed by certain news media?

733
00:28:22,080 --> 00:28:25,560
Division right and left, division upper and lower class.

734
00:28:25,560 --> 00:28:27,080
I mean, what is the core division?

735
00:28:27,120 --> 00:28:28,800
Political, political division.

736
00:28:28,800 --> 00:28:30,120
Yeah, because the class division,

737
00:28:30,120 --> 00:28:31,760
nobody really wants to talk about.

738
00:28:33,280 --> 00:28:35,240
I don't know enough about the class division

739
00:28:35,240 --> 00:28:37,280
to have a firm opinion on that, but okay.

740
00:28:37,280 --> 00:28:40,080
So you're talking about the more or less

741
00:28:40,080 --> 00:28:41,360
the left and right to some degree.

742
00:28:41,360 --> 00:28:42,680
Right, right.

743
00:28:43,600 --> 00:28:47,960
Like this recent example of the video

744
00:28:47,960 --> 00:28:51,960
of George Floyd getting killed by the police,

745
00:28:51,960 --> 00:28:56,120
which is horrible, and it started a lot of consequences

746
00:28:56,120 --> 00:28:57,360
as a result of it.

747
00:28:57,360 --> 00:29:00,360
But in a couple of years, a fake video can come up

748
00:29:00,360 --> 00:29:03,200
of a very same kind of a thing

749
00:29:03,200 --> 00:29:06,520
that half of the audience and half of the viewers

750
00:29:06,520 --> 00:29:08,600
will believe it right off the bat.

751
00:29:08,600 --> 00:29:11,000
Well, this is going to have very real consequences

752
00:29:11,000 --> 00:29:12,440
socially and politically.

753
00:29:12,440 --> 00:29:17,040
Yeah, you know, this really is on my mind a lot.

754
00:29:17,040 --> 00:29:20,120
So, I mean, again, UN, I've been on,

755
00:29:20,120 --> 00:29:22,960
I was in television in Singapore on this same topic.

756
00:29:22,960 --> 00:29:24,360
What does this mean for the future?

757
00:29:24,360 --> 00:29:26,680
And also with reference to this political division,

758
00:29:26,680 --> 00:29:27,920
I've thought a lot about it.

759
00:29:27,920 --> 00:29:32,920
And on some level, I didn't see it coming as fast as it was.

760
00:29:33,600 --> 00:29:36,760
So I was at Facebook headquarters in 2016.

761
00:29:36,760 --> 00:29:38,520
You're talking about Deepfakes.

762
00:29:38,520 --> 00:29:39,480
No, I wasn't talking about Deepfakes,

763
00:29:39,480 --> 00:29:42,040
but I was at Facebook headquarters in 2016

764
00:29:42,040 --> 00:29:44,200
talking to the head of core machine learning at the time,

765
00:29:44,200 --> 00:29:46,120
fellow by the name of Hussein Mahena,

766
00:29:46,120 --> 00:29:48,600
very sharp fellow, I think he's with Google now.

767
00:29:48,600 --> 00:29:51,240
They always swap around over there.

768
00:29:51,240 --> 00:29:55,040
There's only so many places to go in SF,

769
00:29:55,040 --> 00:29:56,360
but a really sharp fellow.

770
00:29:56,360 --> 00:29:59,960
And I recall only very lightly talking about

771
00:29:59,960 --> 00:30:01,880
what was beginning to rattle.

772
00:30:01,880 --> 00:30:03,200
This is before Trump was elected.

773
00:30:03,200 --> 00:30:07,520
What was beginning to rattle is this idea of echo chambers

774
00:30:07,520 --> 00:30:09,000
for political perspectives.

775
00:30:09,000 --> 00:30:10,480
And I thought to myself, you know,

776
00:30:10,480 --> 00:30:12,920
I'm not seeing that much of it just yet right now

777
00:30:12,920 --> 00:30:14,400
in my own experience.

778
00:30:14,400 --> 00:30:17,240
I don't really see that as like the real workhorse

779
00:30:17,240 --> 00:30:19,440
of, you know, friction in society.

780
00:30:19,440 --> 00:30:21,280
But by golly, four years later,

781
00:30:22,560 --> 00:30:25,080
you know, I can tell you that the echo chamber thing,

782
00:30:25,080 --> 00:30:27,200
it's not just, well, more people have a voice

783
00:30:27,200 --> 00:30:28,840
and that's why they're fighting each other.

784
00:30:28,840 --> 00:30:32,280
I would actually push back on that idea very firmly.

785
00:30:32,280 --> 00:30:33,800
I would push back on that idea.

786
00:30:33,800 --> 00:30:38,480
I suspect that there are mounted incentives

787
00:30:38,480 --> 00:30:41,320
and money to be made and influence to be wielded

788
00:30:42,400 --> 00:30:47,400
that sort of make incessant division just work, right?

789
00:30:47,920 --> 00:30:49,560
I mean, who has a Twitter following

790
00:30:49,560 --> 00:30:52,520
more than 100,000 people who isn't primarily

791
00:30:52,520 --> 00:30:55,120
basing it on a for or against or an affinity, right?

792
00:30:55,120 --> 00:30:57,720
Oh, you know, I run a puppy kennel or something,

793
00:30:57,720 --> 00:31:00,440
or I like take pictures on the beach

794
00:31:00,440 --> 00:31:01,320
and I'm an attractive woman.

795
00:31:01,320 --> 00:31:02,920
Like, sort them out.

796
00:31:02,920 --> 00:31:06,240
Intellectuals, you're gonna pop off

797
00:31:06,240 --> 00:31:07,720
if you can have a strong enemy, right?

798
00:31:07,720 --> 00:31:09,420
Because you've already got a huge resonance.

799
00:31:09,420 --> 00:31:11,120
If you're kind of middle of the road,

800
00:31:11,120 --> 00:31:13,340
it's like you're the enemy of both, man.

801
00:31:14,320 --> 00:31:16,720
And I think that what I like about Boston

802
00:31:16,720 --> 00:31:18,560
is it's actually okay to be middle of the road.

803
00:31:18,560 --> 00:31:20,720
I am, I'm tied to neither political party.

804
00:31:20,720 --> 00:31:23,000
You can do that and not be a bad guy.

805
00:31:23,000 --> 00:31:27,760
In San Francisco, I mean, you're essentially a Bible thumper.

806
00:31:27,760 --> 00:31:30,080
Now, I've never read the Bible, but oddly enough,

807
00:31:30,080 --> 00:31:32,760
I'm close to being one in the Bay Area.

808
00:31:32,760 --> 00:31:36,280
And you with that American flag, I would not recommend.

809
00:31:36,280 --> 00:31:38,600
So you doing that within 20 miles of San Francisco,

810
00:31:38,600 --> 00:31:40,280
I would not recommend it.

811
00:31:40,280 --> 00:31:42,880
And you know, I was a refugee in Canada

812
00:31:42,880 --> 00:31:45,640
and I'm an ex-Muslim immigrant in the US.

813
00:31:45,960 --> 00:31:46,800
And I'm glad we have you.

814
00:31:46,800 --> 00:31:48,080
I'm glad it's helping out.

815
00:31:48,080 --> 00:31:52,040
All it takes is one, quote unquote,

816
00:31:52,040 --> 00:31:54,720
wrong interpretation of the US flag, for example,

817
00:31:54,720 --> 00:31:56,680
which I had in my room in Tehran.

818
00:31:56,680 --> 00:32:00,080
You know, United States is a beacon of light still.

819
00:32:00,080 --> 00:32:01,480
For people all around the world,

820
00:32:01,480 --> 00:32:03,480
you see protesters in Hong Kong

821
00:32:03,480 --> 00:32:05,240
under that kind of a situation,

822
00:32:05,240 --> 00:32:06,720
they're waving the American flags.

823
00:32:06,720 --> 00:32:11,440
It has a meaning beyond politics and geography.

824
00:32:11,440 --> 00:32:13,880
Yeah, I hope it retains that meaning, that's my hope.

825
00:32:13,880 --> 00:32:14,720
I agree.

826
00:32:14,760 --> 00:32:17,000
I'm not here calling America good ubiquitously, right?

827
00:32:17,000 --> 00:32:18,520
I'm obviously not doing that.

828
00:32:18,520 --> 00:32:21,160
But I am saying that it's a shame

829
00:32:21,160 --> 00:32:23,000
when it's ubiquitously seen as evil,

830
00:32:23,000 --> 00:32:25,560
because I think we actually have a lot more in common

831
00:32:25,560 --> 00:32:27,600
that hopefully we can get along with,

832
00:32:27,600 --> 00:32:29,720
but technology is certainly veering us away

833
00:32:29,720 --> 00:32:32,160
from having concord here.

834
00:32:32,160 --> 00:32:36,200
Yeah, it also shows deeper parts of human nature, right?

835
00:32:36,200 --> 00:32:39,240
Like what you were saying that people depend on division

836
00:32:39,240 --> 00:32:41,240
in order to capitalize on it.

837
00:32:41,240 --> 00:32:44,360
This is something that is very deeply woven

838
00:32:44,360 --> 00:32:45,920
within who we are.

839
00:32:45,920 --> 00:32:49,680
Oh yes, oh yes, the tribe, sir, the tribe.

840
00:32:49,680 --> 00:32:50,640
Yeah.

841
00:32:50,640 --> 00:32:53,400
I mean, you know, we are a tribal species

842
00:32:53,400 --> 00:32:56,920
and us versus them is the automatic frame.

843
00:32:56,920 --> 00:33:01,920
It takes vigilant volitional effort

844
00:33:02,440 --> 00:33:06,280
to do anything other than think us versus them

845
00:33:06,280 --> 00:33:08,240
in almost all circumstances.

846
00:33:08,240 --> 00:33:10,400
I mean, maybe that's a bit of a stretch,

847
00:33:10,400 --> 00:33:13,360
but to be frank, it's quite clearly the norm.

848
00:33:13,360 --> 00:33:16,120
You know, anyone who considers themselves

849
00:33:16,120 --> 00:33:18,160
free entirely of that dynamic,

850
00:33:18,160 --> 00:33:20,840
I think is generally fooling themselves to a spooky degree.

851
00:33:20,840 --> 00:33:23,880
I think we should be aware of just how consistent

852
00:33:23,880 --> 00:33:24,800
and vigilant we have to be

853
00:33:24,800 --> 00:33:28,240
and that we're never gonna be perfect in that category.

854
00:33:28,240 --> 00:33:29,720
But I hope we can work at it.

855
00:33:29,720 --> 00:33:31,240
I don't think we're doing a great job right now

856
00:33:31,240 --> 00:33:32,640
as a country.

857
00:33:32,640 --> 00:33:35,720
As a country, yeah, especially with China

858
00:33:35,720 --> 00:33:37,480
on the other side of the equation.

859
00:33:38,440 --> 00:33:39,360
Yeah, all right, you want to?

860
00:33:39,360 --> 00:33:42,360
Yeah, let's get in there.

861
00:33:43,360 --> 00:33:44,200
Yeah.

862
00:33:46,480 --> 00:33:48,360
I remember talking to someone in Toronto

863
00:33:48,360 --> 00:33:52,720
a couple of years ago at a Toronto transhumanist meetup

864
00:33:52,720 --> 00:33:54,920
and we talked for like three hours.

865
00:33:54,920 --> 00:33:55,760
It's awesome.

866
00:33:55,760 --> 00:33:57,480
And at the end, it was like, you know,

867
00:33:57,480 --> 00:33:59,960
the really only three things matter in this world,

868
00:33:59,960 --> 00:34:02,040
China, United States and what they do

869
00:34:02,040 --> 00:34:04,600
with artificial intelligence.

870
00:34:04,600 --> 00:34:06,200
And this was a couple of years ago

871
00:34:06,200 --> 00:34:09,640
and it seems to be the case.

872
00:34:09,640 --> 00:34:12,400
My friends in Paris would be angry if I agreed with you.

873
00:34:12,400 --> 00:34:14,480
So I'm gonna pretend to disagree with you.

874
00:34:16,480 --> 00:34:17,880
Please expand on that.

875
00:34:17,880 --> 00:34:18,800
Yeah, yeah.

876
00:34:22,800 --> 00:34:24,080
I would take a lot of flack.

877
00:34:24,080 --> 00:34:26,840
No, I mean, what for you is interesting

878
00:34:26,840 --> 00:34:28,560
about the US China AI race?

879
00:34:28,560 --> 00:34:30,680
I mean, I think about this incessantly

880
00:34:30,680 --> 00:34:33,240
and I think about this from a defense perspective.

881
00:34:33,240 --> 00:34:34,800
I think about this from a media perspective.

882
00:34:34,800 --> 00:34:36,760
I spoke in Shanghai for the United Nations.

883
00:34:36,760 --> 00:34:39,160
I've interviewed a tremendous number of companies

884
00:34:39,160 --> 00:34:42,160
with co-founders in China that are in the AI space

885
00:34:42,160 --> 00:34:46,480
as well as investors in the Asian AI ecosystem.

886
00:34:46,480 --> 00:34:48,840
And so I have vantage points

887
00:34:50,040 --> 00:34:53,960
that have fleshed this out pretty reasonably thoroughly.

888
00:34:53,960 --> 00:34:56,080
I mean, I'm not an expert in its entirety,

889
00:34:56,080 --> 00:34:59,680
but what makes it interesting for you?

890
00:34:59,680 --> 00:35:01,120
What's important about that dynamic?

891
00:35:01,120 --> 00:35:03,600
What's interesting to me is within the context

892
00:35:03,600 --> 00:35:06,640
of human nature, the tribalism that we talked about

893
00:35:06,640 --> 00:35:09,400
and how we have always used technology as a tool

894
00:35:09,400 --> 00:35:12,160
in order to pursue our human intentions.

895
00:35:12,160 --> 00:35:14,920
And the biggest tribes right now I see in the world

896
00:35:14,920 --> 00:35:19,920
is the Chinese Communist Party and the United States

897
00:35:19,960 --> 00:35:23,120
government that they're clearly in conflict with each other,

898
00:35:23,120 --> 00:35:25,800
which creates, competition is always good

899
00:35:25,800 --> 00:35:27,840
to develop new things.

900
00:35:27,840 --> 00:35:32,240
It creates like space race and a lot of our technology

901
00:35:32,240 --> 00:35:36,280
is contribution of the wars that we have fought, right?

902
00:35:36,280 --> 00:35:38,480
But at the end of the day, it's very important

903
00:35:38,480 --> 00:35:42,800
to understand that AI is perhaps the ultimate tool

904
00:35:42,800 --> 00:35:46,000
and ultimate depending on the intention

905
00:35:46,000 --> 00:35:48,280
can be used as a weapon, can be used as like,

906
00:35:48,280 --> 00:35:49,600
it's like any other tool, you know,

907
00:35:49,600 --> 00:35:51,280
with fire you can warm yourself up

908
00:35:51,280 --> 00:35:53,000
or burn down your entire building.

909
00:35:53,880 --> 00:35:56,880
And it's very interesting to me because I know China

910
00:35:56,880 --> 00:36:00,640
has access to more data than United States

911
00:36:00,640 --> 00:36:03,720
and US is kind of, now please correct me if I'm wrong,

912
00:36:03,720 --> 00:36:06,960
but it seems like US is lagging behind China

913
00:36:06,960 --> 00:36:10,000
specifically because of the decentralized system

914
00:36:10,000 --> 00:36:13,720
that the US has versus a centrally driven group

915
00:36:13,720 --> 00:36:15,800
of engineers in China that were like,

916
00:36:15,800 --> 00:36:19,040
this is our objective, we're gonna get it no matter how.

917
00:36:19,040 --> 00:36:21,160
And in the United States, you have to, you know,

918
00:36:21,160 --> 00:36:24,360
argue about the definition of the word, for example,

919
00:36:24,360 --> 00:36:29,080
is for many months, costing millions and millions of dollars.

920
00:36:29,080 --> 00:36:31,880
Yeah, and I mean, the communist system,

921
00:36:31,880 --> 00:36:35,520
never having worked within it or understanding it

922
00:36:35,520 --> 00:36:38,640
to a great extent, I'm sure has its own hiccups

923
00:36:38,640 --> 00:36:42,040
and what we might refer to as inefficiencies

924
00:36:42,040 --> 00:36:46,160
with corruption and, you know, however that operates.

925
00:36:46,160 --> 00:36:50,440
But to your point, so I wrote a rather in-depth article

926
00:36:50,440 --> 00:36:52,760
called The Seven Weaknesses of the West.

927
00:36:52,760 --> 00:36:55,120
So someone typed eMERGE, E-M-E-R-J,

928
00:36:55,120 --> 00:36:57,720
Seven Weaknesses of the West, they'd find it on Google.

929
00:36:57,720 --> 00:36:59,960
And this really summarizes a lot of what I consider

930
00:36:59,960 --> 00:37:02,040
to be where we're behind the eight ball with China.

931
00:37:02,040 --> 00:37:04,680
Now the United States, I did another very lengthy piece

932
00:37:04,680 --> 00:37:07,240
on the AI and China ecosystem, how it's developing,

933
00:37:07,240 --> 00:37:09,280
how it's evolving, the kinds of startups, et cetera.

934
00:37:09,280 --> 00:37:12,720
That's its own bit of research I believe is free as well.

935
00:37:12,720 --> 00:37:17,360
But we have more academic muscle.

936
00:37:17,360 --> 00:37:21,240
We have more actual scientists

937
00:37:21,240 --> 00:37:23,920
and actual scientists who have experience deploying it,

938
00:37:23,920 --> 00:37:26,280
which is a really, really good distinction

939
00:37:26,280 --> 00:37:29,200
because coming out of Carnegie Mellon is kinda neat,

940
00:37:29,200 --> 00:37:30,400
you know, like you're not dumb.

941
00:37:30,400 --> 00:37:31,960
That's awesome, that's great.

942
00:37:32,000 --> 00:37:34,560
But actually having experience,

943
00:37:34,560 --> 00:37:36,520
turning that into a user experience

944
00:37:36,520 --> 00:37:39,760
or reducing money laundering in a bank

945
00:37:39,760 --> 00:37:43,480
or improving retention for a subscription service,

946
00:37:43,480 --> 00:37:48,480
that's a much more robust, practical, applicable experience.

947
00:37:48,960 --> 00:37:50,280
And that's frankly not something

948
00:37:50,280 --> 00:37:51,440
you can get in purely academia.

949
00:37:51,440 --> 00:37:53,640
So we have certainly some edges here.

950
00:37:54,920 --> 00:37:56,640
And I think some people are optimistic

951
00:37:56,640 --> 00:37:58,120
that the free market system

952
00:37:58,120 --> 00:38:00,720
will also just be more creative writ large.

953
00:38:00,720 --> 00:38:04,640
That said, there are a lot of disadvantages.

954
00:38:04,640 --> 00:38:06,840
And a lot of them I think have to do with,

955
00:38:06,840 --> 00:38:08,160
you mentioned centralization.

956
00:38:08,160 --> 00:38:10,160
I think a lot of this ties to the ability

957
00:38:10,160 --> 00:38:12,760
for the Chinese Communist Party

958
00:38:12,760 --> 00:38:15,720
to marshal a united front of effort.

959
00:38:15,720 --> 00:38:17,600
And that involves many factors here.

960
00:38:17,600 --> 00:38:19,720
That involves the private sector and academia

961
00:38:19,720 --> 00:38:23,960
essentially just being wings of the great and ruling power.

962
00:38:23,960 --> 00:38:27,960
So being nothing more but wings, nothing more.

963
00:38:28,280 --> 00:38:31,080
That's a very powerful and lovely place to be.

964
00:38:31,080 --> 00:38:36,080
And in fact, Confucius, there's no better setup.

965
00:38:36,760 --> 00:38:38,840
So Xi Jinping, if your objective

966
00:38:38,840 --> 00:38:43,120
was to rule without question,

967
00:38:43,120 --> 00:38:46,240
Xi Jinping would be, I mean, I think he's in a great spot.

968
00:38:46,240 --> 00:38:48,160
I mean, if that was your goal, Ryan, I'm just saying

969
00:38:48,160 --> 00:38:49,880
if it was your goal, he's in the best spot.

970
00:38:49,880 --> 00:38:51,040
Objectively speaking.

971
00:38:51,040 --> 00:38:56,040
There is no, pericles is not revered in China, right?

972
00:38:57,040 --> 00:39:00,760
The Socrates and Christ of China

973
00:39:00,760 --> 00:39:03,160
would be Confucius almost unquestionably.

974
00:39:03,160 --> 00:39:06,280
And man, I mean, Confucius is real congenial

975
00:39:06,280 --> 00:39:10,120
to everything lining up to the great emperor.

976
00:39:10,120 --> 00:39:12,400
Now I'm not saying Confucius advocates for tyranny.

977
00:39:12,400 --> 00:39:14,640
In fact, I think it's pretty clear he doesn't.

978
00:39:14,640 --> 00:39:19,320
But it can be bent and molded so sweetly, so snuggly

979
00:39:19,320 --> 00:39:21,880
in a nice, obedient alignment.

980
00:39:21,880 --> 00:39:24,000
And I think so culturally, he sits in just

981
00:39:24,040 --> 00:39:28,280
an absolutely beautiful, almost impeccable cultural soil

982
00:39:28,280 --> 00:39:29,960
for what he is to do.

983
00:39:30,760 --> 00:39:35,760
And they have the wings of the party dynamic going on.

984
00:39:36,040 --> 00:39:39,160
I think they also have the ruler for life thing, right?

985
00:39:39,160 --> 00:39:41,680
Now, now, ruler for life.

986
00:39:41,680 --> 00:39:45,320
I mean, they don't fall from their position

987
00:39:45,320 --> 00:39:47,800
until the mandate of heaven takes them off of it, right?

988
00:39:47,800 --> 00:39:51,160
This is like BC, we're talking BC here, these precedencies.

989
00:39:51,160 --> 00:39:55,360
This is a culture that is, I mean, rich, in my opinion,

990
00:39:55,360 --> 00:39:57,880
rich and amazing to no end, to no end.

991
00:39:57,880 --> 00:39:58,960
I mean, the Tang dynasty.

992
00:39:58,960 --> 00:40:01,360
I mean, I'm so fascinated with the history of China.

993
00:40:01,360 --> 00:40:03,760
I'm not a big CCP fan, but China,

994
00:40:03,760 --> 00:40:06,360
I'm just like ravenously interested across the board.

995
00:40:06,360 --> 00:40:08,480
But many of these precedents are very early.

996
00:40:08,480 --> 00:40:11,160
And now they're wielding them and they're leveraging them

997
00:40:11,160 --> 00:40:13,040
in a way that really makes a lot of sense.

998
00:40:13,040 --> 00:40:16,320
And so Xi can think about a 20-year goal.

999
00:40:16,320 --> 00:40:18,320
Trump has gotta get elected, you know?

1000
00:40:18,320 --> 00:40:19,680
He's gotta get elected.

1001
00:40:19,680 --> 00:40:21,920
And whoever's after him has gotta get elected.

1002
00:40:21,920 --> 00:40:26,440
For Xi, it's kind of like, he can set a 20-40 goal

1003
00:40:26,440 --> 00:40:31,440
and he can genuinely take a hard swing at that, right?

1004
00:40:31,520 --> 00:40:35,400
Deng Jinping was up there for Lord knows how long.

1005
00:40:35,400 --> 00:40:37,600
What was he, 90 or something when he stepped down?

1006
00:40:37,600 --> 00:40:40,920
It's a, so these guys can take a really good run at it.

1007
00:40:40,920 --> 00:40:43,840
And that ability to think long-term, I think,

1008
00:40:43,840 --> 00:40:45,880
puts us at a sincere disadvantage.

1009
00:40:45,920 --> 00:40:50,040
So do you see, this is actually a very interesting point.

1010
00:40:50,040 --> 00:40:54,440
Do you see the representative democracy as a system

1011
00:40:54,440 --> 00:40:57,560
is a system worthy of being disrupted

1012
00:40:57,560 --> 00:41:00,480
considering that your competitor is being driven

1013
00:41:00,480 --> 00:41:02,680
on a centrally driven kind of a system?

1014
00:41:02,680 --> 00:41:03,960
Which I also wanna add,

1015
00:41:03,960 --> 00:41:06,840
this was something brought up in Joe Rogan podcast,

1016
00:41:06,840 --> 00:41:09,000
as Salma mentioned, I keep bringing this up,

1017
00:41:09,000 --> 00:41:11,160
that China is run by engineers,

1018
00:41:11,160 --> 00:41:13,680
United States is run by lawyers.

1019
00:41:13,680 --> 00:41:15,680
So there's a very big difference,

1020
00:41:16,480 --> 00:41:17,640
not only implementing solutions,

1021
00:41:17,640 --> 00:41:20,880
but also recognizing problems and solutions

1022
00:41:20,880 --> 00:41:23,960
to be the most practical kind of solutions.

1023
00:41:23,960 --> 00:41:27,320
And we seem to be stuck in this representative democracy

1024
00:41:27,320 --> 00:41:29,200
and all the consequences of it.

1025
00:41:31,120 --> 00:41:33,840
That question, I think it's,

1026
00:41:33,840 --> 00:41:37,800
I would say this is an apt time to ask that question.

1027
00:41:37,800 --> 00:41:40,560
I would also say it is beyond my pale

1028
00:41:40,560 --> 00:41:42,560
to have firm and square opinions

1029
00:41:42,560 --> 00:41:44,640
about the how of the reconstruction.

1030
00:41:44,640 --> 00:41:45,840
Again, it's not my domain.

1031
00:41:45,840 --> 00:41:48,440
I'll speak to things that I feel confident about

1032
00:41:48,440 --> 00:41:49,800
and not so much ones I don't.

1033
00:41:49,800 --> 00:41:51,240
But I do believe,

1034
00:41:52,920 --> 00:41:55,080
I have reason to believe from my vantage point

1035
00:41:55,080 --> 00:41:56,960
and my own experience of where I feel like

1036
00:41:56,960 --> 00:41:59,160
I have expertise that's worthwhile to talk about

1037
00:41:59,160 --> 00:42:01,480
that it's warranted to question it.

1038
00:42:01,480 --> 00:42:03,760
Because some of these points,

1039
00:42:03,760 --> 00:42:05,440
so the whole point of that seven weaknesses

1040
00:42:05,440 --> 00:42:06,360
of the West article,

1041
00:42:06,360 --> 00:42:08,280
and I have some other pieces coming out about China

1042
00:42:08,280 --> 00:42:09,920
in the coming three or four months,

1043
00:42:09,920 --> 00:42:11,560
particularly around media.

1044
00:42:11,560 --> 00:42:12,400
That is to say,

1045
00:42:12,400 --> 00:42:14,560
our media is essentially built to divide us

1046
00:42:15,400 --> 00:42:16,840
and China gets free reign to come in however they want.

1047
00:42:16,840 --> 00:42:18,120
Buy up the movie theaters,

1048
00:42:18,120 --> 00:42:20,240
buy up the movie studios,

1049
00:42:20,240 --> 00:42:22,120
we're all using TikTok, right?

1050
00:42:22,120 --> 00:42:22,960
And what are they using?

1051
00:42:22,960 --> 00:42:24,040
Google and Facebook?

1052
00:42:24,040 --> 00:42:27,320
Nay, my comrade, nay, my comrade.

1053
00:42:27,320 --> 00:42:29,480
So they get to mold this,

1054
00:42:29,480 --> 00:42:31,000
what they would refer to as harmony,

1055
00:42:31,000 --> 00:42:34,960
we might refer to as something more akin to obedience.

1056
00:42:34,960 --> 00:42:37,200
Something more akin to like break your legs

1057
00:42:37,200 --> 00:42:40,000
if you do the wrong kind of yoga in the park,

1058
00:42:40,000 --> 00:42:40,840
Falun Gong.

1059
00:42:42,160 --> 00:42:43,800
But they would call it harmony.

1060
00:42:43,800 --> 00:42:45,520
So they were able to consciously mold that

1061
00:42:45,520 --> 00:42:47,240
through the entirety of their digital ecosystems

1062
00:42:47,240 --> 00:42:48,760
that were not a part of it.

1063
00:42:48,760 --> 00:42:51,320
They can buy the movie theaters out, brother.

1064
00:42:51,320 --> 00:42:55,200
They can buy the people who make the films out, brother.

1065
00:42:55,200 --> 00:42:57,240
They can buy up the big gaming companies.

1066
00:42:57,240 --> 00:42:58,080
You see Blizzard,

1067
00:42:58,080 --> 00:43:01,400
you're not allowed to talk about Uyghurs on Blizzard, right?

1068
00:43:01,400 --> 00:43:05,320
So media is its own ball of wax.

1069
00:43:05,320 --> 00:43:08,600
But yeah, I would say given those present dynamics,

1070
00:43:08,600 --> 00:43:09,840
there's someone I follow on Twitter

1071
00:43:09,840 --> 00:43:11,360
whose name now completely evades me,

1072
00:43:12,240 --> 00:43:15,480
who framed it very well, better than I had previously,

1073
00:43:15,480 --> 00:43:17,560
is that there's a default momentum

1074
00:43:17,560 --> 00:43:20,920
where we would then just follow and keep up with China

1075
00:43:20,920 --> 00:43:23,160
in terms of control and media

1076
00:43:23,160 --> 00:43:25,720
in order to make sure that we're not weaker than them.

1077
00:43:25,720 --> 00:43:27,480
That that might be a natural momentum,

1078
00:43:27,480 --> 00:43:28,680
which is dangerous, right?

1079
00:43:28,680 --> 00:43:32,320
To be a follower and to also follow of anybody, Mao.

1080
00:43:32,320 --> 00:43:35,040
I mean, if you're gonna give me one person to follow,

1081
00:43:35,040 --> 00:43:37,120
I will give you a million human beings

1082
00:43:37,120 --> 00:43:38,040
before I give you Mao.

1083
00:43:38,040 --> 00:43:40,760
I will give you literally a million men,

1084
00:43:40,960 --> 00:43:42,240
a million men or women.

1085
00:43:42,240 --> 00:43:44,400
I'll give you Thatcher, I'll give you, name him.

1086
00:43:44,400 --> 00:43:45,800
I'll give you Joan of Arc.

1087
00:43:45,800 --> 00:43:48,320
I mean, I don't care, anybody but Mao.

1088
00:43:48,320 --> 00:43:53,320
So I think that the default momentum is in that direction.

1089
00:43:54,160 --> 00:43:57,000
We do have to think about what does it look like

1090
00:43:57,000 --> 00:43:59,880
to maintain our strengths and our values

1091
00:43:59,880 --> 00:44:02,120
while also dealing with the reality

1092
00:44:02,120 --> 00:44:04,040
of the digital ecosystem that we exist in.

1093
00:44:04,040 --> 00:44:06,360
And one of my big contentions with defense

1094
00:44:06,360 --> 00:44:08,960
is that I think a tremendous amount of money

1095
00:44:08,960 --> 00:44:10,560
goes into things that rust,

1096
00:44:10,560 --> 00:44:12,720
that is to say tanks, et cetera, et cetera.

1097
00:44:12,720 --> 00:44:15,120
While China, I think intelligently,

1098
00:44:15,120 --> 00:44:17,400
is playing the economic and playing the digital

1099
00:44:17,400 --> 00:44:20,360
and media game, which is much more plausibly deniable.

1100
00:44:20,360 --> 00:44:21,320
And I think in the future

1101
00:44:21,320 --> 00:44:23,880
will be a much bigger lever of influence.

1102
00:44:23,880 --> 00:44:25,960
And so I think that they're playing smarter

1103
00:44:25,960 --> 00:44:27,640
than we are in that domain.

1104
00:44:27,640 --> 00:44:28,920
Very well said.

1105
00:44:28,920 --> 00:44:31,320
Do you think it's corruption that takes more money

1106
00:44:31,320 --> 00:44:34,560
towards the rustable things that you're talking about?

1107
00:44:34,560 --> 00:44:36,880
Like they have lobbyists and contractors

1108
00:44:36,880 --> 00:44:40,040
and connections with certain manufacturers.

1109
00:44:40,040 --> 00:44:42,440
You know, I don't know enough

1110
00:44:42,440 --> 00:44:47,440
about what is called the military industrial complex.

1111
00:44:47,600 --> 00:44:48,920
I don't know it.

1112
00:44:48,920 --> 00:44:50,760
In fact, I don't know enough

1113
00:44:50,760 --> 00:44:53,840
to even have a firm opinion on that phrase.

1114
00:44:54,960 --> 00:44:59,000
So I've spoken with folks in defense, writ large,

1115
00:44:59,000 --> 00:45:00,840
I consider them to be good folks,

1116
00:45:00,840 --> 00:45:04,560
but it wouldn't surprise me if there was all manner

1117
00:45:04,560 --> 00:45:09,320
of dealings and whatnot that might be less than ideal

1118
00:45:09,320 --> 00:45:11,440
and it might be kind of tainted incentives

1119
00:45:11,440 --> 00:45:13,080
in many different regards.

1120
00:45:14,080 --> 00:45:15,320
But I can't speak to it enough.

1121
00:45:15,320 --> 00:45:16,600
I don't have enough experience.

1122
00:45:16,600 --> 00:45:18,840
Is DARPA still the leading,

1123
00:45:18,840 --> 00:45:21,960
I guess you can call them agency or department or whatever

1124
00:45:21,960 --> 00:45:24,640
with respect to artificial intelligence?

1125
00:45:24,640 --> 00:45:25,560
Yeah, you know, they still,

1126
00:45:25,560 --> 00:45:27,600
they have so many different branches now.

1127
00:45:27,600 --> 00:45:32,120
And I think what I am heartened by

1128
00:45:32,120 --> 00:45:35,360
is that there are more of these efforts

1129
00:45:35,360 --> 00:45:38,480
to improve public private sector partnership,

1130
00:45:38,480 --> 00:45:39,920
really, really direct efforts.

1131
00:45:39,920 --> 00:45:41,800
We did a very robust report

1132
00:45:41,800 --> 00:45:45,080
about the all of the published AI strategy docs

1133
00:45:45,080 --> 00:45:46,560
from the US public sector.

1134
00:45:46,560 --> 00:45:49,520
So the AI.gov and all the permutations

1135
00:45:49,520 --> 00:45:50,800
that are publicly available.

1136
00:45:50,800 --> 00:45:51,800
What do they have in common?

1137
00:45:51,800 --> 00:45:53,440
What are the common initiatives?

1138
00:45:53,440 --> 00:45:55,880
A critical thrust across absolutely all of them

1139
00:45:55,880 --> 00:45:58,200
is improving public private sector partnerships.

1140
00:45:58,200 --> 00:46:00,360
And we see new organizations

1141
00:46:00,440 --> 00:46:02,320
like it used to be called the DIUX.

1142
00:46:02,320 --> 00:46:03,640
Now it's called the DIU.

1143
00:46:03,640 --> 00:46:08,280
It's sort of another sort of investment venture wing.

1144
00:46:08,280 --> 00:46:10,200
We have things like Incutel and DARPA

1145
00:46:10,200 --> 00:46:11,040
that have been around forever.

1146
00:46:11,040 --> 00:46:12,960
DARPA is obviously still a huge deal.

1147
00:46:13,920 --> 00:46:16,000
But we also have similar little,

1148
00:46:17,120 --> 00:46:19,680
we could call sort of like accelerator incubators

1149
00:46:19,680 --> 00:46:21,720
slash venture arms of even the Air Force

1150
00:46:21,720 --> 00:46:22,760
has their own shtick.

1151
00:46:22,760 --> 00:46:24,880
They have a little camp that they run in Boston

1152
00:46:24,880 --> 00:46:26,160
and a place that they rent out.

1153
00:46:26,160 --> 00:46:27,280
And I think they make investments

1154
00:46:27,280 --> 00:46:30,040
and work with tech talent and whatnot.

1155
00:46:30,440 --> 00:46:33,400
So there's a spidering proliferation of these groups.

1156
00:46:34,760 --> 00:46:36,560
I don't know enough about their coordination

1157
00:46:36,560 --> 00:46:38,280
to say if I feel good about that.

1158
00:46:38,280 --> 00:46:40,000
My inkling is it's not amazing

1159
00:46:40,000 --> 00:46:42,520
because defense is very complicated.

1160
00:46:42,520 --> 00:46:44,640
But yeah, DARPA still, I would say the best known

1161
00:46:44,640 --> 00:46:46,600
to the best of my knowledge.

1162
00:46:46,600 --> 00:46:48,960
But luckily it's proliferating.

1163
00:46:48,960 --> 00:46:49,960
Excellent.

1164
00:46:49,960 --> 00:46:52,520
Based on the data and understanding

1165
00:46:52,520 --> 00:46:54,640
that you have gathered within these years,

1166
00:46:54,640 --> 00:46:58,720
you must have some kind of end game kind of a picture

1167
00:46:58,720 --> 00:47:02,280
for US AI supremacy and Chinese AI supremacy.

1168
00:47:02,280 --> 00:47:04,200
Do you have something like that?

1169
00:47:04,200 --> 00:47:05,960
I do, I've written an article on it.

1170
00:47:05,960 --> 00:47:10,760
So what are some of the drastic contrast

1171
00:47:10,760 --> 00:47:12,360
between those kind of words?

1172
00:47:12,360 --> 00:47:17,040
The word where AI is driven by a US supremacy

1173
00:47:17,040 --> 00:47:20,200
or the world that AI is driven by Chinese supremacy?

1174
00:47:21,360 --> 00:47:24,120
Yeah, you know, I try my hardest

1175
00:47:24,120 --> 00:47:27,480
to not think about that as my default frame.

1176
00:47:27,520 --> 00:47:28,800
It's very hard not to

1177
00:47:28,800 --> 00:47:30,680
because of the whole Thucydides trap

1178
00:47:30,680 --> 00:47:34,440
and because of the level of tension

1179
00:47:34,440 --> 00:47:36,200
that we're dealing with right now.

1180
00:47:37,600 --> 00:47:40,920
But in my heart of hearts as a first swing,

1181
00:47:40,920 --> 00:47:43,640
I have an article called We Unite or We Fight,

1182
00:47:43,640 --> 00:47:47,480
so E-M-E-R-J, We Unite or We Fight,

1183
00:47:47,480 --> 00:47:50,120
which is really around getting on the same page.

1184
00:47:51,200 --> 00:47:53,320
It's a horrible word, but same-pagedness,

1185
00:47:53,320 --> 00:47:54,920
we could call it solidarity,

1186
00:47:54,920 --> 00:47:58,840
around what kind of 10-year, 20-year,

1187
00:47:58,840 --> 00:48:03,280
even maybe longer, visions we want for a global society

1188
00:48:03,280 --> 00:48:05,600
and being able to somewhere agree on that,

1189
00:48:05,600 --> 00:48:09,680
to align and have transparency and steering,

1190
00:48:09,680 --> 00:48:11,080
shared transparency and steering

1191
00:48:11,080 --> 00:48:14,680
around where the big trajectory actually goes.

1192
00:48:14,680 --> 00:48:17,920
That said, I'm not necessarily an optimist

1193
00:48:17,920 --> 00:48:21,560
to that same-pagedness between China as it stands today

1194
00:48:21,560 --> 00:48:24,520
and the US is really all that possible.

1195
00:48:24,560 --> 00:48:27,880
If China had the government of Japan, for example,

1196
00:48:29,800 --> 00:48:33,840
I probably really wouldn't be all that democratic nation.

1197
00:48:33,840 --> 00:48:35,760
I mean, I just don't really think

1198
00:48:35,760 --> 00:48:38,880
there's gonna be that many problems with that.

1199
00:48:38,880 --> 00:48:42,080
I mean, Japan's a different ballgame than us,

1200
00:48:42,080 --> 00:48:45,800
but democracy-wise, and people are people,

1201
00:48:45,800 --> 00:48:48,080
there's no issues with individual Chinese folks

1202
00:48:48,080 --> 00:48:52,560
by any means, but yeah, I feel as though

1203
00:48:52,560 --> 00:48:54,600
the kind of global solidarity and alignment

1204
00:48:54,600 --> 00:48:56,480
that would be required to get on the same page

1205
00:48:56,480 --> 00:48:58,480
about what are we to turn into,

1206
00:48:58,480 --> 00:49:00,120
what is the human experience to be,

1207
00:49:00,120 --> 00:49:02,560
what is the global community to bloom into

1208
00:49:02,560 --> 00:49:04,000
beyond the present human experience,

1209
00:49:04,000 --> 00:49:05,960
because we're not really gonna be able to stop that train,

1210
00:49:05,960 --> 00:49:07,120
hopefully we can guide it,

1211
00:49:07,120 --> 00:49:08,680
hopefully it can be something other than war

1212
00:49:08,680 --> 00:49:10,640
that drives it forward.

1213
00:49:10,640 --> 00:49:14,600
So that article articulates what my hoped vision would be,

1214
00:49:14,600 --> 00:49:17,640
which is not war, which is solidarity,

1215
00:49:18,800 --> 00:49:21,000
but I'll tell you, to get to that solidarity,

1216
00:49:22,720 --> 00:49:24,920
that's, there's gotta be a smarter guy than me for that one.

1217
00:49:24,920 --> 00:49:28,200
Sometimes just parts come from different kind of

1218
00:49:28,200 --> 00:49:29,880
experience and background.

1219
00:49:29,880 --> 00:49:32,320
It's been true with certain religions that

1220
00:49:33,440 --> 00:49:36,200
some religions, they just do not wanna get along

1221
00:49:36,200 --> 00:49:38,520
with other religions or other competitors.

1222
00:49:38,520 --> 00:49:39,960
They demand submission.

1223
00:49:39,960 --> 00:49:41,240
That's what they're demanding.

1224
00:49:41,240 --> 00:49:44,760
However, we fool ourselves in this part of the world.

1225
00:49:44,760 --> 00:49:49,360
That doesn't change their status quo and default state.

1226
00:49:49,360 --> 00:49:52,520
So it seems like there is a fundamental disconnect

1227
00:49:53,480 --> 00:49:55,000
between the Chinese Communist Party,

1228
00:49:55,000 --> 00:49:58,320
and I'm not dividing based on good and bad

1229
00:49:58,320 --> 00:50:02,400
or any kind of ethical or moral judgment,

1230
00:50:02,400 --> 00:50:05,320
but they're just coming from a very different kind of a place

1231
00:50:05,320 --> 00:50:08,000
and looking for a very different kind of an outcome

1232
00:50:08,000 --> 00:50:10,400
than the United States.

1233
00:50:11,800 --> 00:50:14,000
I think that is certainly the case.

1234
00:50:14,880 --> 00:50:17,560
What I will say about the United States,

1235
00:50:17,560 --> 00:50:21,160
as absolutely horrible as I think we're doing right now

1236
00:50:21,160 --> 00:50:23,480
on the global stage from Trump to you name it.

1237
00:50:23,480 --> 00:50:28,480
I mean, as just brutal as our perception is globally,

1238
00:50:28,640 --> 00:50:31,720
our value prop is, it ain't that bad.

1239
00:50:31,720 --> 00:50:33,040
It ain't that bad, man.

1240
00:50:33,040 --> 00:50:37,400
I mean, there's a lot of countries that dig our value prop

1241
00:50:37,400 --> 00:50:40,000
more than whatever they were rocking with

1242
00:50:40,000 --> 00:50:41,000
before they had it.

1243
00:50:41,000 --> 00:50:44,360
And I'm now, am I advocating for colonial,

1244
00:50:44,360 --> 00:50:45,960
not even freaking close, right?

1245
00:50:45,960 --> 00:50:47,360
But somebody would misinterpret that

1246
00:50:47,360 --> 00:50:50,520
because that's the world we live in right now.

1247
00:50:50,520 --> 00:50:53,280
What I will say is that just as a culture,

1248
00:50:53,280 --> 00:50:55,840
the general value prop I think is really strong.

1249
00:50:55,840 --> 00:50:58,360
It's very hard for China to,

1250
00:50:58,360 --> 00:51:02,000
so I believe there's a permutation of a Machiavelli quote,

1251
00:51:02,000 --> 00:51:04,320
something akin to, and this might be bastardized

1252
00:51:04,320 --> 00:51:06,320
and I apologize, but something akin to,

1253
00:51:06,320 --> 00:51:11,320
when you take over a country or a county or what have you,

1254
00:51:11,320 --> 00:51:13,920
you have the choice to let them keep their arms

1255
00:51:13,920 --> 00:51:16,000
or to take away their arms.

1256
00:51:16,000 --> 00:51:17,920
But you can't let them keep their arms

1257
00:51:17,920 --> 00:51:20,160
and then later take away their arms.

1258
00:51:20,160 --> 00:51:22,880
And I think that when it comes to China

1259
00:51:22,880 --> 00:51:26,680
selling their value proposition to other countries,

1260
00:51:26,680 --> 00:51:29,560
those countries would have to be at brutal war

1261
00:51:29,560 --> 00:51:31,240
with each other for a very long time

1262
00:51:31,240 --> 00:51:34,200
and essentially lose all semblance of a stable democracy

1263
00:51:34,200 --> 00:51:38,760
before you would say yay to the kind of vulgar tyranny

1264
00:51:38,760 --> 00:51:41,040
of the Chinese Communist Party.

1265
00:51:41,960 --> 00:51:46,400
And so I think that the sale of their schtick

1266
00:51:46,400 --> 00:51:47,880
is a tougher sell.

1267
00:51:47,880 --> 00:51:51,360
And so if we have nothing else going for us in the States,

1268
00:51:52,280 --> 00:51:55,560
it would be that you can tweet what you want,

1269
00:51:55,560 --> 00:52:00,120
including about the man who purportedly runs the country

1270
00:52:00,120 --> 00:52:02,720
and you're gonna go to bed just fine.

1271
00:52:02,720 --> 00:52:04,680
Yeah, the fundamental American values,

1272
00:52:04,680 --> 00:52:09,120
there I say constitutional values, absolutely.

1273
00:52:09,120 --> 00:52:13,560
Dare you say, yeah, you're a Bible thumper, aren't you?

1274
00:52:13,560 --> 00:52:15,760
Must be, and white supremacists, I guess.

1275
00:52:17,920 --> 00:52:21,240
Anyway, but yes, I just think about them as,

1276
00:52:21,240 --> 00:52:23,920
in the OECD, I have a great,

1277
00:52:23,920 --> 00:52:28,840
I have a really warm and abiding respect

1278
00:52:28,840 --> 00:52:31,360
for the OECD as an organization.

1279
00:52:31,360 --> 00:52:34,400
I really recommend people interested in solidarity,

1280
00:52:34,400 --> 00:52:36,440
certainly in sort of the broad Western dynamic

1281
00:52:36,440 --> 00:52:39,200
versus the hardcore communism

1282
00:52:39,200 --> 00:52:41,040
to learn more about the OECD,

1283
00:52:41,040 --> 00:52:43,680
but they refer to it as like-minded nations, right?

1284
00:52:43,680 --> 00:52:45,480
It's not any one person's constitution.

1285
00:52:45,480 --> 00:52:47,160
It's just, can you say what you want

1286
00:52:47,160 --> 00:52:48,400
and nobody breaks your legs?

1287
00:52:48,400 --> 00:52:50,760
Can you pray to what you want and nobody breaks your legs?

1288
00:52:50,760 --> 00:52:52,720
Can you come together and congregate how you want

1289
00:52:52,720 --> 00:52:54,080
and nobody breaks your legs?

1290
00:52:54,080 --> 00:52:57,040
It's just basic stuff, it's just basic stuff.

1291
00:52:57,040 --> 00:52:58,360
So for me, it's not even constitutional,

1292
00:52:58,360 --> 00:52:59,960
it's just like-minded nations

1293
00:52:59,960 --> 00:53:03,600
where somewhat essential sort of freedoms,

1294
00:53:03,600 --> 00:53:06,400
as far back as Greece or we wanna tie to John Locke

1295
00:53:06,400 --> 00:53:08,320
or I don't really care who gets the credit,

1296
00:53:08,320 --> 00:53:10,600
just the kinds of freedoms that we enjoy, right?

1297
00:53:10,600 --> 00:53:13,040
So I can extrapolate it even beyond the constitution

1298
00:53:13,040 --> 00:53:13,920
if we wanted to.

1299
00:53:13,920 --> 00:53:15,120
Yeah, interesting.

1300
00:53:15,120 --> 00:53:18,680
Transparency is the key, definitely.

1301
00:53:18,680 --> 00:53:21,120
Are you at all confident and optimistic

1302
00:53:21,120 --> 00:53:24,600
about decentralization on the basis of blockchain

1303
00:53:24,600 --> 00:53:26,200
to reach some of these goals?

1304
00:53:30,200 --> 00:53:33,400
I don't know enough about blockchain

1305
00:53:33,400 --> 00:53:35,760
to have a square opinion there.

1306
00:53:35,760 --> 00:53:37,520
What I will tell you is this,

1307
00:53:37,520 --> 00:53:40,240
there appears to be very little blockchain

1308
00:53:40,240 --> 00:53:44,320
tinkering its way into the actual enterprise right now.

1309
00:53:44,320 --> 00:53:45,840
Now, the same could be said of AI,

1310
00:53:45,840 --> 00:53:48,960
but AI is much farther along in let's say big banks,

1311
00:53:48,960 --> 00:53:53,360
big media companies, e-commerce, manufacturing.

1312
00:53:53,360 --> 00:53:56,400
Artificial intelligence is eking its way in in the corners.

1313
00:53:56,400 --> 00:53:58,760
I'm not saying it's easy and at a fun time,

1314
00:53:58,760 --> 00:53:59,920
but it's making a difference.

1315
00:53:59,920 --> 00:54:01,880
It's clearly going to be transformative.

1316
00:54:01,880 --> 00:54:03,920
Blockchain, I don't think that the use cases

1317
00:54:03,920 --> 00:54:06,080
are so nascent that I really don't have firm opinions.

1318
00:54:06,080 --> 00:54:08,000
When I look at the intersection of blockchain and AI,

1319
00:54:08,000 --> 00:54:09,640
which I have done some research on,

1320
00:54:12,560 --> 00:54:13,560
it's nascent to the point

1321
00:54:13,560 --> 00:54:15,400
where I literally don't care about it yet.

1322
00:54:15,400 --> 00:54:17,960
I mean, didn't you talk about that with Ben Gertzell?

1323
00:54:17,960 --> 00:54:20,400
Because Ben Gertzell's Singularity Net is-

1324
00:54:20,400 --> 00:54:23,720
And I have a tremendous respect for Gertzell, by the way.

1325
00:54:23,720 --> 00:54:26,960
I mean, I consider Gertzell, in my personal opinion,

1326
00:54:26,960 --> 00:54:30,240
he's one of probably four living intellectuals

1327
00:54:30,240 --> 00:54:33,360
who I would say have really influenced my thought

1328
00:54:33,400 --> 00:54:38,120
and who I consider to be toweringly intelligent.

1329
00:54:38,120 --> 00:54:41,200
And so I have a great respect for Gertzell.

1330
00:54:41,200 --> 00:54:44,360
But yeah, I mean, and I hope his firm succeeds.

1331
00:54:44,360 --> 00:54:45,200
I'm rooting for him.

1332
00:54:45,200 --> 00:54:46,480
I'm rooting for Singularity Net.

1333
00:54:46,480 --> 00:54:49,920
It still seems to me to be a great many different ideas

1334
00:54:49,920 --> 00:54:51,760
that they're exploring in terms of what the business model

1335
00:54:51,760 --> 00:54:53,040
will be and how it's going to work out.

1336
00:54:53,040 --> 00:54:55,280
And if anybody can do it, hopefully it's him.

1337
00:54:55,280 --> 00:54:56,120
But I think-

1338
00:54:56,120 --> 00:54:57,120
Yeah, what can lead to AGI?

1339
00:54:57,120 --> 00:54:59,320
That's the thing, because we don't really know

1340
00:54:59,320 --> 00:55:02,080
what can lead to AGI at this point.

1341
00:55:02,080 --> 00:55:02,960
That's exactly it.

1342
00:55:03,520 --> 00:55:04,360
Yeah, we don't have a clue yet.

1343
00:55:04,360 --> 00:55:06,000
So Ben's got some theories, though,

1344
00:55:06,000 --> 00:55:08,360
and he's got more informed theories than me.

1345
00:55:08,360 --> 00:55:10,200
He's been thinking about it since I was five years old.

1346
00:55:10,200 --> 00:55:11,840
Fascinating, fascinating individual.

1347
00:55:13,480 --> 00:55:16,840
We talked about having an hour long conversation,

1348
00:55:16,840 --> 00:55:18,560
so I don't wanna leak.

1349
00:55:18,560 --> 00:55:20,360
Yeah, you have a two o'clock, unfortunately.

1350
00:55:20,360 --> 00:55:21,200
Okay.

1351
00:55:21,200 --> 00:55:22,040
I'm gonna blast with you.

1352
00:55:22,040 --> 00:55:22,860
It's been a real fun day.

1353
00:55:22,860 --> 00:55:23,700
Yeah, man.

1354
00:55:23,700 --> 00:55:26,000
Dan's website is emerge.com.

1355
00:55:26,000 --> 00:55:28,280
Go there and find all the other information.

1356
00:55:28,280 --> 00:55:31,800
Let me ask you the last question I ask all my guests.

1357
00:55:31,800 --> 00:55:34,040
That if you come across an intelligent alien

1358
00:55:34,040 --> 00:55:35,520
from a different civilization,

1359
00:55:35,520 --> 00:55:39,080
what would you say is the worst thing humanity has done?

1360
00:55:39,080 --> 00:55:41,680
And what would you say is our greatest achievement?

1361
00:55:43,400 --> 00:55:44,520
Good gracious, brother.

1362
00:55:44,520 --> 00:55:45,800
I have no idea.

1363
00:55:45,800 --> 00:55:47,600
What is the worst thing we've done?

1364
00:55:48,720 --> 00:55:53,600
Well, I mean, tribal and religious wars seem pretty rough.

1365
00:55:53,600 --> 00:55:54,760
That's a very big umbrella.

1366
00:55:54,760 --> 00:55:56,620
I apologize for the vagary there.

1367
00:55:57,920 --> 00:56:01,280
And in terms of the best things that we've done,

1368
00:56:01,640 --> 00:56:06,640
I think it's determined ways to both govern ourselves

1369
00:56:07,080 --> 00:56:09,320
and progress forward in terms of our potential

1370
00:56:09,320 --> 00:56:13,360
at the same time with reasonable periods of peace.

1371
00:56:13,360 --> 00:56:14,880
I think that that's a worthy,

1372
00:56:14,880 --> 00:56:19,000
it's a laudable achievement for hairless apes like ourselves.

1373
00:56:19,000 --> 00:56:21,520
And I think we can deserve a pat on the back for that one.

1374
00:56:21,520 --> 00:56:23,360
I think that's a worthy achievement for hairless apes like ourselves.

1375
00:56:51,520 --> 00:56:52,760
I think that's a worthy achievement for hairless apes like ourselves.

