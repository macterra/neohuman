WEBVTT

00:00.000 --> 00:06.560
up until now, human labor has been indispensable to the whole economy. But if in the future,

00:06.560 --> 00:11.840
at least for a lot of people, it's not going to be so important that they work because

00:11.840 --> 00:15.240
technology is going to take over more of this work, then we've got to shift our values a

00:15.240 --> 00:22.080
little bit and be more open to providing an income to people that don't have traditional

00:22.080 --> 00:32.720
full-time work in a way that we would expect today.

00:32.720 --> 00:39.480
Hello and welcome to the 37th episode of Neo Human Podcast. I'm Agah Bahari, an agologist

00:39.480 --> 00:47.440
on Twitter and Instagram, and you can follow the show on LiveInLimbo.com, iTunes, and YouTube.

00:47.440 --> 00:51.560
With me today, I have Martin Ford. Welcome to the show, Martin.

00:51.560 --> 00:52.560
Thanks for having me.

00:52.560 --> 00:57.160
It's my pleasure. Why don't we start by some of your background, the works you've done,

00:57.160 --> 01:00.000
and what you're focusing on mainly now these days?

01:00.000 --> 01:05.080
Well, I've written two books, both of which are focused on the impact that robotics and

01:05.080 --> 01:09.240
artificial intelligence are going to have on the economy and especially the job market.

01:09.240 --> 01:13.120
I mean, the basic thesis of the books is that a lot of jobs are going to be automated. That's

01:13.120 --> 01:18.880
something that's interested me for a while. I started out running a small software business

01:18.880 --> 01:24.440
here in Silicon Valley. I started that back in the 1990s, and I saw even in that little

01:24.440 --> 01:30.920
business the impact on jobs. It used to be that software was shipped on tangible media,

01:30.920 --> 01:35.360
on CD-ROMs, and so there was work there for people to do the fulfillment to actually ship

01:35.360 --> 01:40.440
that product to customers, but that disappeared very rapidly. That's one of the things I think

01:40.440 --> 01:45.480
that got me really focused on this issue of this coming impact.

01:45.480 --> 01:51.640
Back in 2009, I wrote my first book, which was called Lights in a Tunnel. That was self-published

01:51.640 --> 01:55.440
but did well enough that eventually led to the opportunity to write my second book, Rise

01:55.440 --> 02:01.120
of the Robots, which has gotten a lot more attention. That's how I got into this and

02:01.120 --> 02:03.720
became really focused on this particular issue.

02:03.720 --> 02:06.320
Because of personal experience, basically.

02:06.320 --> 02:11.640
Yeah, and that and also, of course, working in the industry and being very close to Moore's

02:11.640 --> 02:18.840
law to seeing what has happened with the acceleration of computing power. So, those two things together,

02:18.840 --> 02:19.840
I guess.

02:19.840 --> 02:24.520
Well, let's start from a very basic place. What is automation and how wide the impact

02:24.520 --> 02:28.240
of automation is now and will be in the coming years and decades?

02:28.240 --> 02:35.640
Well, automation is just any substitution of machines or computers for people in the workplace.

02:35.640 --> 02:39.400
And of course, that's something that's been going on for hundreds of years since the Luddite

02:39.400 --> 02:46.760
revolt. And certainly, within manufacturing, for example, it's already had a dramatic impact

02:46.760 --> 02:51.160
in terms of making factories in developed countries like the United States much less

02:51.160 --> 02:58.880
labor intensive. And that isn't new. What is new is that automation is now becoming

02:58.880 --> 03:03.760
artificial intelligence. And what that means is that it's pushing into the cognitive realm.

03:03.760 --> 03:10.440
So it's not just about machines that replace physical movements and manual labor anymore.

03:10.440 --> 03:18.560
It's about machines that are getting better and better at displacing cognitive skills.

03:18.560 --> 03:23.680
And most importantly, learning, our ability to learn and adapt, which is what all machine

03:23.680 --> 03:29.720
learning is all about. Partly because of the acceleration that's been going on for such

03:29.720 --> 03:34.280
a long time, we're now at a point where things are just moving at a dramatic pace. And we're

03:34.280 --> 03:39.280
at the point where the machines are starting to think, at least in a limited way. And that's

03:39.280 --> 03:43.960
going to be quite disruptive. Aside from technology, it seems like for centuries

03:43.960 --> 03:49.400
we've had the mentality to skip the work we don't want to do by outsourcing it to others.

03:49.400 --> 03:53.680
And it has resulted in the current civilization that we're living in. Would it be an accurate

03:53.680 --> 03:58.200
assumption that the rise of automation is a natural continuation of that mentality and

03:58.200 --> 04:02.920
approach? Sure. I mean, it's also driven by the market,

04:02.920 --> 04:09.480
right? I mean, the market creates a very powerful incentive for any business to economize on

04:09.480 --> 04:15.160
all its inputs, but certainly including it in that labor, which is for most businesses,

04:15.160 --> 04:20.640
the biggest expense that they have. So yeah, you're right. It's a natural progression

04:20.640 --> 04:27.680
of capitalism, really. And that, you know, so it's not anything particularly surprising.

04:27.680 --> 04:33.480
It's just that the technology is finally providing capabilities that weren't there before.

04:33.480 --> 04:37.360
And the consumers and workers who are being affected by it seems to be people who are

04:37.360 --> 04:44.200
feeding data to this whole system to begin with, right? Because a lot of people, for

04:44.200 --> 04:49.280
example, they complain about privacy, but they still share everything on Facebook and

04:49.280 --> 04:54.560
they use their smartphone for many different aspects of their life. They're basically training

04:54.560 --> 05:00.800
those artificial intelligence and workers in the factories, training those robots to

05:00.800 --> 05:05.320
catch on the pattern and then just do it again and again and again and again and improve

05:05.320 --> 05:08.640
upon it. Yeah, I mean, that's essentially right.

05:08.640 --> 05:13.760
I mean, there's enormous amounts of data being collected, especially inside corporations,

05:13.760 --> 05:19.840
big businesses. And in many cases, that data will include, will kind of encapsulate a lot

05:19.840 --> 05:23.720
of jobs, you know, and so eventually a machine learning algorithm will be able to go through

05:23.720 --> 05:29.600
that and figure out how to do a lot of things. And that's, you know, there's no easy way

05:29.600 --> 05:33.000
to turn that around. I think that, again, that's a natural thing that's going to continue

05:33.000 --> 05:36.920
to happen. So we have to figure out a way to adapt to all this, because it's going

05:36.920 --> 05:41.240
to be, I think, quite disruptive. Exactly, because technology is advancing exponentially

05:41.240 --> 05:46.760
while we seem to be hardwired to be linear. So do you think that at some point it will

05:46.760 --> 05:52.280
be necessary for us biological humans to start merging with technology more and more just

05:52.280 --> 05:55.600
so we can evolve with technology instead of being left behind?

05:55.600 --> 06:01.560
Well, that's one of the, you know, the big ideas in Silicon Valley and people like Ray

06:01.560 --> 06:07.160
Kurzweil and Peter Thiel and so forth certainly buy into that. I think that's absolutely true.

06:07.160 --> 06:12.040
To some extent, that's natural. Of course, we're going to have technology that will enhance

06:12.040 --> 06:16.760
us in many ways. Whether there's going to be this huge movement that will turn us all

06:16.760 --> 06:21.120
into cyborgs or not, I don't know. I think that's very speculative, but certainly we're

06:21.120 --> 06:28.000
going to incorporate more and more technology into ourselves. My view is that that won't

06:28.000 --> 06:34.540
necessarily solve this problem of, you know, jobs disappearing. But I do think it's probably

06:34.540 --> 06:40.960
an inevitable thing that's going to happen. How do you see humans evolving, if not into

06:40.960 --> 06:46.480
cyborgs? Well, I mean, I think that we will either

06:46.480 --> 06:54.000
remain relatively human and therefore it will be harder for us to keep up with the technology

06:54.000 --> 06:59.280
or maybe we will incorporate machines and brain implants and things like that to enhance

06:59.280 --> 07:05.000
our capabilities. Maybe it will be more on the biological side. You know, there's going

07:05.000 --> 07:11.120
to be certainly genetic engineering going on and especially in China. You know, they're

07:11.120 --> 07:16.920
doing a lot of research into how they might be able to essentially create smarter people

07:16.920 --> 07:23.640
through genetic engineering. That kind of thing is, you know, here in the West, it kind

07:23.640 --> 07:31.920
of is tainted with eugenics and all of that and we're a bit put off by that. So I don't

07:31.920 --> 07:35.720
think, you know, we could well see that the countries like China, where they don't see

07:35.720 --> 07:39.420
that as a taboo necessarily, are going to make a lot more progress. So there are many

07:39.420 --> 07:43.840
directions that we can evolve and I don't pretend to know what they are. The question

07:43.840 --> 07:49.880
though is, are human beings going to be able to keep up with artificial intelligence? And

07:49.880 --> 07:54.080
certainly there are good reasons to believe that the answer to that is going to be no,

07:54.080 --> 07:59.720
at least for a lot of typical people in our population. So just thinking specifically

07:59.720 --> 08:05.520
about the job market and the impact on work, I think it's very likely that all of this

08:05.520 --> 08:10.740
is going to greatly increase inequality and probably a lot of average typical workers

08:10.740 --> 08:15.920
are probably going to find it harder and harder to find a foothold in this economy. So they're

08:15.920 --> 08:18.720
probably going to be left behind.

08:18.720 --> 08:22.960
Universal basic income has been introduced as one of the solutions or the main solution

08:22.960 --> 08:29.520
for unemployment resulted by automation. What are your thoughts on UBI and are there any

08:29.520 --> 08:35.000
other ways beside UBI to create safety nets for those workers who are inevitably are going

08:35.000 --> 08:36.000
to lose their jobs?

08:36.000 --> 08:42.640
Well, yeah, I'm generally a proponent of a UBI. That's what I propose in my book. I do

08:42.640 --> 08:49.480
think it can be refined and improved in some ways. Most importantly, I've suggested that

08:49.480 --> 08:55.340
you could build incentives in it, especially for education. So imagine you're a struggling

08:55.340 --> 08:59.600
high school student and you're at risk of dropping out of school. Now, if you know that

08:59.600 --> 09:03.960
no matter what, you're going to end up with a basic income, same basic income as everyone

09:03.960 --> 09:05.960
else, then that creates a...

09:05.960 --> 09:10.360
Why would you go to school to begin with then?

09:10.360 --> 09:14.340
So I think what we should do is maybe pay people who graduate from high school a little

09:14.340 --> 09:19.700
bit more than those who just drop out, create those kinds of incentives, maybe also incentives

09:19.700 --> 09:24.280
to work in the community to do useful things. And that helps to also address the issue of

09:24.280 --> 09:28.360
what would people do then? Are they going to just take drugs and play video games if

09:28.360 --> 09:32.760
everyone has a basic income? So there are a lot of issues that need to be worked out

09:32.760 --> 09:36.840
there. I think that a basic income, as an idea, it's just kind of a starting point and

09:36.840 --> 09:44.240
we can refine it further. It is, of course, also tremendously expensive. And the problem

09:44.240 --> 09:50.360
is that it goes against our values in many cases, because most people don't like the

09:50.360 --> 09:55.320
idea of just giving people money. I mean, they disparage that as paying people to be

09:55.320 --> 09:58.040
alive or something like that.

09:58.040 --> 10:03.640
What I would say is that up until this point, our values have really emphasized production.

10:03.640 --> 10:07.640
And there's a good reason for that, of course, because up until now, human labor has been

10:07.640 --> 10:13.800
indispensable to the whole economy. But if in the future, at least for a lot of people,

10:13.800 --> 10:18.520
it's not going to be so important that they work because technology is going to take over

10:18.520 --> 10:23.620
more of this work, then we got to shift our values a little bit. And so to be more open

10:23.620 --> 10:30.040
to providing an income to people that don't have traditional full-time work in a way that

10:30.040 --> 10:34.080
we would expect today. And that's a big value shift. I mean, it's going to be very, very

10:34.080 --> 10:40.120
hard to accomplish that, especially here in the US, where we're more conservative than

10:40.120 --> 10:43.720
many other countries.

10:43.720 --> 10:48.680
So I do think it's a good idea. It's going to be very difficult to implement or to get

10:48.680 --> 10:53.560
people to accept the idea. But my feeling is that as this trend develops and we see

10:53.560 --> 10:56.760
a bigger and bigger impact on the workforce, it's almost inevitable that we have to move

10:56.760 --> 11:04.120
in that direction, something like that. If not that, then something probably equally

11:04.120 --> 11:05.480
radical.

11:05.480 --> 11:10.600
There are other ideas I've heard of proposed, but most of those are, I think, even more

11:10.600 --> 11:11.600
difficult.

11:11.600 --> 11:14.320
What are some of them? Can you give me an example?

11:14.320 --> 11:18.520
One that's been proposed is simply to give people lots of capital, give them an ownership

11:18.520 --> 11:24.840
and a mutual fund or something. Just give them money, not on an ongoing basis as income,

11:24.840 --> 11:31.200
but as a large sum to allow everyone to be a capitalist. I think that's even more politically

11:31.200 --> 11:33.000
infeasible than a basic income.

11:33.000 --> 11:37.240
But Martin, what is going to happen to the value of money itself, to the concept of money

11:37.240 --> 11:43.080
itself? Because money will be made out of the work that robots are doing. Bill Gates

11:43.080 --> 11:50.680
is saying that we have to tax robots. But what is going to happen to the value of money

11:50.680 --> 11:55.080
itself? Because people don't need to work as hard regardless of if they're going to

11:55.080 --> 12:03.600
keep their jobs or not. So what is the value system going to be in the next couple of years?

12:03.600 --> 12:08.520
Our values have to shift, but in terms of money, money is just a mechanism. It's what

12:08.520 --> 12:16.640
we use to enable transactions. So I don't think this changes the value of money as long

12:16.640 --> 12:20.400
as it's managed well and it's done in a way that doesn't create inflation, for example.

12:20.400 --> 12:27.240
But there's no particular reason it should. Inflation happens when you've got too much

12:27.240 --> 12:34.280
money out there and not enough things for people to buy. In the past, if you go back

12:34.280 --> 12:42.360
to the 1970s, there was a wage and price spiral where prices kept up and then people demanded

12:42.360 --> 12:48.240
higher wages off into the unions and so wages went off. But we don't see that anymore. That

12:48.240 --> 12:56.680
cycle has pretty much been broken, at least in the US. So I think none of this affects

12:56.680 --> 12:59.400
what money is. That's a separate question.

12:59.400 --> 13:05.160
So would you consider this whole argument about raising a minimum wage a viable argument?

13:05.160 --> 13:09.680
Because to me, it seems like when you raise a minimum wage, it just makes it harder for

13:09.680 --> 13:13.920
people with less education and less skill to get jobs to begin with because the businesses

13:13.920 --> 13:20.380
are spending more. Therefore, they're going to hire people who are more capable or they

13:20.380 --> 13:24.400
completely go towards automation to skip all of that.

13:24.400 --> 13:30.520
Right. It's a complex thing. My feeling is that no matter whether you raise the minimum

13:30.520 --> 13:36.760
wage or not, this phenomenon with automation is going to happen. Here in the US, the minimum

13:36.760 --> 13:42.480
wage is pretty low relative to a lot of other countries. So I'm not necessarily against

13:42.480 --> 13:48.120
raising it. I do think that it might, in some cases, produce automation sooner. But on the

13:48.120 --> 13:52.720
other hand, it would be helpful to a lot of people right now that are really struggling.

13:52.720 --> 13:55.760
If you're going to say we should never raise the minimum wage because of automation, then

13:55.760 --> 14:01.840
really what you're saying is that the solution to automation is to allow wages for people

14:01.840 --> 14:05.640
to sort of fall towards zero. And that's really not a solution.

14:05.640 --> 14:11.440
Now, if, however, you had a basic income, people would still have an incentive to work

14:11.440 --> 14:14.880
on the whole idea of a basic income as you implement it in a way that it does not destroy

14:14.880 --> 14:19.800
the incentive for people to work too. So people have that basic income and then they can work

14:19.800 --> 14:24.040
and earn more. So I think if you had a basic income, then maybe you could get rid of the

14:24.040 --> 14:29.920
minimum wage because people would already have a safety net. Beyond that, you could

14:29.920 --> 14:36.840
let people work for whatever the market says. So that's one way to approach it.

14:36.840 --> 14:40.680
Well, let me ask it in a different way because the reason I mentioned minimum wage because

14:40.680 --> 14:45.840
there's a movement behind it that they're using raising minimum wage as one of the main

14:45.840 --> 14:50.480
policies that they're backing. Do you think that the public is sufficiently informed and

14:50.480 --> 14:54.960
educated to adapt to the automated world?

14:54.960 --> 15:01.520
Well, I don't hear much about automation from the people who are protesting for raising

15:01.520 --> 15:06.080
minimum wage, for example. They're not as passionate about it or they don't know about

15:06.080 --> 15:07.760
it. I don't really know.

15:07.760 --> 15:15.040
I think in terms of all of this, it's still not something that's really visible to the

15:15.040 --> 15:18.320
public so much. I mean, definitely a lot of people are concerned about it. I go around

15:18.320 --> 15:22.680
doing speaking engagements all the time. There are a lot of interest in this subject, but

15:22.680 --> 15:26.880
it may be that it's coming from more elite groups and so forth. I think people are aware

15:26.880 --> 15:33.240
of it. But right now, I mean, there's a lot of dissatisfaction out there. We've got Donald

15:33.240 --> 15:38.280
Trump in the Oval Office. You might ask, why is that? Well, in part, that's because a lot

15:38.280 --> 15:43.280
of people feel that they're being left behind by progress. Now, those people are much more

15:43.280 --> 15:49.280
focused right now on globalization. They're likely to blame workers in China or they're

15:49.280 --> 15:55.680
likely to blame immigrants and less so on technology. But the reality is that that situation

15:55.680 --> 16:00.360
has been created to a large extent because of technology. It's probably the most important

16:00.360 --> 16:08.400
factor in the fact that those good solid middle-class jobs have disappeared. That's going to continue.

16:08.400 --> 16:12.360
It's going to accelerate. I do think there are things coming that will make it very,

16:12.360 --> 16:18.160
very visible. Just think of self-driving cars and trucks, for example. Once that happens,

16:18.160 --> 16:21.560
it's going to be in everyone's face. They're going to really see what's going on. And then

16:21.560 --> 16:28.720
at that point, I suspect you will see more of a political backlash against automation,

16:28.720 --> 16:32.600
which is not something I think is good. I think that's a bad thing because I don't think

16:32.600 --> 16:38.040
we want to stop this progress at all. But we do need to find a way to adapt to it.

16:38.040 --> 16:44.600
Do you think we're headed, let's say in 2020, as you rightly mentioned, all the unemployment,

16:44.600 --> 16:52.880
which was a major issue in the past U.S. presidential election race, but nobody talked about automation.

16:52.880 --> 16:58.480
But do you think automation will become a main scapegoat maybe for politicians in order

16:58.480 --> 17:01.740
to be elected? I think it would definitely become a political

17:01.740 --> 17:07.840
issue. I'm not quite sure how the politicians will leverage it or spin it. But I mean, it

17:07.840 --> 17:13.160
might not be in 2020. That's only three years away. But within 10 years, say, I think it's

17:13.160 --> 17:18.040
going to definitely be a big political issue and a social issue. And it will become much

17:18.040 --> 17:22.600
more central. Right now, there's still a debate. There's not consensus on this, OK? You can

17:22.600 --> 17:28.800
find economists, certainly very smart people, who would disagree with everything that I'm

17:28.800 --> 17:34.520
saying about this. And I'll say, don't worry. Everything will work out. Maybe they're right.

17:34.520 --> 17:40.280
But I suspect not. And I think that within 10 years or so, it'll become a lot more obvious

17:40.280 --> 17:44.360
that that's the case. Yeah, we've been talking about automation

17:44.360 --> 17:51.660
a lot on this show. But it seems to me that the public is very either dismissive towards

17:51.660 --> 17:57.780
it as a mainstream subject. They're still calling it sci-fi. And maybe that's why politicians

17:57.780 --> 18:01.920
are not really talking about it now. But it's something that we have to deal with it regardless.

18:01.920 --> 18:06.240
Yeah, I mean, it definitely is going to be a huge issue. I mean, I think ultimately it

18:06.240 --> 18:12.160
could be not too different from climate change in terms of the impact all of this has. So

18:12.160 --> 18:16.240
it's going to be an enormously important issue, I think. That's my opinion. Again, we don't

18:16.240 --> 18:21.160
have the consensus on this that we do on an issue like climate change, where virtually

18:21.160 --> 18:28.520
all the scientists agree. But that's my view. It's going to be a huge thing that's going

18:28.520 --> 18:33.480
to demand a policy response. Yeah, I agree.

18:33.480 --> 18:38.680
I was watching the coverage of President Trump's speech at NATO headquarters earlier today.

18:38.680 --> 18:43.200
And they were talking about how to plan for building the new headquarters for NATO established

18:43.200 --> 18:49.600
in 1999. Politics obviously has had a lot to do with the almost 20 years gap between

18:49.600 --> 18:53.880
the plan and the execution. And there are many other examples of inefficiencies that

18:53.880 --> 18:59.560
exist in our political systems. Are we going to see automation entering politics itself?

18:59.560 --> 19:04.880
In other words, as technology is changing our world fast, yet politics seems to be slowing

19:04.880 --> 19:10.680
things down. Do you think these systems, including politics, are in need of reformation themselves?

19:10.680 --> 19:16.360
Politics, our economy, banking system, insurance system?

19:16.360 --> 19:22.960
Yeah, I mean, it's definitely a huge issue. The political system moves very slow in all

19:22.960 --> 19:29.280
countries, especially in the U.S. I mean, the example I always use here in the U.S.

19:29.280 --> 19:35.680
is healthcare. It took at least 80 years from the time we started first talking about universal

19:35.680 --> 19:40.480
healthcare to get to Obamacare. And now, of course, they're trying to destroy that. So

19:40.480 --> 19:45.840
the problem we face with this is this technology is accelerating. And all of this is going

19:45.840 --> 19:51.200
to happen really fast, potentially. We don't know how fast. I mean, I guess 10 years, something

19:51.200 --> 19:55.360
like that. But who knows? Maybe it's really five years. And the political system moves

19:55.360 --> 19:59.320
very, very slowly. So it's going to be a big problem in terms of adapting to all of this.

19:59.320 --> 20:06.920
I mean, we saw back in the financial crisis 2008 that our regulatory system was not equipped

20:06.920 --> 20:11.800
to deal with these complex, sophisticated derivatives and so forth that were being used

20:11.800 --> 20:16.480
on Wall Street. And those, of course, emerged from computer power. I mean, without computers,

20:16.480 --> 20:20.440
you wouldn't have had those. So that's just one example of how it's already had an enormous

20:20.440 --> 20:28.520
impact. It could get a lot bigger and worse. So yeah, I've heard proposals. Let's use artificial

20:28.520 --> 20:33.480
intelligence to replace politicians and so forth. I mean, you know, that's not at this

20:33.480 --> 20:42.760
point constitutional. So why isn't it constitutional? Well, actual politicians are, you know, we

20:42.760 --> 20:49.280
the people in the constitutions. So, you know, I don't think you can replace them with an

20:49.280 --> 20:59.120
artificial entity. But certainly, AI and big data and algorithmic approaches have got more,

20:59.120 --> 21:02.960
are going to play more of a role in government. I mean, this will happen in government too,

21:02.960 --> 21:06.560
you know, slower than it happens in the private sector, certainly. But it's going to have

21:06.560 --> 21:13.080
an impact there. I mean, maybe someday that we will go the route of using more AI in a

21:13.080 --> 21:17.920
political arena. But that's a huge jump. And remember that the politicians are very, very

21:17.920 --> 21:22.320
interested in protecting their jobs, even as everyone else's jobs are automated, and

21:22.320 --> 21:26.440
they're the ones that ultimately have the power to protect their own jobs. So, you know,

21:26.440 --> 21:31.800
that's not something I would expect to see probably in my lifetime. But no, well, they're

21:31.800 --> 21:36.000
businessmen, right? Business is conservative. Do you think the constitution itself needs

21:36.000 --> 21:42.720
to change? Because after all, it's a document that's been written before the discovery or

21:42.720 --> 21:49.440
invention of electricity. Yeah, I mean, it's not, I don't believe that the constitution

21:49.440 --> 21:55.440
is a divine, perfect document, as some people do. I mean, it's certainly had a good history.

21:55.440 --> 22:02.560
It was designed to be amended, right? I mean, it's just, it's turned out it's very, very

22:02.560 --> 22:09.040
difficult to do that. Particularly because of the political polarization in the

22:09.040 --> 22:14.920
United States. Partisan issues, right? Yes. But it wouldn't surprise me if we do need

22:14.920 --> 22:21.080
to modify it in the future. It is definitely, you know, many people think the United States

22:21.080 --> 22:27.120
is really run into a situation where our system of government is putting us at a disadvantage

22:27.120 --> 22:32.080
relative to countries that have a parliamentary system, because you can, it's a lot easier

22:32.080 --> 22:39.000
to get stuff done in a parliamentary system than it is in our presidential constitutional

22:39.000 --> 22:43.080
system, where there are so many checks and balances, which have always been perceived

22:43.080 --> 22:50.040
as a good thing. But right now, the checks are really becoming a bit overwhelming in

22:50.040 --> 22:54.160
some cases, I think. Do you think it also has something to do with the impact of religion

22:54.160 --> 22:59.920
in the United States? Because I'm from Iran originally, and talking to ordinary people

22:59.920 --> 23:05.760
in America, they're just as religious as ordinary people in Iran. They just have different religion.

23:05.760 --> 23:10.080
So you mentioned China, how they're advancing in certain areas that the United States is

23:10.080 --> 23:14.840
not advancing. And to me, it has something to do with religion as well. What's the deal

23:14.840 --> 23:21.400
with religion and the impact of it in our society? Well, you know, I myself would not

23:21.400 --> 23:25.520
be unhappy at all to see religion get toned down somewhat and more like it is in Europe

23:25.520 --> 23:31.240
where people are less religious, but it is the case in United States is definitely playing

23:31.240 --> 23:38.120
a major role. You know, some people do believe that all of our morality and values derive

23:38.120 --> 23:43.100
from from religion. I mean, that may be true historically, perhaps, but I don't think it's

23:43.100 --> 23:47.480
essential to be, you know, to have morality to be religious. I mean, you can, people like

23:47.480 --> 23:54.800
Sam Harris, I think have made a pretty strong case that you can derive basic morality directly

23:54.800 --> 23:59.920
from science. Right. So I just don't see that happening in the United States. I mean, I

23:59.920 --> 24:03.020
don't think that this is a country that's going to walk away from religion. So that's

24:03.020 --> 24:09.400
just a reality that we've got to live with. And unlike, you know, while I'm kind of ambivalent

24:09.400 --> 24:17.120
about it, I'm not going to be like, you know, someone like Richard Dawson, who is very aggressive

24:17.120 --> 24:22.640
at, you know, attacking religion, because I that's probably counterproductive. I mean,

24:22.640 --> 24:29.560
religion is something that people pick up from a very, very early age, right? It becomes

24:29.560 --> 24:36.560
sort of central to who you are. And it's very, very difficult to to to, you know, do anything

24:36.560 --> 24:42.480
to to eliminate that unless it sort of happens organically, the way it did in, for the most

24:42.480 --> 24:47.520
part in Europe, I guess. So but, you know, there are a lot of interesting interesting

24:47.520 --> 24:53.280
implications to all this. As you said, we're more likely to have taboos. I mean, stem stem

24:53.280 --> 24:57.920
cell research is, you know, sort of the best example of that of how the United States is

24:57.920 --> 25:04.000
perhaps held back by that in terms of some scientific endeavors. The thing I mentioned

25:04.000 --> 25:09.960
with the Chinese working on intelligence and so forth may be another thing, you know, so

25:09.960 --> 25:15.560
that may actually give other countries an advantage in pursuing some of these scientific

25:15.560 --> 25:24.200
endeavors if if the religious forces in the United States tend to hold us back. The other

25:24.200 --> 25:28.720
thing is that there's all this talk of, you know, the singularity and things like this

25:28.720 --> 25:38.120
out of Silicon Valley and that in some ways, this is technological futuristic kind of take

25:38.120 --> 25:43.000
is becoming a religion in its own right. You know, there you hear a lot of people in Silicon

25:43.000 --> 25:48.440
Valley who believe that they are literally going to live forever. They're going to they're

25:48.440 --> 25:55.640
going to get immortality not in the way that that, you know, religious people wanted in

25:55.640 --> 26:00.840
the afterlife, but they're going to get it right here and now in this life. So what happens

26:00.840 --> 26:06.320
when that kind of thing really, you know, intersects with with religion? I mean, there

26:06.320 --> 26:09.040
are a lot of unpredictable things there, I think.

26:09.040 --> 26:14.800
That's interesting. You mentioned I had some transhumanist Christians on the show and it

26:14.800 --> 26:20.840
seems to me that they're using technology and transhumanism. However, they're defining

26:20.840 --> 26:27.520
it to justify the religion. So, for example, I had a gentleman named Christopher Benwick

26:27.520 --> 26:33.880
who started Christian Transhumanist Association and he believed that our rise of artificial

26:33.880 --> 26:41.360
intelligence and mortality through technology has no opposition to teachings of Bible and

26:41.360 --> 26:45.240
Jesus. It just has to be interpreted correctly.

26:45.240 --> 26:53.120
Yeah, I mean, I think people that have very strong religious beliefs, they can adapt to

26:53.120 --> 26:58.160
almost anything, you know, I mean, if aliens from outer space landed, I mean, they figure

26:58.160 --> 27:02.640
out a way to adapt to that. Right. So just like many religious people have found a way

27:02.640 --> 27:09.720
to find a common ground with evolution, right? Not all religious people reject the idea of

27:09.720 --> 27:16.640
religion or evolution and cling to the idea that, you know, the creation story is true

27:16.640 --> 27:20.640
and so forth. So, you know, I mean, there's some flexibility there.

27:20.640 --> 27:24.040
Do you think that we're at the point that we have to address certain things that we

27:24.040 --> 27:28.840
have taken for granted, like ethics and morality, because we are translating all of that into

27:28.840 --> 27:34.360
machines, into artificial intelligence. So one of the questions that I usually ask my

27:34.360 --> 27:41.880
guess is that, do you think ethics and morality are subjective or objective?

27:41.880 --> 27:48.040
I think that certain things are fairly objective. I mean, you know, sort of the golden rule,

27:48.040 --> 27:51.240
don't hurt anyone else, treat people the way that they...

27:51.240 --> 27:56.040
But we're still doing that though on a daily basis. It's not like laws of physics. So how

27:56.040 --> 27:59.920
are we going to tell an artificial intelligence, for example, that killing is bad?

27:59.920 --> 28:07.000
Well, I think that, you know, that's not quite my area, but I mean, I assume they will program

28:07.000 --> 28:11.760
that in. They will, you know, they will, or it will arise from the data, from the learn,

28:11.760 --> 28:20.160
you know, that the AI is trained on, right? I mean, I think that there's an enormous amount

28:20.160 --> 28:23.600
of attention to that right now. Actually, people like Elon Musk, for example, have invested

28:23.600 --> 28:29.720
a lot in building think tanks that are thinking about this issue of how to build friendly

28:29.720 --> 28:34.000
AI and so forth. So I think a lot of work is being done there, whether it will be successful.

28:34.000 --> 28:39.360
I mean, that's speaking in terms of a general artificial intelligence. Now, they're going

28:39.360 --> 28:43.720
to be specialized artificial intelligence that are specifically designed to kill people,

28:43.720 --> 28:47.920
certainly in military applications and so forth. So, I mean, you're right, that's a

28:47.920 --> 28:57.680
bit scary. But as long as we view technology as a tool and something that's inanimate,

28:57.680 --> 29:05.720
I think that, you know, we're not going to worry too much about what it does. The morality

29:05.720 --> 29:11.360
aspect is going to come from the people that control that technology, not the technology

29:11.360 --> 29:12.360
itself.

29:12.360 --> 29:17.080
Exactly. I was reading an article about how Google AI start to learn that aggression pays

29:17.080 --> 29:22.400
back in certain situations and becoming aggressive in certain situations because they just learn

29:22.400 --> 29:27.840
from data. So that's basically what I mean, that aren't we entering an era that we have

29:27.840 --> 29:33.400
to start discussing some fundamental core that define all of us, which, by the way,

29:33.400 --> 29:36.280
is different culture to culture, a lot of them.

29:36.280 --> 29:44.760
Yeah, I mean, I think that the discussion of what to do with AI is certainly happening,

29:44.760 --> 29:48.840
you know, discussing morals. The problem is having a discussion doesn't necessarily lead

29:48.840 --> 29:49.840
to any answers.

29:49.840 --> 29:50.840
Of course not, yes.

29:50.840 --> 29:53.960
You can argue that we are having the discussion. I mean, certainly there are people discussing

29:53.960 --> 29:58.040
that now. I mean, I mentioned Sam Harris. He's someone that is very into this. You can

29:58.040 --> 30:04.720
listen to his podcast, right? I mean, so there's a discussion, but I mean, is it going to really

30:04.720 --> 30:13.520
achieve consensus? I mean, let me look around the world today, especially with the radical,

30:13.520 --> 30:18.920
you know, the terrorists and everything and how that's driven by religion. I mean, good

30:18.920 --> 30:24.680
luck in getting everyone together and agreeing on, you know, some basic rules. So it seems

30:24.680 --> 30:32.160
almost like that's so remote and we certainly can't wait for that before we start worrying

30:32.160 --> 30:36.520
about what to do with artificial intelligence because, I mean, I think we'd basically be

30:36.520 --> 30:39.600
waiting forever.

30:39.600 --> 30:45.160
What would you suggest to people in order to preparing themselves for what's coming,

30:45.160 --> 30:48.680
artificial intelligence and automation and everything that comes with it?

30:48.680 --> 30:53.000
I would say specifically in terms of your job and you want to remain relevant, you know,

30:53.000 --> 30:58.560
you want to avoid doing anything that's routine, repetitive, predictable, right? Because you

30:58.560 --> 31:02.740
know that those jobs are going to be susceptible. So instead try to be doing something creative,

31:02.740 --> 31:07.960
something that really involves working deeply with other people, you know, building those

31:07.960 --> 31:12.720
kind of interpersonal relationships. Those are the areas that are going to be least susceptible

31:12.720 --> 31:16.960
to automation. So that's what I would advise people to do that in their own careers. And

31:16.960 --> 31:20.980
if you have kids in school studying, you want to make them make sure that they're studying

31:20.980 --> 31:24.440
things that can lead to that kind of career as opposed to sitting in front of a computer

31:24.440 --> 31:28.280
doing the same thing again and again, because that's, you know, something that's going to

31:28.280 --> 31:33.240
be automated. And then generally, I think we need to have more awareness of this so

31:33.240 --> 31:38.400
that people can begin to think about what we need to do in terms of policies, you know,

31:38.400 --> 31:42.440
public policies to address all of these issues, whether it's going to be a basic income to

31:42.440 --> 31:50.720
deal with the impact of automation. And you mentioned, you know, the moral and ethical

31:50.720 --> 31:55.400
issues that there are many of those that are going to intersect with this technology, certainly

31:55.400 --> 32:02.280
in military arenas and so forth and security arenas. So people need to be more aware of

32:02.280 --> 32:07.720
it and more ready to engage in this discussion or they're not going to have a voice in how

32:07.720 --> 32:09.160
all this unfolds.

32:09.160 --> 32:16.060
Yeah. So you recommend them to focus on what makes them unique, basically, because I heard

32:16.060 --> 32:20.960
from Ray Kurzweil that he was suggesting if you have kids in school, suggest to them to

32:20.960 --> 32:24.920
do something that has something to do with computer. But as you're correctly mentioning,

32:24.920 --> 32:28.320
not everything doing with computer is going to guarantee that you will have a job or you

32:28.320 --> 32:29.320
have a career.

32:29.320 --> 32:33.240
Not at all. I mean, everyone thinks if you learn to program a computer that you're going

32:33.240 --> 32:37.960
to have a safe job. And that's not true. I mean, basic computer programming, routine

32:37.960 --> 32:41.240
computer programming is also something that's going to be subject to automation. There's

32:41.240 --> 32:45.240
already a lot of people working on that already have been some important developments there.

32:45.240 --> 32:50.560
So if you're going to work with computers, you're probably going to have to, you know,

32:50.560 --> 32:55.640
it's going to require some fairly high level skills, you know, in terms of, you know, design

32:55.640 --> 33:00.480
and creativity and that kind of thing. It's just not a question of learning how to program

33:00.480 --> 33:03.360
that's going to keep you ahead of the game.

33:03.360 --> 33:08.280
Right. Another argument is that in the beginning of industrial revolution, 95% of jobs apparently

33:08.280 --> 33:14.320
completely wiped out. But they couldn't imagine that we have a job like web developer or robotic

33:14.320 --> 33:21.760
engineer. So new jobs will be created as a consequence of the situation. What do you

33:21.760 --> 33:22.760
think?

33:22.760 --> 33:26.960
Yeah, sure they will. Yeah. The question is how many new jobs and are those jobs going

33:26.960 --> 33:34.840
to be a good match for the people that lose jobs? Okay. So yeah, you talked about website

33:34.840 --> 33:40.360
designer and robotics engineer, but not everyone can do those jobs. All right. Now people are

33:40.360 --> 33:44.440
right now working at McDonald's or driving a taxi that, you know, you're not going to

33:44.440 --> 33:51.000
make them all into those kinds of professions. And nor is it the case that there's necessarily

33:51.000 --> 33:55.280
going to be enough of those jobs. Okay. So those new areas do exist, but they're actually

33:55.280 --> 33:59.800
not a very big fraction of employment. I mean, most people, but the vast majority of people

33:59.800 --> 34:05.000
still work in traditional areas, you know, things like driving vehicles, preparing and

34:05.000 --> 34:14.040
serving food. I mean, the most common occupations are retail salesperson, cashier, truck driver.

34:14.040 --> 34:20.040
You know, these are all the most common occupations that we have. A lot of people doing that kind

34:20.040 --> 34:24.040
of work and a lot of that is going to be definitely subject to automation.

34:24.040 --> 34:29.840
Yeah. And I think it would be fair to assume that a lot of those people are just not going

34:29.840 --> 34:35.600
to make it in the new world, right? It's going to get increasingly hard. Right.

34:35.600 --> 34:39.000
And that's again, why I think we need that basic income because otherwise we're going

34:39.000 --> 34:46.840
to have all kinds of social upheaval, political disruption. You know, it'll be pretty ugly,

34:46.840 --> 34:53.120
I think, if things play out the way I suspect they are. So, you know, again, we really need

34:53.120 --> 34:57.040
to think about what policies we're going to have to have in place in order to adapt to

34:57.040 --> 34:59.040
all of this.

34:59.040 --> 35:04.840
The way we're evolving is changing really fast because we're more and more dependent

35:04.840 --> 35:12.840
on technology. For example, I was talking on a space channel here in Toronto about how

35:12.840 --> 35:18.680
Ghost in a Shell, when it came out, it was almost completely for everybody except maybe

35:18.680 --> 35:24.920
some academics, science fiction. But the new one that was made came out a few days after

35:24.920 --> 35:31.840
Elon Musk started his new company that connects human brain to the machine. Where do you think

35:31.840 --> 35:33.080
we're going with evolution?

35:33.080 --> 35:39.540
I mean, all this is just kind of so unpredictable that, I mean, it's to some extent pointless

35:39.540 --> 35:43.680
to really speculate on specific outcomes. What we can say is that there's going to be

35:43.680 --> 35:49.640
a big disruption. I think we'll be on this planet for quite a while. I mean, I know Elon

35:49.640 --> 35:54.480
is planning to go to Mars, but my guess is that from a practical standpoint, if we're

35:54.480 --> 35:59.160
going to explore space, the way to do it is probably robotic spaceships for the foreseeable

35:59.160 --> 36:05.600
future rather than really sending people. To me, that sounds a lot more effective as

36:05.600 --> 36:09.820
a way to do it. But this merging with the machines is definitely going to happen to

36:09.820 --> 36:14.560
some extent. The question is how radical will that be? And I don't pretend to know the answer

36:14.560 --> 36:24.640
to that. Other people feel very strongly about it. Ray Kurzweil. Some other people have made

36:24.640 --> 36:29.000
similar predictions. And then of course, there's also on the biological side. If we can enhance

36:29.000 --> 36:34.560
ourselves genetically and so forth, maybe that's how the evolution will occur.

36:34.560 --> 36:40.000
So I don't pretend to know which is going to be, but I do think that we should prepare

36:40.000 --> 36:44.360
ourselves for dramatic change. It's difficult to speculate, but there is

36:44.360 --> 36:49.480
a trend that we can see how fast things are changing now compared to maybe 10, 20 years

36:49.480 --> 36:52.320
ago. Absolutely. I mean, there's no doubt that

36:52.320 --> 36:55.360
the next 10 years is not going to be like the last 10 years. That's for sure. Things

36:55.360 --> 37:00.920
are moving at a rapid rate. But I think that to some extent, that's all you can say is

37:00.920 --> 37:05.160
that things are going to move a lot faster. They're going to change. You can point to

37:05.160 --> 37:10.220
some fairly obvious things, like I think the impact on employment is fairly straightforward.

37:10.220 --> 37:17.800
In terms of the evolution of human beings, it's quite speculative. I think they really

37:17.800 --> 37:21.400
take a strong stand on how that's going to play out.

37:21.400 --> 37:29.080
Right. Yeah. I always had a little difficulty buying completely into bio-enhancements and

37:29.080 --> 37:33.360
maybe bio-immortality because we still have to deal with this planet and everybody who's

37:33.360 --> 37:41.200
living in it. We're dealing with climate change. We're dealing with wars. A lot of things are

37:41.200 --> 37:47.680
specific to humans, this territorial nature of humanity and tribal nature of humanity.

37:47.680 --> 37:52.000
It just seems to me, well, we can live longer if we enhance our self-biologically, but is

37:52.000 --> 37:58.080
it really going to be any better if we live 1,000 years and war and you know?

37:58.080 --> 38:05.560
Yeah. There are a lot of issues there. There are people like Aubrey DeGray who are very

38:05.560 --> 38:12.120
heavily invested in this idea of immortality. I'm not myself. I don't anticipate living

38:12.120 --> 38:13.480
forever.

38:13.480 --> 38:16.140
You don't like it or you don't think it's going to happen?

38:16.140 --> 38:21.080
I don't think it's going to happen. I'll certainly, like anyone else, do my best to stick around

38:21.080 --> 38:26.440
as long as possible, at least as long as I'm healthy. I certainly would not want to have

38:26.440 --> 38:30.240
longevity without good health.

38:30.240 --> 38:35.120
Some people say death gives meaning to life. I don't buy into that.

38:35.120 --> 38:42.000
I wouldn't say that, but I do think it's probably inevitable for the foreseeable future. I'm

38:42.000 --> 38:46.360
not very optimistic that they're going to solve the mortality problem.

38:46.360 --> 38:51.440
Of course, as you say, there are huge issues with that in terms of population and stuff

38:51.440 --> 38:57.080
like that. DeGray says people will decide they're not going to have kids anymore. That

38:57.080 --> 39:05.600
seems a little unlikely to me based on my experience. There definitely are enormous

39:05.600 --> 39:10.440
challenges associated with that if it happens. There's also the issue of inequality, the

39:10.440 --> 39:16.320
fact that these technologies, at least initially, would be available only to very rich people

39:16.320 --> 39:24.080
probably. That's going to create all kinds of conflict worse than we have now. In many

39:24.080 --> 39:30.040
ways, it could be quite ugly if those technologies are really realized, I think.

39:30.040 --> 39:35.280
Speculations aside, would you agree that the best way for people to being able to adapt

39:35.280 --> 39:38.080
to what's coming is just keep educating themselves?

39:38.080 --> 39:42.640
Sure. Everything is going to move at a more rapid rate. Certainly, the days where you

39:42.640 --> 39:46.240
go to school, stop going to school, get a job, stay in that job for your whole career,

39:46.240 --> 39:52.480
that's over already, I think. Most people understand that. People are going to have

39:52.480 --> 39:58.280
to continuously retool. I think that's great. We should make sure that there are educational

39:58.280 --> 40:02.440
resources out there for people to do that. We also have to understand that probably not

40:02.440 --> 40:05.280
everyone is going to succeed in that kind of world.

40:05.280 --> 40:10.800
That's why we're going to need to have that basic income or some other kind of safety

40:10.800 --> 40:11.800
net.

40:11.800 --> 40:16.360
Martin, let me ask you the last question I ask all my guests, that if you come across

40:16.360 --> 40:21.600
an intelligent alien from a different civilization, what would you say is the worst thing humanity

40:21.600 --> 40:27.960
has done and what would you say is humanity's greatest achievement?

40:27.960 --> 40:33.320
Probably the worst thing is to destroy our environment. We're really putting the earth

40:33.320 --> 40:37.520
at risk. Plenty of other things you could list, things that we've done to ourselves

40:37.520 --> 40:45.320
and to other people, the genocides and all of that, but the single thing that really

40:45.320 --> 40:51.680
stands out is that we've generally just not managed our planet well.

40:51.680 --> 40:58.240
Our greatest achievement is our technology. The thing that we're talking about here, that

40:58.240 --> 41:05.280
we're maybe on the verge of building another intelligence that might someday be superior

41:05.280 --> 41:12.720
to us. That's a fairly unique achievement. Certainly no other entity on this planet could

41:12.720 --> 41:18.520
even come close to that. That's pretty remarkable. I suspect that if aliens did arrive, then

41:18.520 --> 41:25.040
they've gone through that same process. It may well be that whatever arrives is a robotic

41:25.040 --> 41:50.040
spaceship rather than an actual alien.

41:50.040 --> 41:57.040
.

