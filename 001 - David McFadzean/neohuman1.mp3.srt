1
00:00:00,000 --> 00:00:04,400
I would offer, actually, the World Wide Web is our greatest achievement.

2
00:00:06,080 --> 00:00:12,400
The technology that makes self-expression possible and collaboration, I think it's

3
00:00:13,440 --> 00:00:20,480
instigated a renaissance across the world. It's amazing. And in a sense, it's only getting started.

4
00:00:20,480 --> 00:00:29,600
So, ask me 10 years from now and I'll say the successor for the web, whatever that is.

5
00:00:35,200 --> 00:00:41,200
Hello, this is Agha Bahari and I am here with the first episode of Neo-Human.

6
00:00:42,240 --> 00:00:47,120
I will be talking to interesting people about interesting subjects. We don't have any specific

7
00:00:47,120 --> 00:00:53,040
subject or title or style for the podcast, whatever happens, it's just going to be interesting.

8
00:00:53,040 --> 00:00:58,400
That's the most important thing. For the first episode, right here with me,

9
00:00:58,400 --> 00:01:08,400
we have David McFadzen, who is a lead engineer at a Synaptic, which is a Toronto-based technology

10
00:01:08,400 --> 00:01:14,240
company. Synaptive. Synaptive. Horrible. I asked you the name before the show too.

11
00:01:14,240 --> 00:01:19,440
So, what do you do at Synaptive and what does Synaptive do?

12
00:01:21,200 --> 00:01:28,880
Yeah, Synaptive is a startup that's about almost four years old now and we build technology to

13
00:01:28,880 --> 00:01:34,480
mainly assist neurosurgeons in difficult brain surgeries. So, we have a line of products

14
00:01:35,280 --> 00:01:41,040
to look at the scans from the brain and render them in 3D and help the surgeon

15
00:01:41,040 --> 00:01:50,880
plan the surgery in advance. And then in the OR, our software acts sort of like a GPS for surgeons

16
00:01:50,880 --> 00:01:57,040
because it's like a Google Maps to show them where they are and help her along in their route.

17
00:01:58,000 --> 00:02:02,560
And if they get off course, they can get back on with the 3D map of the brain.

18
00:02:02,560 --> 00:02:09,440
And this is usable with robotics and with human doctors. Both can use it. Because I remember

19
00:02:09,440 --> 00:02:14,640
you were saying that you have a robotic arm at the work. That's right. I'm the lead software

20
00:02:14,640 --> 00:02:21,840
engineer on our robotic arm project. It's a device positioning system that's designed to assist the

21
00:02:21,840 --> 00:02:27,360
surgeon. So, it's holding the optics for the surgery, which is the lighting in the camera

22
00:02:28,240 --> 00:02:36,000
and a microscope. And it's recording video. And the robotic arm on MacLea positions the

23
00:02:36,000 --> 00:02:42,000
optics of the camera into ideal positions for the surgeon. We just have to make a gesture that they

24
00:02:42,000 --> 00:02:48,240
want it in some particular place and press a foot pedal. And the robotic arm will move the

25
00:02:48,240 --> 00:02:52,560
optics into the correct position. That's amazing. How long did it take

26
00:02:54,800 --> 00:03:03,040
from the concept idea of such a thing to being at this level that you guys are, which

27
00:03:03,040 --> 00:03:07,280
I believe you're ready to be launched? It launched last February actually.

28
00:03:07,280 --> 00:03:12,960
It launched last February? Yeah. From concept to when it was FDA approved and actually used in

29
00:03:12,960 --> 00:03:18,400
our real surgery was less than two years. Oh, wow. That's amazing. And you still consider the whole

30
00:03:18,400 --> 00:03:23,120
company as a startup. Like you're not a corporation per se. We're in transition now.

31
00:03:23,920 --> 00:03:28,800
Since we got FDA approval last February and we're deployed now, we're out in the field at,

32
00:03:28,800 --> 00:03:33,760
I think, eight or nine different installations now, different hospitals around the United States.

33
00:03:35,040 --> 00:03:41,920
And it's to date in the last eight months or so since it's heard. It's been used in 500 real

34
00:03:41,920 --> 00:03:51,600
surgeries. That's amazing. What do you think is, I'm an enthusiast entrepreneur, I have to say.

35
00:03:52,240 --> 00:03:58,400
Like, I'm very interested in creative ideas and then implementing those ideas and with

36
00:03:58,400 --> 00:04:03,840
computers and coding, it's never really been easier, I think, to make an idea from ground up

37
00:04:03,840 --> 00:04:14,480
with a very limited number of people. That's very true. Yeah. What do you think the effect of such

38
00:04:14,480 --> 00:04:19,600
technologies in our, because the company that you're explaining is a very specific company

39
00:04:19,600 --> 00:04:26,080
that doing very specific thing? How do you think those technologies will translate into a popular

40
00:04:26,080 --> 00:04:35,280
or mainstream consumer to be able to take advantage of that? For example, let's say this is just off

41
00:04:35,280 --> 00:04:40,800
the top of my head. Would there be a possibility of mixing this technology or would you see the

42
00:04:40,800 --> 00:04:49,680
possibility of mixing this technology with a very straightforward variable like the EEG

43
00:04:49,680 --> 00:04:57,360
variables that exist right now that assist you to meditate? I think they're also a company from

44
00:04:57,920 --> 00:05:06,240
Toronto, Emuse or Muse, I think. Right. I think we actually used Muse at a hackathon that we

45
00:05:06,240 --> 00:05:12,320
did internally at Synaptive a couple of months ago. And using Lemuse will detect brainwaves of

46
00:05:12,320 --> 00:05:17,840
some sort and encode them in a way that software can take advantage of. So one of the demos was

47
00:05:17,840 --> 00:05:25,040
to move the robotic arm just by thinking about it. Wow. It was amazing. I bet, yeah. Yeah,

48
00:05:25,040 --> 00:05:30,160
so we're definitely looking at new consumer electronics to bring that into the OR to help

49
00:05:30,160 --> 00:05:37,120
the surgeon. Stuff like the Oculus Rift or the Microsoft HoloLens to give the surgeon a heads

50
00:05:37,120 --> 00:05:42,800
up display and show them the 3D rendering of the brain as if he was living in that space.

51
00:05:42,800 --> 00:05:49,680
That's amazing. You would imagine this also beside the surgery itself for training. This is a

52
00:05:49,680 --> 00:05:57,680
very great achievement. The way that you don't necessarily have to experiment on even like

53
00:05:57,680 --> 00:06:03,840
animals for a lot of stuff that are going on inside. When we want to test their brain pulses

54
00:06:03,840 --> 00:06:09,680
or anything instead of doing it on the animal, it will be able to done in a virtual space in

55
00:06:09,680 --> 00:06:14,720
the virtual brain. Yeah, definitely. You could see this in the Microsoft proportional material

56
00:06:14,720 --> 00:06:19,440
for the HoloLens. Some of the early demos are in the classroom where a teacher is showing

57
00:06:19,440 --> 00:06:25,680
the kids a solar system or whatever and the kids have HoloLenses and the solar system is

58
00:06:25,680 --> 00:06:31,040
right there in the classroom with them and animated and interactive. Or games. Yeah. Yeah,

59
00:06:31,040 --> 00:06:38,400
it's just unbelievable. Another use that this kind of technology has had in the past couple

60
00:06:38,400 --> 00:06:43,680
years has been a military and they've done it for good and for evil and per se.

61
00:06:45,840 --> 00:06:53,280
Even though it might be necessary evil. But the technology has been used to treat PTSD very

62
00:06:53,280 --> 00:07:02,080
seriously and they've had very successful results from it. Where do you see the future

63
00:07:02,080 --> 00:07:09,920
of that technology with that assistance are going? And how does technology as a whole will play

64
00:07:11,120 --> 00:07:16,240
a bigger and bigger role in the army and in military industry complex?

65
00:07:18,160 --> 00:07:23,520
We see with the rise of drones that has been, I dare say, almost a decade now.

66
00:07:26,320 --> 00:07:31,280
Where do you see this whole thing is going? Because we are not going to stop fighting.

67
00:07:31,280 --> 00:07:38,880
The war is going to continue. And I think the difference would be that we will have less

68
00:07:38,880 --> 00:07:44,320
casualty because technology is covering most of the ground that people had to do before.

69
00:07:44,960 --> 00:07:50,160
Where do you see this whole thing going with the usage of technology and its dependence to

70
00:07:50,160 --> 00:07:57,520
advance technology? Well, in military particularly, I think the trend is to push more and more

71
00:07:57,520 --> 00:08:04,000
computational power down to the ranks and replacing humans with automation. Obviously,

72
00:08:04,000 --> 00:08:10,560
it's much safer for their side. But they're going to run into a pretty significant ethical

73
00:08:10,560 --> 00:08:17,600
decision probably really soon about giving autonomous drones the power to kill or the right to kill.

74
00:08:19,760 --> 00:08:24,000
Would you think that they will announce that they're about to do that or they'll just do it

75
00:08:24,000 --> 00:08:30,960
until they get caught? Judging from recent events, I think probably the latter.

76
00:08:32,160 --> 00:08:36,080
It will find out about it on WikiLeaks. Yeah, I think so.

77
00:08:38,720 --> 00:08:44,160
It's been amazing. I think it's a great achievement of technology that you can

78
00:08:44,160 --> 00:08:56,400
go in a small room in Arizona and control a drone in Afghanistan and drop a bomb because

79
00:08:58,800 --> 00:09:03,520
of course, it's not resulting always in good. Sometimes it does and there are some people

80
00:09:03,520 --> 00:09:08,960
that need to be killed and that's the only way. And I much rather a drone will kill them than a

81
00:09:08,960 --> 00:09:18,000
soldier on our side basically being put in risk to deal with a Taliban leader or whoever.

82
00:09:18,000 --> 00:09:22,560
But at the same time, you see how people are treating, for example, even driverless cars.

83
00:09:22,560 --> 00:09:27,520
There are already a lot of people freaked out about it. People are freaked out about drones

84
00:09:27,520 --> 00:09:33,280
use of mainstream drones. You see these conspiracy theorists that have

85
00:09:33,280 --> 00:09:42,160
YouTube channels and iTunes podcasts and all of that. They're talking about, shoot the drones

86
00:09:42,160 --> 00:09:48,400
down with your shotguns and all the people who are very concerned about these kind of thing

87
00:09:48,400 --> 00:09:58,720
conspiracies. How do you think the challenge that technologists and technology enthusiasts have

88
00:09:58,720 --> 00:10:04,560
who are between two sides, one being the politics and politicians and the other being

89
00:10:05,760 --> 00:10:11,120
people who are basically always scared of a new thing. That new thing has to be proven

90
00:10:11,120 --> 00:10:16,480
many, many times and more and more people using it for masses to accept the usage of that. How

91
00:10:16,480 --> 00:10:23,840
do you think it will result in? Will it slow the advancement down or slow it down or speed it up?

92
00:10:23,840 --> 00:10:30,880
I think the way it will actually play out is we'll see some significant pushback initially

93
00:10:32,240 --> 00:10:39,840
because it is a big change, but it's also fairly inevitable that this is going to happen.

94
00:10:39,840 --> 00:10:44,800
It reminds me of David Brin's Transparent Society when surveillance becomes pervasive

95
00:10:45,360 --> 00:10:51,200
and that could have actually net positive effects on society, as we've already seen with

96
00:10:51,200 --> 00:10:58,560
the way the police in the US have reacted to their cameras. All of a sudden,

97
00:10:59,760 --> 00:11:04,960
police are getting caught all the time and I think it's changing the way they behave. They're

98
00:11:04,960 --> 00:11:11,120
becoming more ethical in their professional behavior now. Yeah, it's like a double-edged sword

99
00:11:11,120 --> 00:11:18,080
because it's become more transparent for the authorities to control that, but at the same time

100
00:11:18,080 --> 00:11:24,960
a general population also are seeing it more, so they do believe that it's happening more.

101
00:11:26,800 --> 00:11:31,040
But once it reaches some critical threshold, I think it will just become accepted,

102
00:11:31,040 --> 00:11:35,520
like cell phones have become accepted. There was pushback initially about those two,

103
00:11:36,720 --> 00:11:40,160
that people were just walking down the street talking to themselves that looked weird,

104
00:11:41,520 --> 00:11:46,160
but now it's so calm, nobody looks twice. It's just accepted.

105
00:11:46,160 --> 00:11:51,760
Also, virtual reality, I think, will have a lot to do with that because people would have

106
00:11:52,640 --> 00:12:00,000
the possibility to be whoever they want physically. On the game, Second Life,

107
00:12:00,000 --> 00:12:07,600
that you can create your own avatar and live in that. People get married, people buy stuff.

108
00:12:08,400 --> 00:12:13,040
Sweden, I heard, has an embassy in that virtual world. Nike has a store.

109
00:12:13,040 --> 00:12:21,680
I think it will begin with, this is weird that people are doing because people will do that

110
00:12:21,680 --> 00:12:26,960
and film it and put it on YouTube to create a community and find like-minded people. It's

111
00:12:26,960 --> 00:12:33,680
amazing that we can do that all around the world now. At the same time, you freak certain people

112
00:12:33,680 --> 00:12:45,600
out, the people who watch Fox News and all those propaganda, basically. Those people are not that

113
00:12:45,600 --> 00:12:52,640
much of a concern, I agree, but we hear also from someone like Elon Musk and Bill Gates and Steve

114
00:12:52,640 --> 00:13:01,600
Wozniak or... I don't want to say scaring people, they're smarter than that. They're making a very

115
00:13:01,600 --> 00:13:10,640
valid case of dangers of artificial intelligence. But at the same time, I think these kind of

116
00:13:12,960 --> 00:13:19,920
warnings, even though necessary, it will lower the budget that research facilities can have to

117
00:13:19,920 --> 00:13:26,400
actually work on it because if public opinion is not behind an idea, it would be very hard to

118
00:13:26,400 --> 00:13:31,760
finance it. It's like, I have a very good idea for a startup, but I'm probably not going to have

119
00:13:31,760 --> 00:13:38,160
more than 10 users. Nobody's going to fund me. Right. Yeah, we saw the same thing happen with

120
00:13:38,160 --> 00:13:43,120
prediction markets a few years ago. Robin Hansen was ready to roll out a prediction market that

121
00:13:43,120 --> 00:13:47,680
would help with public policy. And one of the questions that they were interested in betting

122
00:13:47,680 --> 00:13:55,520
on was the chance of a terrorist event. But some senators got ahold of this information and ran

123
00:13:55,520 --> 00:14:00,400
a scare campaign saying it would be terrible to bet on these sorts of things because one side has

124
00:14:00,400 --> 00:14:07,600
to bet on it happening, which could be seen as promoting it in some sense. There was a silly

125
00:14:07,600 --> 00:14:13,840
reason anyways, but the point is that it turned public opinion and the project was abandoned.

126
00:14:14,400 --> 00:14:18,720
Do you blame that senator or you blame the people who voted that senator into Senate?

127
00:14:18,720 --> 00:14:26,960
I think they're all to blame. Yeah, because senators, one senator has to be thousands of

128
00:14:26,960 --> 00:14:33,680
people at least behind him. And I think that's one of the greatest revelations in the United

129
00:14:33,680 --> 00:14:39,040
States. Donald Trump running for presidency because you see everybody who's behind him.

130
00:14:39,040 --> 00:14:45,360
It's a very interesting phenomenon. Yeah. Yeah. Only only one person predicted that.

131
00:14:45,360 --> 00:14:51,280
Who was that? God Adams, the creator of Dilbert. Really? Seriously. He said Donald Trump will run

132
00:14:51,280 --> 00:14:58,960
for president? He said that Donald Trump will continue to lead in the Republican polls long

133
00:14:58,960 --> 00:15:03,280
before anyone else even considered that possibility. Everyone else thought he was a

134
00:15:03,280 --> 00:15:08,400
joke and that he was popular initially because it was funny. But that's actually not the case.

135
00:15:08,400 --> 00:15:14,880
No. What's very interesting is that they were reading the polls,

136
00:15:17,920 --> 00:15:24,720
not polls really, but how much time media is covering Donald Trump in comparison to

137
00:15:24,720 --> 00:15:29,840
the other people. It's twice as much as Hillary Clinton, the time that Trump is getting. And

138
00:15:29,840 --> 00:15:36,320
I think 13 times more than Bernie Sanders. What Trump understands is how to manipulate

139
00:15:36,320 --> 00:15:41,840
media to his advantage. Absolutely. And he knows that whoever has name recognition is going to be

140
00:15:41,840 --> 00:15:48,160
highest in the polls. It's such a great transition though because with reality shows become more

141
00:15:48,160 --> 00:15:53,040
popular in the United States and the celebrity culture. I had this conversation with somebody

142
00:15:53,040 --> 00:16:00,400
else that I was like celebrities became the idols. The greatest thing that you can be in the 21st

143
00:16:00,400 --> 00:16:07,040
century. And she was saying that no, people were celebrities before that. But I was like, no, not

144
00:16:07,040 --> 00:16:13,680
like this that you use, for example, Bono from YouTube for any humanitarian campaign that you

145
00:16:13,680 --> 00:16:19,600
have to get attention from people. It's not the cause that is getting the attention of the people.

146
00:16:19,600 --> 00:16:27,280
It's a celebrity. And then by the rise of that, and now you see a person who's a creator of a

147
00:16:27,280 --> 00:16:36,640
reality show. And it's an apprentice, misuniverse, all those things. Nobody knows better than him

148
00:16:36,640 --> 00:16:44,320
how to manipulate the media, as he said. And I mean, it's appealing to most people, I think,

149
00:16:44,320 --> 00:16:50,000
because he's not getting any money from big corporations and CPACs and nobody.

150
00:16:50,000 --> 00:16:57,760
Right. He's posing as the outsider. He is above corruption. And the thing that I think people

151
00:16:57,760 --> 00:17:05,440
most find appealing is he speaks his mind. He doesn't care what people think about what he says.

152
00:17:05,440 --> 00:17:11,440
I mean, he goes to extremes to make that point. Do you think he believe a lot of?

153
00:17:11,440 --> 00:17:14,720
No, I think he does that in order to get media attention.

154
00:17:14,720 --> 00:17:21,120
I think so, too. I think so, too. I'm at this point, as we had this conversation

155
00:17:22,320 --> 00:17:30,720
before, I'm a Bernie Sanders supporter. And I think it would be very good for America to have

156
00:17:30,720 --> 00:17:36,960
a change in that direction. It's a very extreme change. We were a capitalist, but now we are

157
00:17:36,960 --> 00:17:45,360
being run by a Jew, liberal, a socialist. It would be a very good change, in my opinion. But

158
00:17:45,360 --> 00:17:51,680
if he drops out, I'm supporting Donald Trump, because I think he does not believe a lot of

159
00:17:51,680 --> 00:18:01,200
stuff that he says. You cannot possibly be that blind and make it that far. I think in a city

160
00:18:01,200 --> 00:18:09,680
like New York City. And Hillary Clinton, I have no interest in her. No, I think Hillary would just

161
00:18:09,680 --> 00:18:15,600
be a continuation of Obama, which turned out to be just a continuation of W. Bush. Surprisingly.

162
00:18:15,600 --> 00:18:21,440
I agree. Against all odds. Everyone's wishes. Is it really a surprise? Because one man in a

163
00:18:21,440 --> 00:18:27,760
democracy doesn't supposed to be able to change anything. You have to go. So it's a democratic

164
00:18:27,760 --> 00:18:36,480
system that is backstabbing itself, in my opinion, because the parties' rhetorics against each other.

165
00:18:37,600 --> 00:18:45,440
And one person who's being elected as somebody who you can use as a punching box,

166
00:18:46,720 --> 00:18:51,680
you know, punching bag. I don't think he can do much.

167
00:18:51,680 --> 00:18:57,680
Well, perhaps not. But you'd expect him to push in the directions of his promises when he's actually

168
00:18:57,680 --> 00:19:02,080
gone in the other direction. I think the good things that he did, I'm not really sure about

169
00:19:02,080 --> 00:19:06,720
healthcare system, because apparently it's one of those things you have to live in the U.S. to

170
00:19:06,720 --> 00:19:12,640
experience. I heard good things and bad things about Obamacare from different people who I respect

171
00:19:12,640 --> 00:19:22,320
their opinions. I think when he made it legal for the same-sex marriages to become federal,

172
00:19:22,320 --> 00:19:30,160
I think that was a good thing. And I still think he will do more in this last year. I'm not an

173
00:19:30,160 --> 00:19:38,320
Obama fan at all. In the first term, when he was running, I was totally supporting John McCain.

174
00:19:38,320 --> 00:19:46,880
And then John McCain elected Sarah Palin as vice president. Why would you do that? It's such a

175
00:19:46,880 --> 00:19:53,200
weird thing. That was bizarre. Yeah, they made a really cool movie about it. Woody Harrelson,

176
00:19:53,200 --> 00:20:01,600
isn't it? I can't remember the name. But it's about that era of Sarah Palin. It's based on a book

177
00:20:01,600 --> 00:20:11,520
whose Sarah Palin's advisor after the campaign was over, he wrote, and it was a disaster with her.

178
00:20:13,040 --> 00:20:18,560
But again, she's one person. There are thousands of people who are supporting her. She's still

179
00:20:18,560 --> 00:20:25,200
apparently making the most money in the conservative charities and conservative fundraisers.

180
00:20:25,200 --> 00:20:32,560
Who knows? It's really strange. You think the new government of Canada will have any effect in

181
00:20:34,960 --> 00:20:44,720
the social life of Americans in a way that this is a liberal government. We pulled out of Syria

182
00:20:44,720 --> 00:20:51,600
in the first week that he became a prime minister. And then we started getting refugees,

183
00:20:51,600 --> 00:20:57,760
even though the numbers are not comparable with what Europe is doing. And that we can talk about.

184
00:20:59,200 --> 00:21:04,400
I fully expect the Trudeau's government to be kind of an example for Americans to show what's

185
00:21:04,400 --> 00:21:14,240
possible. Yeah. Other than the ridiculous choices they have now. I mean, it's sad. I don't know.

186
00:21:14,240 --> 00:21:18,480
Me and my girlfriend, we have this conversation. She's a big supporter of Hillary Clinton,

187
00:21:18,480 --> 00:21:23,040
because she also has worked for Hillary Clinton. And she was very impressed with her as a boss.

188
00:21:23,040 --> 00:21:30,880
She's a smart person. She brought technology in a very involved way into state department.

189
00:21:31,840 --> 00:21:37,360
And she's a defender of women rights. But then you watch a lot of different stuff that's been

190
00:21:37,360 --> 00:21:44,800
put together, how she has flip flopped from time to time. And then you wonder, you know,

191
00:21:44,800 --> 00:21:51,920
is this really the best that United States of America can offer as a candidate for presidency?

192
00:21:51,920 --> 00:21:56,000
Like there's nobody else. My favorite candidate would be Eric Schmidt of Google.

193
00:21:57,280 --> 00:22:05,920
Yeah, because they are obviously smarter than anybody who's running, you know, for president

194
00:22:05,920 --> 00:22:12,480
politicians. But unfortunately, we are kind of taken hostage by lawyers, it seems like.

195
00:22:12,480 --> 00:22:17,280
Neil deGrasse Tyson was also saying that you look at the Congress and Senate,

196
00:22:17,280 --> 00:22:22,000
the lawyer, lawyer, businessman, lawyer, lawyer, businessman. Where are the scientists? Where

197
00:22:22,000 --> 00:22:29,120
are the engineers? They're in Europe. I mean, the Europeans don't have the same problem with

198
00:22:29,120 --> 00:22:35,680
their politicians. They actually have academics and politics. Which they should. They should.

199
00:22:35,680 --> 00:22:42,080
You'd expect that. Yeah. But no, it's, it's, I mean, a lot of good news is coming from

200
00:22:44,320 --> 00:22:50,560
the West in the sense that countries with innovation, you know, the first world countries.

201
00:22:51,760 --> 00:22:56,880
But at the same time, you see a lot of nonsense is also coming out of them.

202
00:22:56,880 --> 00:23:04,080
Look at Europe, the problem that they have now with the refugee crisis.

203
00:23:06,000 --> 00:23:10,480
And what happened in Germany, which I'm sure you know about in the New Year's Eve,

204
00:23:11,440 --> 00:23:17,360
it just kind of makes it worse. And already, I think people are completely ignoring them.

205
00:23:18,480 --> 00:23:24,480
They're the radical right who are, they're rising very powerful in decades.

206
00:23:24,480 --> 00:23:33,920
You can say in Germany, in Greece, in France, in Finland, in Sweden, you see people who were

207
00:23:33,920 --> 00:23:39,760
national socialists and they, they couldn't even breathe really in a political spectrum in those

208
00:23:39,760 --> 00:23:45,520
countries. They're, they have a lot of supporters now and the extreme right group in Germany,

209
00:23:45,520 --> 00:23:55,760
in Germany, Pegida, they had a, they had a rally here in Ottawa and there are videos on

210
00:23:55,760 --> 00:24:00,240
YouTube that they were on the streets and there are also people who are countering them,

211
00:24:00,240 --> 00:24:06,480
who are anti-racism and all of that. Where do you, where do you see this whole social conflict

212
00:24:06,480 --> 00:24:15,840
is going? Well, from what I understand, the attacks in Cologne New Year's Eve were actually suppressed,

213
00:24:15,840 --> 00:24:19,200
news of the attacks were suppressed by the government because they're afraid it would

214
00:24:19,840 --> 00:24:25,440
fuel kind of anti-immigration sentiment on the right. And then the mayor, as soon as the news

215
00:24:25,440 --> 00:24:31,200
came out, it made it much worse because now the socialist governments are protecting the bad actors.

216
00:24:31,200 --> 00:24:38,400
That's right. That's not the side you want to be on. No. And the mayor of Cologne, the mayor

217
00:24:38,400 --> 00:24:44,640
basically blamed the women. The women, great. Yeah. It's like, keep, keep the man at the arm

218
00:24:44,640 --> 00:24:50,480
length. I'm like, what are you talking about? Like, do you flip this easily? Like, what did it take?

219
00:24:51,040 --> 00:24:58,160
And my understanding is that they're getting 1 million, 1 million plus refugees into Germany.

220
00:24:58,160 --> 00:25:03,360
This is not going to go away, this problem, because then they're going to push for them to

221
00:25:03,360 --> 00:25:08,880
become citizen faster so they can vote for that party. Right. That's my thinking. And I have the

222
00:25:08,880 --> 00:25:13,920
same thinking about abortion and people who are against it, that they don't care about that baby.

223
00:25:13,920 --> 00:25:18,800
They just care about one more Christian and one more Muslim or one more whatever. It's a number.

224
00:25:18,800 --> 00:25:27,120
Mm-hmm. And the fact that they're committing suicide, I think, in Europe for the sake of

225
00:25:27,120 --> 00:25:35,360
their own political game and agenda. Yeah, it looks ironic from outside observer.

226
00:25:35,360 --> 00:25:44,560
Yeah, for sure. And as you're saying, Europeans, they master a lot of things much faster than

227
00:25:44,560 --> 00:25:52,400
Americans. They already, as you said, have academics and scientists and their political system

228
00:25:53,200 --> 00:25:59,360
completely involved. But they, at the same time, you see the openness in the European way and

229
00:25:59,360 --> 00:26:05,520
multiculturalism in the European way that they promoted, for example, in England is causing

230
00:26:05,520 --> 00:26:16,000
a lot of problem right now. See all those Muslim British or British Muslims who joined the terrorist

231
00:26:16,000 --> 00:26:21,280
group ISIL in the Middle East. A lot of them are British, the guy who-

232
00:26:21,280 --> 00:26:23,760
They were born and raised in Brittany.

233
00:26:23,760 --> 00:26:29,360
Yeah, like they went to school, sometimes in BBC, they're talking to people who knew them

234
00:26:29,360 --> 00:26:37,200
when they were a kid. The guy, Jihadi John, who just got assassinated like two weeks ago,

235
00:26:37,200 --> 00:26:42,640
he was replaced by another British guy. And they right away found out who he was.

236
00:26:44,080 --> 00:26:55,520
And he was one of the people who are very closely related to, excuse me, to the Islamic extremist

237
00:26:55,520 --> 00:27:00,720
Anjum Chowdhury. I don't know if you know him, he's like a preacher for extreme Islam in London

238
00:27:01,600 --> 00:27:05,600
and completely protected by the government. He's getting paid by the government.

239
00:27:07,520 --> 00:27:13,680
But that guy who I had seen before in the documentaries about them, now he joined

240
00:27:14,640 --> 00:27:20,080
ISIL and he became a new Jihadi John. He just released a video. It's like very surreal.

241
00:27:20,080 --> 00:27:26,880
It is. I think it's really interesting to see the recent backlash against these progressive

242
00:27:26,880 --> 00:27:33,120
values, the ones that are protecting the terrorists. In fact, there's a new term for them, the regressive

243
00:27:33,120 --> 00:27:39,040
left. They're the ones that are attacking Sam Harris and his colleagues for actually addressing

244
00:27:39,040 --> 00:27:46,800
the problem. So Glenn Greenwald and Ben Affleck, of course, on the Bill Marshall calling Sam a

245
00:27:46,800 --> 00:27:56,240
racist for saying that this is actually an issue. Yeah. I hope that we come to rationality and

246
00:27:57,680 --> 00:28:03,600
smart thinking in a way that we don't really have to get our facts from Ben Affleck, for example.

247
00:28:05,040 --> 00:28:11,760
Because the other side of his story, who at that occasion was Sam Harris, he's more than qualified

248
00:28:11,760 --> 00:28:19,600
to talk about any of those things. He has studied it. He's a philosopher and a neuroscientist

249
00:28:20,560 --> 00:28:27,040
and he's been writing and talking about these issues for a while, for a long time.

250
00:28:27,760 --> 00:28:29,120
Since he published the end of faith.

251
00:28:29,840 --> 00:28:36,880
Absolutely. And I think 9-11 was a horrible thing, but in a way, a lot of good came out of it in a

252
00:28:36,880 --> 00:28:45,600
sense that people like Sam Harris, people like Richard Dawkins, Christopher Hitchens, Douglas

253
00:28:45,600 --> 00:28:53,040
Morey, this people started targeting what the problem really they think is. And I agree with

254
00:28:53,040 --> 00:28:58,560
them completely. Do you think there would have been no God Delusion published, if not for 9-11?

255
00:28:59,840 --> 00:29:06,240
End of faith or then it's breaking the spell? Do you think those were all a result of 9-11?

256
00:29:06,240 --> 00:29:14,800
I think so. I think so. Possibly. Earlier in the podcast, you said something that triggered this

257
00:29:14,800 --> 00:29:21,360
odd idea. We were talking about elections in Trump and in the context of Trump,

258
00:29:21,360 --> 00:29:30,240
he immediately brought up reality shows. And it just made me wonder, what if the elections are

259
00:29:30,240 --> 00:29:37,840
a reality show and have been since, say, JFK debated Nixon in 1960 on TV. That was the first

260
00:29:37,840 --> 00:29:45,280
televised debate. What if the election has been a reality show ever since? What if?

261
00:29:48,880 --> 00:29:57,200
And when it gets through the free world for four years. Yeah. It could be, I guess. I think

262
00:29:57,200 --> 00:30:04,320
everything is possible. But the question would be, who is producing the reality show? Who's

263
00:30:04,320 --> 00:30:15,840
okaying it? Maybe it's the media. Let's see. Let's follow the money. There's a lot of spending

264
00:30:15,840 --> 00:30:28,080
on elections. Where does that money go? I would imagine mostly goes to media. Some lobbyists,

265
00:30:28,080 --> 00:30:37,920
maybe, to shift the position. Elections are all about advertising. That's right. So advertisers.

266
00:30:37,920 --> 00:30:47,520
Yeah, they're running the federal election reality show. Yeah, that's awesome. Well,

267
00:30:47,520 --> 00:30:54,880
you're right by saying that let's follow the money and see where it leads because everything

268
00:30:54,880 --> 00:31:01,760
is basically money. Well, that's been Bernie Sanders being complete this election cycle,

269
00:31:01,760 --> 00:31:08,640
isn't it? The big business owns the politicians. I think that's absolutely true. The politicians

270
00:31:08,640 --> 00:31:14,240
are absolutely corrupt. It's special interests that control legislation. That's been proven

271
00:31:14,240 --> 00:31:25,520
statistically. Yes. That makes a lot of sense. So he offers a solution to add even more

272
00:31:25,520 --> 00:31:32,160
legislation to prevent this sort of thing. But that might make it worse. That just adds more

273
00:31:32,160 --> 00:31:38,800
regulations to be that it would be subject to regulatory capture. It's just a bigger government.

274
00:31:39,760 --> 00:31:43,920
The more power the government has, the more value there is in controlling it.

275
00:31:43,920 --> 00:31:55,920
Right. That's very true. Jeremy, what's his family name? He wrote a book, Future Money,

276
00:31:57,360 --> 00:32:02,880
and our friend Nicola interviewed him too, which he's talking about.

277
00:32:02,880 --> 00:32:09,920
Is that Rivkin? Rivkin. Yes, Jeremy Rivkin. Thanks. A zero margin society.

278
00:32:09,920 --> 00:32:19,920
How do you think that kind of a society will affect this money talks? Can you describe what

279
00:32:21,600 --> 00:32:30,880
in a way that, let's say, more and more human resources being replaced by machines.

280
00:32:31,600 --> 00:32:34,800
So you don't have to pay them salary. You don't have to pay for their...

281
00:32:34,800 --> 00:32:39,200
You don't have to pay payment. Is it the argument that robots are going to put

282
00:32:39,200 --> 00:32:44,800
everyone out of work? That money will have no value. Basically, the end of capitalism.

283
00:32:46,160 --> 00:32:54,320
That's the idea. There will be no money in this way. But is there still value in this society?

284
00:32:55,680 --> 00:33:01,840
Value is a concept? Will people value one thing over another? Will they value living

285
00:33:01,840 --> 00:33:08,080
in one place over another? Will they value seeing somebody talk or seeing some show more than another?

286
00:33:08,080 --> 00:33:14,720
You know what they were saying that in the past 20 years, apparently, a trend has begun that

287
00:33:16,400 --> 00:33:20,960
a newer generation, a younger generation, they're not buying houses anymore. They rent.

288
00:33:24,320 --> 00:33:27,360
A lot of people go to college, but a lot of people don't go to college

289
00:33:27,360 --> 00:33:37,200
because it's a very big bet. I went to film school and I still have not paid my student loan fully.

290
00:33:38,160 --> 00:33:46,160
So you think about, because when you spend money, it's about a business. Am I making

291
00:33:46,160 --> 00:33:53,840
the right investment? Am I going to get anything back from it? And that just adds to it. It's like

292
00:33:53,840 --> 00:34:01,520
buying a house. It's like you have a mortgage before even you graduated. So more and more

293
00:34:01,520 --> 00:34:08,800
people are shifting, I think, towards individualism. And I think this is actually a time that Ayn Rand

294
00:34:08,800 --> 00:34:16,560
got to get big again. More and more people will be interested in objectivism and think about

295
00:34:16,560 --> 00:34:25,280
yourself. But isn't staying away from mortgages and renting instead just a shift from long-term

296
00:34:25,280 --> 00:34:36,640
investment to short-term flexibility? I don't know. I don't know. I'm confused about how you

297
00:34:36,640 --> 00:34:45,360
derive some trend towards individualism from that. It's not from that. I think it's just an

298
00:34:45,360 --> 00:34:55,120
observation about our social life that in early 90s, you would see boomboxes and a group of people

299
00:34:55,120 --> 00:35:00,080
gathering around it and walking on the street under a shoulder. One person has a boombox on

300
00:35:00,080 --> 00:35:04,560
the shoulder and five, six other people walking with them. People were talking more to each other.

301
00:35:04,560 --> 00:35:12,080
You don't have to talk to anybody literally anymore. Yeah, I'm with you. I'm not saying it

302
00:35:12,080 --> 00:35:20,160
in the way that, oh, even if it was like old days, I'm not a fan of that at all. But I think

303
00:35:20,160 --> 00:35:30,320
it is more about, it's less dependent on a group, which I think is a good thing. When you have

304
00:35:30,320 --> 00:35:35,360
Instagram and the culture of selfie, for example, why is it that big, you think?

305
00:35:35,360 --> 00:35:49,760
That's a mystery to me. That people have been a fan of it since it's crazy. It's one of those

306
00:35:49,760 --> 00:35:57,600
things that begun with being able to take a photo of yourself and see yourself with a phone and

307
00:35:57,600 --> 00:36:03,840
selfie has begun. It's just a no-brainer. Kim Kardashian has a book of selfies. I don't know

308
00:36:03,840 --> 00:36:07,840
if you know about that. I did not. Yeah, full of selfies. But thanks for telling me.

309
00:36:10,640 --> 00:36:15,680
So I don't think it's a bad thing. I think we are dependent less on group. Maybe it's a,

310
00:36:15,680 --> 00:36:22,080
maybe you could look at these selfie trends as kind of an artistic expression. I'm all for

311
00:36:22,080 --> 00:36:29,840
having online avatars and having many of them. I consider none of them are identical to me.

312
00:36:29,840 --> 00:36:38,000
They're crafted or designed by me for specific purposes in specific communities. I've got a

313
00:36:38,000 --> 00:36:43,280
social one on Facebook. I've got a very professional one on LinkedIn. And every different community

314
00:36:43,280 --> 00:36:50,320
I'm in, I have a different persona for that community. And I don't really worry about

315
00:36:50,320 --> 00:36:57,680
privacy very much because I only enter the data for that persona that I want to be associated

316
00:36:57,680 --> 00:37:03,120
with that persona. Privacy, I'm not concerned about that at all. And I think people are freaking out

317
00:37:03,120 --> 00:37:09,600
about something that hasn't existed in many, many years. It's like, there was a really cool

318
00:37:09,600 --> 00:37:16,960
interview with Larry Ellison on, I watched it on YouTube, Oracle. Yeah. And they're talking about

319
00:37:16,960 --> 00:37:22,560
NSA and the effect of that. And he's like, do you know who, before NSA had all your information,

320
00:37:22,560 --> 00:37:30,000
credit card companies? And there was an article, and I think MIT

321
00:37:31,120 --> 00:37:38,880
publication too, about how privacy has died. And it's a very new thing. And it has died already.

322
00:37:38,880 --> 00:37:48,400
How people have always favored comfort and accessibility to privacy. But people are making

323
00:37:48,400 --> 00:37:53,200
a huge deal out of it. And at the same time, like Facebook does that, Google does that,

324
00:37:53,200 --> 00:37:59,600
don't use them then. Right. Exactly. And if you want to keep something private, I think it's

325
00:37:59,600 --> 00:38:06,880
your responsibility to encrypt it or use secure channels. I agree. Technology is the solution.

326
00:38:06,880 --> 00:38:12,560
Absolutely. It's your responsibility to learn new ways to protect yourself and your information.

327
00:38:12,560 --> 00:38:19,360
Exactly. I feel the same way about when you're saying, Google is selling our information.

328
00:38:20,240 --> 00:38:26,640
So that's not a good thing. Google is not charging you for anything. So you can use anything. And

329
00:38:26,640 --> 00:38:33,920
then that's their business model. Right. That's the cost of using Google. Exactly. So don't use it.

330
00:38:33,920 --> 00:38:41,440
If it's too expensive, then don't. Yeah. Don't use, use DuckDuckGo or whatever else,

331
00:38:41,440 --> 00:38:48,320
which is pretty cool. It's not Google. I guess the opponents are mistaking these companies for

332
00:38:48,320 --> 00:38:58,160
charities. Yeah. Like you, the users are the product. They're not really hiding that fact.

333
00:38:58,160 --> 00:39:03,120
Absolutely. They don't charge you. They do charge their advertisers. You are the product.

334
00:39:03,120 --> 00:39:06,240
Absolutely. And there's nothing wrong with that, I don't think.

335
00:39:08,000 --> 00:39:11,280
That's your side of the bargain. Yeah. That's why you get all these great,

336
00:39:11,280 --> 00:39:15,760
facilities. This is where we are now, you know, and I see it in a way that

337
00:39:17,440 --> 00:39:23,920
from the time that man has sat in the cave and decided I want to go outside, that's a decision

338
00:39:23,920 --> 00:39:29,360
that you made and everything else that can happen to you outside of that cave are the

339
00:39:29,360 --> 00:39:34,720
alternative of that decision. So don't leave if you don't want to deal with anything. It's the

340
00:39:34,720 --> 00:39:40,480
same thing. If you don't want to deal with these things, don't use internet. You know, people make

341
00:39:40,480 --> 00:39:47,440
the same thing about aging. I think the age reversal aging technology that there are a couple of

342
00:39:49,040 --> 00:39:55,520
organizations that are working on it. That that's impossible. If you take this, this is like one

343
00:39:55,520 --> 00:40:01,040
of the stupidest things I've ever heard. If you take death out of life, life has no meaning.

344
00:40:01,680 --> 00:40:08,720
I've heard that a lot. Yeah. Maybe for you. The meaning for me is that I want to live and

345
00:40:08,720 --> 00:40:17,680
not being stuck with being alive. But I don't want to be stuck with dying either. I want to have

346
00:40:17,680 --> 00:40:24,000
options. You know, and I'm seeing it in a way that when we get to a point that we can back up our

347
00:40:24,000 --> 00:40:31,680
brain inside the machine, maybe one of me want to be backed up in the dropbox of a future. And

348
00:40:31,680 --> 00:40:36,480
then I just want to commit suicide because I want to know how that feels. And then that

349
00:40:36,480 --> 00:40:40,880
version of me, my backup continues. I'm like, Oh, cool. I don't even remember.

350
00:40:43,200 --> 00:40:52,480
Yeah. That's might absolutely be possible. We have uploads. Do you expect to see uploads within

351
00:40:52,480 --> 00:41:07,920
your lifetime? It's very hard to say like it's Yeah.

352
00:41:12,000 --> 00:41:18,560
Also, you know, I guess, if technology and science advance in a rate that they have been advancing,

353
00:41:18,560 --> 00:41:30,000
and no major thing happens, possibly. But you don't really know, you know, I mean,

354
00:41:30,000 --> 00:41:38,560
it's it's a crazy world. The fact that it's funny when this whole ISIL thing began began in Iraq.

355
00:41:40,960 --> 00:41:46,880
I was telling my friends, this is a very good thing that it's Iraq, because if it was Pakistan,

356
00:41:46,880 --> 00:41:51,440
for example, they have nuclear weapon, they would have nuclear weapon, the game will be

357
00:41:51,440 --> 00:41:58,880
very different than and now they're in Afghanistan. You know, and I was watching a documentary about

358
00:41:58,880 --> 00:42:06,800
it today by frontline. It's very good. ISIS in Afghanistan. And they're saying a lot of Taliban,

359
00:42:06,800 --> 00:42:14,320
they're leaving Taliban to join ISIS because ISIS pays $700 a month for each fighter. Apparently,

360
00:42:14,320 --> 00:42:20,080
that's a rate. And that's a lot of money. So they did they joined them. So when that's

361
00:42:20,720 --> 00:42:24,000
so the Taliban's working with them were no, it's joining ISIS.

362
00:42:24,720 --> 00:42:31,360
ISIS is fighting Taliban, but they're men, but they're members of Taliban who've

363
00:42:31,360 --> 00:42:35,600
deserted Taliban and they're continuing doing that because ISIS pays more.

364
00:42:36,560 --> 00:42:40,640
That's the thing. Also, do they think they want to join the winning side?

365
00:42:40,640 --> 00:42:47,520
I would think that having a higher pay rate isn't going to help if you're on the losing side.

366
00:42:48,720 --> 00:42:52,640
Well, I mean, I don't know, because well, ISIS,

367
00:42:55,040 --> 00:43:00,560
at the same time, it's very hard to kind of imagine what would a Taliban know information wise, like

368
00:43:02,000 --> 00:43:10,240
it's very difficult to judge because it's the opposite side of the side that we are on and

369
00:43:10,240 --> 00:43:16,880
watching the news and conceiving the news. For that Taliban, the problem as they're explaining

370
00:43:16,880 --> 00:43:21,760
is that we don't need ISIS because we already have Islamic government under Taliban.

371
00:43:23,440 --> 00:43:28,640
But then they're making the argument some of the Taliban who desert because it's just a religion

372
00:43:28,640 --> 00:43:41,520
filled with loopholes. It's like, no, it says in the Quran that when there is a caliphate,

373
00:43:42,800 --> 00:43:52,240
which is like the state, Islamic state, then you have to join that. It's an obligation for you.

374
00:43:52,240 --> 00:43:59,280
So with all these insane people around the world, if they get ahold of some serious weapon, they can

375
00:43:59,280 --> 00:44:07,040
cause a massive pause to this whole thing. At the same time, they might speed things up,

376
00:44:10,320 --> 00:44:19,120
like Manhattan Project. It was because of war that we build, not we Americans built,

377
00:44:19,120 --> 00:44:27,680
uh, West allies built nuclear weapon, nuclear bomb. And then again, it was because of war that

378
00:44:27,680 --> 00:44:37,360
we went to the moon. Cold war. And then it stopped and we haven't gone back to the moon or anywhere

379
00:44:37,360 --> 00:44:44,560
else. And we are started doing it now, maybe because Chinese are getting there too. They said

380
00:44:44,560 --> 00:44:50,880
they're going to land the man on the moon 2020 or something like that, which is not even comparable.

381
00:44:50,880 --> 00:44:57,680
And I like the Chinese system, but I much rather speak in English and deal with you like about

382
00:44:57,680 --> 00:45:05,200
the Chinese system. I think Chinese have figured out their country so well and mix two different

383
00:45:05,200 --> 00:45:10,880
systems together so well. And they're so strict about it that they've been getting China to where

384
00:45:10,880 --> 00:45:16,800
it is now. But at the same time, China has become a bubble because it has a very small percentage

385
00:45:16,800 --> 00:45:23,280
of really, really rich people now. And then you go to central China and all those farmlands and it's

386
00:45:23,280 --> 00:45:32,320
just like 16th century. But they're the best capitalists in the world, I think. But again,

387
00:45:32,320 --> 00:45:40,800
like Vietnam now, you can hire people much cheaper than China, for example, Philippines.

388
00:45:40,800 --> 00:45:46,560
So a lot of businesses are going for clothing and stuff. I know it's going to Vietnam. It has

389
00:45:46,560 --> 00:45:56,240
been going to Vietnam for years now. But with less dependence on China, it also depends a lot on

390
00:45:56,240 --> 00:46:04,560
how we can improve our manufacturing technologies in the West, in the West. So 3D printing or

391
00:46:04,560 --> 00:46:13,360
machines running things. I think they've, but they also have a brain program, right?

392
00:46:14,400 --> 00:46:18,640
Which Ben Goertzel is working with. Who's the other guy?

393
00:46:18,640 --> 00:46:29,360
Oh, right. Ben Goertzel, mean the brain in software? I know the guy who Ben Goertzel is working with,

394
00:46:31,200 --> 00:46:38,320
who's also in Ray Kurzweil's documentary. He has a very negative perspective about artificial

395
00:46:38,320 --> 00:46:48,880
intelligence, Hugo. Hugo Degares. Yes, yes. And his artillect war. He sees a coming global violent

396
00:46:48,880 --> 00:46:55,200
war between the humans and the AIs. What do you think about that? Well, it's certainly a possibility.

397
00:46:55,200 --> 00:47:02,320
That's why the Machine Intelligence Research Institute exists. That's why Elon Musk and Bill

398
00:47:02,320 --> 00:47:10,480
Gates and the other scientists have joined in raising the issue around. Do you think it's

399
00:47:10,480 --> 00:47:16,720
going to be, because it's like one of the biggest concerns that makes sense, as you explained. That

400
00:47:16,720 --> 00:47:22,080
is, there's a possibility that there will be conflict in that way. Do you think it will be

401
00:47:22,080 --> 00:47:30,320
between humans and machines or humans and human machine fusion? This third thing, because I'm

402
00:47:30,320 --> 00:47:46,480
thinking, if you have, for example, nanobots inside your brain and you evolve, you become the third

403
00:47:46,480 --> 00:47:53,200
thing. I think if there is a symbiosis between humans and AI, if every human has an AI partner

404
00:47:53,200 --> 00:48:05,440
or a bunch of them, then that is a path to a safe future. That's the best possible future,

405
00:48:05,440 --> 00:48:13,120
I think. But wouldn't it be like... The problem would be if there's a single AI that's not human

406
00:48:13,120 --> 00:48:20,640
and not associated with any human that is competing with us for resources and it doesn't

407
00:48:20,640 --> 00:48:30,800
necessarily have any values shared with us. But that's terrifying, but at the same time,

408
00:48:33,040 --> 00:48:38,720
wouldn't that represent a population on Earth right now? Like I'm thinking, imagine

409
00:48:39,520 --> 00:48:46,560
a fundamentalist Christian would have their own AI. It would be the projection of them.

410
00:48:46,560 --> 00:48:53,120
Or you think the AI would have the ability to think... I think the AI and the human together

411
00:48:53,120 --> 00:49:04,160
would be a new, greater entity. Yes, that's exactly. To other people or other agents,

412
00:49:04,160 --> 00:49:10,080
it would look like a single agent, but it's actually a synthesis of the two.

413
00:49:10,080 --> 00:49:18,160
It's connected to the same network, basically. Neural network. Possibly. Yeah. Do you think

414
00:49:18,160 --> 00:49:22,720
they have to though? The way the Tesla machines, for example, they're communicating with each other

415
00:49:22,720 --> 00:49:32,000
now, that again can bring up the privacy issue and all of that because the information that

416
00:49:32,000 --> 00:49:36,000
my car has, I don't want to share it. But if you don't want to share it, you wouldn't be able to

417
00:49:36,000 --> 00:49:43,760
take advantage of the full possibility of that enhancement. So same thing with AI. Don't you

418
00:49:43,760 --> 00:49:51,520
think if they're... I always think of the same thing with open source, AI to reach its full

419
00:49:51,520 --> 00:49:59,920
potential. It has to be open source. Probably. Yeah. It's full potential, yeah. What would be the other

420
00:49:59,920 --> 00:50:09,360
way? The other way to what? The other way to have a functional, superior intellectually,

421
00:50:10,000 --> 00:50:14,400
artificial intelligence by not having it open source. What would be the other way?

422
00:50:16,320 --> 00:50:24,240
It's a proprietary company organization can keep it to themselves just the way software companies

423
00:50:24,240 --> 00:50:35,360
do already. Most source code is not open source. Right. You mentioned HoloLens and Oculus Rift.

424
00:50:35,360 --> 00:50:44,160
We know that 2016 is the year that virtual reality will enter a mainstream market.

425
00:50:45,440 --> 00:50:52,960
At least the commercial early adopter market. Yeah. Well, it's been around for a couple of years

426
00:50:52,960 --> 00:50:59,680
now. Just the development kits. Yeah. But it's become more accessible. You can pre-order Oculus

427
00:50:59,680 --> 00:51:06,160
now. I think it's 599. Just a couple of days ago. Which is awesome. I was waiting for this

428
00:51:07,680 --> 00:51:18,000
version, Crescent Bay, because it has the headphones now. And for me, as an audio engineer

429
00:51:18,000 --> 00:51:23,600
and sound designer, this is like amazing because this is like the difference between black and

430
00:51:23,600 --> 00:51:34,000
white and color TV to me. It's when you can create virtual 360 sonic environments, which is a necessity

431
00:51:34,000 --> 00:51:39,440
for virtual reality worlds to be believable. The sound has to be there. It's just an amazing

432
00:51:39,440 --> 00:51:45,760
opportunity after years of just doing the same thing, basically. How do you think virtual

433
00:51:45,760 --> 00:51:52,880
reality will start affecting, as you said, early adopters and then how it's going to get into

434
00:51:52,880 --> 00:51:57,680
mainstream and how people will feel about it and what do you think people will use it for and how

435
00:51:57,680 --> 00:52:04,880
it's going to change everything? Well, for the first couple of years, at least, I think the

436
00:52:04,880 --> 00:52:11,600
major markets are going to be an entertainment, probably mostly computer games. The Rift is coming

437
00:52:11,600 --> 00:52:19,680
out packaged with the game Eve Valkyrie, a 3D space game, and the videos that I've seen on

438
00:52:19,680 --> 00:52:25,360
YouTube are mind-blowing. I cannot wait to try this out. I'm a big fan of Eve anyway, so...

439
00:52:25,360 --> 00:52:32,320
That's awesome. This is going to be really interesting. But I think almost any 3D computer

440
00:52:32,320 --> 00:52:37,040
game is going to lend itself to the Rift and the developers. The games are going to

441
00:52:37,040 --> 00:52:44,720
incorporate integration points. They'll support the Rift and other similar devices like the HTC.

442
00:52:47,280 --> 00:52:52,800
And then what we'll see then is the price point is going to come down due to volume,

443
00:52:52,800 --> 00:52:56,880
and the next generation is going to be just like we've seen with smartphones.

444
00:52:56,880 --> 00:53:01,520
Oh, absolutely. I think it's smart. I can't remember who said that. Maybe Elon Musk said

445
00:53:01,520 --> 00:53:08,080
that years ago that always wait for the third generation of a technology. Because by the

446
00:53:08,080 --> 00:53:13,680
time it gets to the third generation, it's working good enough and it's priced well enough

447
00:53:14,720 --> 00:53:22,000
for everybody to use it, basically. It's a bit difficult to predict the next market

448
00:53:22,000 --> 00:53:32,720
after games, but you can see getting into education easily and 3D movies. Journalism also.

449
00:53:33,600 --> 00:53:41,280
Yeah. There's been a movie commissioned by Vice, Vice Media, that they made in Syria,

450
00:53:41,280 --> 00:53:50,000
and it's 360. I haven't seen it, but you can't imagine how big of a difference it makes when

451
00:53:50,000 --> 00:53:56,720
you're in the middle of that environment to really feel it and get connected to it. And also,

452
00:53:56,720 --> 00:54:07,360
you take away the middleman. Because more and more, I'm imagining when citizen journalists,

453
00:54:07,360 --> 00:54:11,680
they start arising because of blogging and because of how easy you got to take photos

454
00:54:11,680 --> 00:54:18,640
and videos and put it on YouTube and all those other websites. It will be possible

455
00:54:18,640 --> 00:54:26,720
to make a 360 video quite soon, I think. I think it already is. There's an iPhone app for them.

456
00:54:27,360 --> 00:54:33,600
For 360 videos? Oh, that's amazing. I knew 360 cameras because one of the biggest

457
00:54:33,600 --> 00:54:41,200
challenge back then when they were have to put a couple of GoPro cameras, like four or eight or

458
00:54:41,200 --> 00:54:49,040
12 or something, to get a 360 image, the thing you had to put a lot of cameras. And then the

459
00:54:49,040 --> 00:54:55,440
image that it was giving you needed to be stitched. And the software to stitch those

460
00:54:56,800 --> 00:55:03,520
footage is very complex to work with. It was expensive. But again, like any other technology,

461
00:55:03,520 --> 00:55:12,400
you know, the first generation is basically the first step in trial and error. But now there are

462
00:55:12,400 --> 00:55:19,120
360 cameras that they cost maybe like 400 bucks, the price of a GoPro. And it gives you a already

463
00:55:19,120 --> 00:55:25,920
stitched image that you can use. So somebody in Syria, when they're in war and doing, you know,

464
00:55:25,920 --> 00:55:32,960
a video of, for example, whatever rally or attack or a terrorist, a suicide bomber or

465
00:55:32,960 --> 00:55:37,680
something like that. Of course, I will watch that over a footage that is being transcribed

466
00:55:37,680 --> 00:55:44,000
for me by CNN, for example. It might be in the minority there. Really? How many people want

467
00:55:44,000 --> 00:55:51,200
to experience a suicide bomber? I don't know. I would imagine if I'm interested in what's going

468
00:55:51,200 --> 00:55:59,120
on there, I want to kind of feel it to maybe it just gives you perspective, I think, in my opinion.

469
00:55:59,120 --> 00:56:09,440
Yeah, I think it could definitely have a lot of value. Maybe for films and TVs too, or treatments of

470
00:56:11,040 --> 00:56:17,040
I know that they're using that. There was a video of this girl who was claustrophobic.

471
00:56:17,840 --> 00:56:24,960
And she was so extreme that she couldn't take elevators. So they designed a game for her that

472
00:56:24,960 --> 00:56:32,080
was emulating her being inside the elevator. And she trained with that and she she took an elevator

473
00:56:32,080 --> 00:56:38,880
for first year after who knows how many years and it's amazing. So in therapy and treatment.

474
00:56:38,880 --> 00:56:46,160
Yeah, absolutely. PTSD. They've they've been trying that. I'm looking forward to that. It's

475
00:56:46,160 --> 00:56:54,800
apparently one of the biggest products and technology in CES. This year was also about

476
00:56:55,440 --> 00:56:58,960
virtual reality, everything about virtual reality. So this is the year.

477
00:57:02,080 --> 00:57:09,600
As a last question before my last standard question, which I will ask everybody. But last

478
00:57:09,600 --> 00:57:20,480
question of series of question conversations that we've had. How do you think ethics and morality

479
00:57:21,440 --> 00:57:26,080
will translate for an artificial intelligence?

480
00:57:29,840 --> 00:57:32,400
Well, let's see. Pretty deep question.

481
00:57:32,400 --> 00:57:41,520
One way to look at AI is it's a system designed to make choices based on its goals and beliefs.

482
00:57:43,280 --> 00:57:50,720
An AI that's extremely smart, super intelligent, will tend to have very accurate and sophisticated

483
00:57:50,720 --> 00:58:01,040
beliefs. The goals are basically beliefs about preferences, which states of the world are preferable

484
00:58:01,040 --> 00:58:05,040
to other states of the world. Can we even conceive what those beliefs will be?

485
00:58:06,320 --> 00:58:14,640
It's we can conceive of hard coding beliefs in such a way that the AI will be unable to

486
00:58:15,440 --> 00:58:22,400
change its own goals. But at the same time, that may very well cripple the AI in the sense that

487
00:58:22,400 --> 00:58:29,440
it won't be able to get past certain level of sophistication or intelligence if we

488
00:58:29,440 --> 00:58:34,240
cripple its ethical system. So presumably, if it's much smarter than humans.

489
00:58:34,240 --> 00:58:36,720
Oh, absolutely. As it should be. As it should be.

490
00:58:36,720 --> 00:58:45,760
We can't really expect to force our own morality, per se, on a machine that's supposed to be much

491
00:58:45,760 --> 00:58:51,520
better and bigger and the evolution of us. But that is exactly the project of friendly AI,

492
00:58:52,160 --> 00:58:58,480
is to enforce our own values onto the AI. But our values aren't that good. We still lie.

493
00:58:58,480 --> 00:59:02,560
There's a problem. We're not that good ourselves.

494
00:59:02,560 --> 00:59:03,200
We are not.

495
00:59:03,200 --> 00:59:06,320
We don't even know what would be good.

496
00:59:07,760 --> 00:59:15,840
I mean, we have no rigorous way of proving that any set of values is better than any other set.

497
00:59:15,840 --> 00:59:24,560
It's also very confusing to, for example, it's very interesting how it comes to you when you

498
00:59:24,560 --> 00:59:28,640
talk about these kind of things with children, because children have no filter and they just

499
00:59:28,640 --> 00:59:34,960
ask you whatever that comes to their mind. You can't tell a child that killing is wrong

500
00:59:35,600 --> 00:59:41,440
as a rule, because then he or she will be asking why those people are dying then.

501
00:59:41,440 --> 00:59:44,080
Why, you know.

502
00:59:44,080 --> 00:59:47,040
You can tell them you just won't be very convincing.

503
00:59:47,040 --> 00:59:53,440
Exactly. And you can't say it as a rule, right? And that's like one of the biggest

504
00:59:53,440 --> 00:59:59,520
parts of morality, that killing is wrong. This is just an example. How are we going to explain

505
00:59:59,520 --> 01:00:04,960
that to an AI? And that is one of the first things that we need to explain it now that

506
01:00:04,960 --> 01:00:11,760
you're talking about as we spoke about it in military, that an autonomous machine,

507
01:00:12,640 --> 01:00:21,200
okay, killing is wrong, just wound people. But if this guy did this, then killing is fine.

508
01:00:21,200 --> 01:00:27,040
You can't really do that. And that's why I think a lot of these attempts, because

509
01:00:27,920 --> 01:00:33,920
moralities are different from place to place. The morality that we have is a very different

510
01:00:33,920 --> 01:00:40,480
morality that, for example, people have in Yemen, for instance. And at the same time,

511
01:00:40,480 --> 01:00:47,920
the machine to get to its full potential, it needs to be open source. So like Watson

512
01:00:47,920 --> 01:00:55,920
or IBM, it's open source. So if you have a strong enough of a hardware, you would be able to have.

513
01:00:57,600 --> 01:01:00,880
Are you sure Watson's open source? I think so, because they were asking

514
01:01:02,800 --> 01:01:12,160
for programmers and programmers mainly to use Watson capability and do their own projects.

515
01:01:12,160 --> 01:01:22,720
But let's say an open source AI, it would be usable by some Chinese kid in his grandparents'

516
01:01:23,440 --> 01:01:30,720
basement to build whatever he wants to build with it, or a Russian hacker, or an evil person,

517
01:01:30,720 --> 01:01:36,880
per se, evil quote unquote. Yes, that's absolutely true. Ben Goertzel has his own

518
01:01:36,880 --> 01:01:43,920
open source project, OpenCog, I think. And just a couple of weeks ago, Elon Musk and some Peter

519
01:01:43,920 --> 01:01:49,120
Thiel, I think, and some other courts announced that they were funding another open source project,

520
01:01:49,120 --> 01:01:54,480
OpenAI, and they were investing a... Peter Thiel was involved in that, so I didn't know that.

521
01:01:55,680 --> 01:02:02,960
I'm not sure, I think. I would call that. But I think they were putting something like a billion

522
01:02:02,960 --> 01:02:11,600
dollars into this project, significant funding. And at the same time, they came into quite a bit

523
01:02:11,600 --> 01:02:19,840
of criticism saying, if you think AI is dangerous, why on earth would you open source it? And the

524
01:02:19,840 --> 01:02:25,040
analogy is made with nuclear arms. If you think nukes are dangerous, you certainly don't want to

525
01:02:25,040 --> 01:02:31,520
publish an easy how-to on the web, because it could easily fall into the wrong hand. So why are

526
01:02:31,520 --> 01:02:40,400
they doing it with AI? What's the explanation? I haven't heard their explanation. One possibility is

527
01:02:40,400 --> 01:02:51,360
that unless you start looking into these issues right away, hardware is going to be at a point

528
01:02:51,360 --> 01:02:56,480
sometime in the future that there might be an extremely quick ramp up time from human level

529
01:02:56,480 --> 01:03:03,440
to superhuman level AI, and we won't be prepared. So it's better to start now, even if it is dangerous.

530
01:03:04,720 --> 01:03:09,840
To slow it down, basically. Well, to be better prepared for when it happens,

531
01:03:11,120 --> 01:03:16,720
even if you end up spinning it up in some ways. Would we know it when it happens, you think?

532
01:03:18,560 --> 01:03:26,160
It depends how it is realized. I think if it's a pure AI rather than, say, a human emulation

533
01:03:26,160 --> 01:03:32,160
where a brain is scanned and then emulated in the hardware to get an AI, if it's just a pure

534
01:03:32,160 --> 01:03:41,520
code AI, it could happen in a small group that's secret. Maybe some company wants to build an AI

535
01:03:41,520 --> 01:03:47,920
to do trading on the stock exchanges in order to make a lot of money. And it gets more and more

536
01:03:47,920 --> 01:03:52,960
intelligent. It gathers more and more data in order to interpret the financial data coming in.

537
01:03:52,960 --> 01:03:59,760
And so it builds up a belief system. All it wants to do is make money. And it gets very good at trading

538
01:03:59,760 --> 01:04:06,240
and starts buying companies. But doing it in a way... That's a great idea for a movie.

539
01:04:07,040 --> 01:04:12,640
But does it in a way that's decentralized using lots of versions of itself, lots of

540
01:04:12,640 --> 01:04:18,560
subsidiaries that can't be traced back to the main AI so nobody realizes that it's all controlled by a

541
01:04:18,560 --> 01:04:25,840
single mind until it owns so many resources, it can take over countries?

542
01:04:27,600 --> 01:04:33,680
Would he want to take over countries? If it needs more resources. If that's its main goal is to

543
01:04:33,680 --> 01:04:38,560
acquire resources, financial resources, it could become tremendously powerful

544
01:04:39,520 --> 01:04:46,480
by controlling humans with its resources. Well, it could have a million people working

545
01:04:46,480 --> 01:04:49,600
for it. They don't even know that they're all working for the same mind.

546
01:04:49,600 --> 01:04:54,480
I think if the AI will be based on human morality, that's exactly what the AI would do.

547
01:04:55,280 --> 01:04:57,680
Yeah, he will go after the money.

548
01:05:00,560 --> 01:05:03,600
Until it can build the hardware to make it some robots.

549
01:05:03,600 --> 01:05:13,680
Yeah, that's very interesting. I think I've always enjoyed looking at things as much as I could

550
01:05:13,680 --> 01:05:17,600
as a double-edged sword, everything, because they're good and bad about

551
01:05:18,560 --> 01:05:23,520
literally everything. With a hammer, you can build a house so you can bash somebody's brains.

552
01:05:24,960 --> 01:05:29,760
So it really depends on how we're using it and for what purpose we're using it. And I think

553
01:05:31,200 --> 01:05:37,280
core values of us, unfortunately, have been corrupted a lot, a majority of people,

554
01:05:37,280 --> 01:05:43,440
because of the money centric, money for the sake of money materialistic money.

555
01:05:46,320 --> 01:05:53,920
Whereas I see that as a positive force. I do that people in the current system can

556
01:05:55,360 --> 01:05:58,320
protect their own interests better and advance their own interests better

557
01:05:58,880 --> 01:06:00,800
by serving others commercially.

558
01:06:00,800 --> 01:06:04,720
Mm-hmm. That's interesting.

559
01:06:04,720 --> 01:06:09,520
So it channels their own personal interests. Everyone is self-interested.

560
01:06:09,520 --> 01:06:12,560
Even those that claim not to be, I think, are basically

561
01:06:13,520 --> 01:06:15,840
deluding themselves and others.

562
01:06:17,760 --> 01:06:18,720
Well, they believe as...

563
01:06:19,280 --> 01:06:27,120
But the way to channel that self-interest into a net good for society is to put in a system where

564
01:06:27,120 --> 01:06:32,800
you do well by helping others by selling them products and services, for example.

565
01:06:32,800 --> 01:06:36,320
That's so interesting. Last night, we were watching the documentary called I Am.

566
01:06:37,440 --> 01:06:46,480
It's about this director named Tom Shaliak. He directed a whole bunch of comedies and made

567
01:06:46,480 --> 01:06:52,720
millions and millions of dollars. And then he went through this experience that he thought

568
01:06:52,720 --> 01:07:03,440
that he's going to die, and then he survived that. And he became very, you know, instead of

569
01:07:03,440 --> 01:07:10,320
making our objective profit to make it how we can help each other and help each other grow.

570
01:07:11,440 --> 01:07:16,400
And he was talking to his father and said, why can't we do that?

571
01:07:16,400 --> 01:07:24,080
And his father said, it's a very nice utopian idea that will never, ever happen, you know.

572
01:07:24,080 --> 01:07:31,600
And I think the key is everybody starts within themselves that you just make yourself a better

573
01:07:31,600 --> 01:07:40,320
person and then try to get that understanding to the circle close to you and just expand the circle

574
01:07:40,320 --> 01:07:50,560
more and more. And, you know, I think it would worth more to focus on what we can do individually

575
01:07:51,280 --> 01:07:55,920
than what, why people aren't doing this and this and that, because people are, you know,

576
01:07:55,920 --> 01:08:04,640
built from individuals. But also, all these new technologies have given the opportunity

577
01:08:04,640 --> 01:08:11,280
empowered an individual to change the world if they have a right idea. Yeah, technology is making

578
01:08:11,280 --> 01:08:16,880
people more powerful every day. Exactly. I think it's great and it's a great time that we can live

579
01:08:17,600 --> 01:08:26,720
and enjoy all of these things. The last question literally is if you come in contact with a

580
01:08:26,720 --> 01:08:37,200
species from another civilization with an alien, per se, intelligent aliens,

581
01:08:39,040 --> 01:08:46,080
what would be the biggest achievement that you would mention to that intelligent alien

582
01:08:46,080 --> 01:08:52,080
that we have made and we've achieved as a race? And what would be the biggest mistake that we've

583
01:08:52,080 --> 01:09:09,440
made? Wow. That's a big question. I would offer, actually, the World Wide Web is our greatest

584
01:09:09,440 --> 01:09:16,080
achievement. The technology that makes self-expression possible in collaboration,

585
01:09:16,080 --> 01:09:24,960
I think it's instigated a renaissance across the world. It's amazing. And in a sense,

586
01:09:24,960 --> 01:09:30,560
it's only getting started. So ask me 10 years from now and I'll say the successor for the web,

587
01:09:30,560 --> 01:09:42,560
whatever that is. And as for our worst mistake, I think it's the war machines that we worship

588
01:09:42,560 --> 01:09:48,400
as our nation-states, that they've caused more death, destruction, and suffering than anything

589
01:09:48,400 --> 01:09:54,160
else in human history. And most people are blind to it, that they're proud citizens, they're

590
01:09:55,280 --> 01:10:01,040
proud of their nationality, their patriots, and they're brainwashed into thinking that

591
01:10:01,040 --> 01:10:10,240
this is a good idea. I hope the aliens can shed some perspective on that. I hope they've overcome

592
01:10:10,240 --> 01:10:14,400
it themselves. And if they've come here, I'm pretty sure they have. They haven't destroyed

593
01:10:14,400 --> 01:10:22,080
themselves. So one more thing. I want to thank you very much for having me as your first guest.

594
01:10:22,080 --> 01:10:27,760
I'm really flattered. Yeah, my pleasure, man. And it's been a blast. Yeah. It's been excellent.

595
01:10:27,760 --> 01:10:40,880
Glad to hear. Thanks a lot. Thank you.

