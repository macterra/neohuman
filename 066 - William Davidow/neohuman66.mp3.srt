1
00:00:00,000 --> 00:00:10,520
What happens is that when we're insecure because of rapid change, we tend to react by saying,

2
00:00:10,520 --> 00:00:17,740
this is what worked in the past, and we become more committed to doing precisely what we

3
00:00:17,740 --> 00:00:19,040
did in the past.

4
00:00:19,040 --> 00:00:37,200
So we try to impose past solutions on future problems, but that doesn't work.

5
00:00:37,200 --> 00:00:42,760
Hello and welcome to the 66th episode of Near Human Podcasts at Magabahari Edicologist

6
00:00:42,760 --> 00:00:48,040
on Twitter and Instagram, and you can follow this show on liveinlimbo.com, iTunes, YouTube,

7
00:00:48,040 --> 00:00:49,120
and BitChute.

8
00:00:49,120 --> 00:00:54,480
And today with me, I have the pleasure of having William Davido, the legend of Silicon

9
00:00:54,480 --> 00:00:55,480
Valley.

10
00:00:55,480 --> 00:00:58,200
Thank you so much for your time and being here, Bill.

11
00:00:58,200 --> 00:01:00,280
Well, thank you for having me.

12
00:01:00,280 --> 00:01:01,280
Yeah, absolutely.

13
00:01:01,280 --> 00:01:03,160
It's an honor.

14
00:01:03,160 --> 00:01:07,960
Just to give our audience some more familiarity with you, let's start with your background,

15
00:01:07,960 --> 00:01:11,760
the work you've done, the lives you've lived, and what are you mainly focused on now these

16
00:01:11,760 --> 00:01:12,760
days?

17
00:01:12,760 --> 00:01:25,080
Okay, well, you know, in 1957, I thought I would save the world from Russia when Sputnik

18
00:01:25,080 --> 00:01:26,080
was launched.

19
00:01:26,080 --> 00:01:27,080
How old were you then?

20
00:01:27,080 --> 00:01:32,040
Oh, I was 22.

21
00:01:32,040 --> 00:01:37,280
And the professor announced that Russia had launched Sputnik, and I thought if you have

22
00:01:37,280 --> 00:01:44,920
a grain of technology in your blood, you better study technology and help us win the space

23
00:01:44,920 --> 00:01:45,920
race.

24
00:01:45,920 --> 00:01:50,280
And that started me on a path that led me to Silicon Valley.

25
00:01:50,280 --> 00:01:55,800
And I ultimately ended up running the microprocessor area at Intel and then doing some venture

26
00:01:55,800 --> 00:02:07,560
capitalism, and then at some point, I decided I would try and write a book.

27
00:02:07,560 --> 00:02:13,520
So I did, and you are about to talk to me about my most recent book.

28
00:02:13,520 --> 00:02:18,240
Absolutely, which the book is called The Autonomous Revolution Reclaiming the Future.

29
00:02:18,240 --> 00:02:19,780
We have sold to machines.

30
00:02:19,780 --> 00:02:22,920
You were with Intel since the very beginning, am I right?

31
00:02:22,920 --> 00:02:26,120
Well, I joined them in 1973.

32
00:02:26,120 --> 00:02:30,840
They were five years old, but they were $66 million.

33
00:02:30,840 --> 00:02:37,160
And when I left them in 1985, we were $1.6 billion.

34
00:02:37,160 --> 00:02:38,160
Wow.

35
00:02:38,160 --> 00:02:44,960
Are you impressed by the kind of advancement in chip making and hardware, considering you

36
00:02:44,960 --> 00:02:49,160
know exactly what's been going on, you know the evolution of it, or you're not impressed

37
00:02:49,160 --> 00:02:52,280
because you know all the possibilities that are ahead?

38
00:02:52,280 --> 00:03:00,920
Well, I am tremendously impressed with what we did with the microprocessor.

39
00:03:00,920 --> 00:03:07,900
But you've got to understand that what we did then was very different than what is going

40
00:03:07,900 --> 00:03:08,900
on now.

41
00:03:08,900 --> 00:03:15,880
At that time, with the microprocessor, we were improving the world the way it was.

42
00:03:15,880 --> 00:03:21,600
Like we were making cash registers better and stoplights better and things like that.

43
00:03:21,600 --> 00:03:27,400
And that's very different than what is going on now, where we're changing the social structure

44
00:03:27,400 --> 00:03:29,120
of the world.

45
00:03:29,120 --> 00:03:34,280
And if I look back on it, the rate of advance was really slow.

46
00:03:34,280 --> 00:03:39,160
We were doubling chip density at the rate of 40% a year.

47
00:03:39,160 --> 00:03:43,120
That's a factor of a thousand in 20 years.

48
00:03:43,120 --> 00:03:49,120
I suspect that the internet reduced the cost of communication by a factor of a million,

49
00:03:49,120 --> 00:03:51,600
probably in less than 10 years.

50
00:03:51,600 --> 00:03:57,800
So we're dealing with rates of change or rates of improvement that are a thousand times faster

51
00:03:57,800 --> 00:04:00,800
than what the semiconductor did.

52
00:04:00,800 --> 00:04:06,200
And this rate, would you say, is an inevitable outcome of exponential rate of growth, technological

53
00:04:06,200 --> 00:04:07,200
growth?

54
00:04:07,200 --> 00:04:08,200
Is that right?

55
00:04:08,200 --> 00:04:15,620
Well, it's just all these advances that are taking place in a new world.

56
00:04:15,620 --> 00:04:24,160
We're living in virtual space, which frees us from the limitations of physical space.

57
00:04:24,160 --> 00:04:34,440
And so we've moved from the physical space for which we were adapted to the virtual space

58
00:04:34,440 --> 00:04:40,000
where things happen very quickly for which we're maladapted.

59
00:04:40,000 --> 00:04:44,360
Now before getting into the book, I just want to bring this up for our audience and get

60
00:04:44,360 --> 00:04:50,960
your response to it because you consider what is happening as the third industrial revolution,

61
00:04:50,960 --> 00:04:51,960
I would say.

62
00:04:51,960 --> 00:04:56,240
You tweeted that for the third time in the history of humanity, civilization is undergoing

63
00:04:56,240 --> 00:04:57,640
faith change.

64
00:04:57,640 --> 00:04:59,880
The first was the agricultural revolution.

65
00:04:59,880 --> 00:05:01,680
The second, the industrial revolution.

66
00:05:01,680 --> 00:05:04,720
We are now in the midst of the autonomous revolution.

67
00:05:04,720 --> 00:05:09,440
Now what we are hearing more and more is the fourth industrial revolution that just to

68
00:05:09,440 --> 00:05:14,520
give a definition to our audience, the world economic based on what World Economic Forum

69
00:05:14,520 --> 00:05:15,520
has talked about.

70
00:05:15,520 --> 00:05:18,960
Again, the first industrial is agriculture, second is industrial.

71
00:05:18,960 --> 00:05:24,760
They're saying that the third used electronics and information technology to automate production.

72
00:05:24,760 --> 00:05:29,080
Now a fourth industrial revolution is building the world on the third, the digital revolution

73
00:05:29,080 --> 00:05:32,160
that has been occurring since the middle of the last century.

74
00:05:32,160 --> 00:05:37,120
So how do you see it as only the third, not the fourth?

75
00:05:37,120 --> 00:05:46,480
Well, this is not an extension of the industrial revolution.

76
00:05:46,480 --> 00:05:52,240
The industrial revolution, as far as I'm concerned, has come to an end.

77
00:05:52,240 --> 00:06:00,040
What happened in the various phases of the industrial revolutions is that we made technological

78
00:06:00,040 --> 00:06:09,440
advances, but the forms of our institutions remained the same, like we replaced the handwritten

79
00:06:09,440 --> 00:06:18,260
spreadsheet with Excel, but we still did accounting in firms the same way.

80
00:06:18,260 --> 00:06:23,760
We replaced steam power with electrical power, but we still had factories.

81
00:06:23,760 --> 00:06:30,560
Suddenly, what is happening and why this is not an extension of the industrial revolution

82
00:06:30,560 --> 00:06:33,920
is that the form of what we are doing is changing.

83
00:06:33,920 --> 00:06:37,860
A bank becomes a cell phone application.

84
00:06:37,860 --> 00:06:40,880
It vanishes into virtual space.

85
00:06:40,880 --> 00:06:43,600
That's very different.

86
00:06:43,600 --> 00:06:47,000
The factory did not vanish in the industrial revolution.

87
00:06:47,000 --> 00:06:50,400
It just changed and became more efficient.

88
00:06:50,400 --> 00:06:55,100
Yeah, they're also seeing it as maybe the extension of the third, as they're calling

89
00:06:55,100 --> 00:06:59,880
it, industrial revolution that started in the mid of 20th century by building computers

90
00:06:59,880 --> 00:07:03,280
and chips and then it led to internet and all that.

91
00:07:03,280 --> 00:07:12,760
Yeah, I think the biggest mistake we can make is to think that this is business as usual

92
00:07:12,760 --> 00:07:15,720
with just a little change.

93
00:07:15,720 --> 00:07:20,840
What is happening today is something very fundamental.

94
00:07:20,840 --> 00:07:24,940
Think of what happened during the agricultural revolution.

95
00:07:24,940 --> 00:07:29,480
We had the agricultural revolution and the form of government changed.

96
00:07:29,480 --> 00:07:35,280
We went from being tribal chiefs to having kings and city states.

97
00:07:35,280 --> 00:07:37,360
We used to deal with barter.

98
00:07:37,360 --> 00:07:42,880
We invented money and suddenly we had exchange and markets.

99
00:07:42,880 --> 00:07:48,320
When the industrial revolution came along, we changed very differently.

100
00:07:48,320 --> 00:07:52,900
Most of us prior to the industrial revolution were entrepreneurs.

101
00:07:52,900 --> 00:07:59,120
We worked on farms and lived by what we produced or we were in a cottage industry and made

102
00:07:59,120 --> 00:08:02,800
clothes and shoes that we sold to others.

103
00:08:02,800 --> 00:08:07,560
After the industrial revolution, we became a dependent civilization.

104
00:08:07,560 --> 00:08:13,360
We depended on others to pay us to do work.

105
00:08:13,360 --> 00:08:19,540
That happened that we created the job as a result of that.

106
00:08:19,540 --> 00:08:23,720
Before the industrial revolution, we had small government.

107
00:08:23,720 --> 00:08:33,040
After the industrial revolution, government grew from 3% of GDP to around 40% of GDP today.

108
00:08:33,040 --> 00:08:38,560
These are things and those are what we call social phase changes.

109
00:08:38,560 --> 00:08:44,720
They are very different from just improving the efficiency of society.

110
00:08:44,720 --> 00:08:47,240
What made you write this book at this point?

111
00:08:47,240 --> 00:08:51,520
Was it any specific kind of an event or you just see the trend that is building writing

112
00:08:51,520 --> 00:08:57,640
the book The Autonomous Revolution Reclaiming the Future We've Sold to Machines?

113
00:08:57,640 --> 00:09:01,680
I had three objectives in writing the book.

114
00:09:01,680 --> 00:09:05,920
Number one, I like to write, so it's kind of fun.

115
00:09:05,920 --> 00:09:09,080
It's kind of like being a painter.

116
00:09:09,080 --> 00:09:14,080
The other one was I thought it would be fun to talk to local groups just to get to know

117
00:09:14,080 --> 00:09:16,440
people in the community better.

118
00:09:16,440 --> 00:09:23,040
The third one was the thing I really hope will happen, which is that I hope that I could

119
00:09:23,040 --> 00:09:34,240
influence the thinking of a thousand policymakers so that we wouldn't just treat this as being

120
00:09:34,240 --> 00:09:37,480
just a little bit different than in the past.

121
00:09:37,480 --> 00:09:44,600
We're going to have to think about how we distribute wealth in the future.

122
00:09:44,600 --> 00:09:54,040
You see, in the past, the fundamental mechanism for distributing wealth was the job.

123
00:09:54,040 --> 00:09:58,040
That's how we put money in people's pockets.

124
00:09:58,040 --> 00:10:05,880
If there was a shortage of work, then a lot of people can't participate in that mechanism.

125
00:10:05,880 --> 00:10:12,940
We've got to think about new mechanisms for distributing wealth.

126
00:10:12,940 --> 00:10:15,920
How well do you see our politicians doing this job?

127
00:10:15,920 --> 00:10:21,840
So far, I only can think of Andrew Yang, who's been talking about the... I'm talking about

128
00:10:21,840 --> 00:10:26,000
him beyond the partisan kind of structure because the partisan structure seems to be

129
00:10:26,000 --> 00:10:32,440
counter to measures that need to be taken in order to adopt into what's coming technologically

130
00:10:32,440 --> 00:10:33,800
speaking.

131
00:10:33,800 --> 00:10:44,100
What happens is that when we're insecure because of rapid change, we tend to react by saying,

132
00:10:44,100 --> 00:10:51,340
this is what worked in the past, and we become more committed to doing precisely what we

133
00:10:51,340 --> 00:10:52,800
did in the past.

134
00:10:52,800 --> 00:11:01,160
We try to impose past solutions on future problems.

135
00:11:01,160 --> 00:11:04,300
That doesn't work.

136
00:11:04,300 --> 00:11:09,320
New problems require new solutions.

137
00:11:09,320 --> 00:11:12,200
We have to do those things.

138
00:11:12,200 --> 00:11:17,840
Most of what people are talking about, as far as I'm concerned today, is they're talking

139
00:11:17,840 --> 00:11:24,840
about doing conventional solutions to future problems.

140
00:11:24,840 --> 00:11:30,520
I hope that the free market will create lots of jobs.

141
00:11:30,520 --> 00:11:36,680
I hope that by reducing taxes and getting rid of red tape, it will do enough.

142
00:11:36,680 --> 00:11:46,080
I hope that by creating, moving more manufacturing back, it will help solve the problem.

143
00:11:46,080 --> 00:11:54,400
But I would like to give you sort of a counter example here to think about.

144
00:11:54,400 --> 00:12:00,720
Every time we have gone through one of these social phase changes, we have dramatically

145
00:12:00,720 --> 00:12:07,640
changed the physical infrastructure of our society, like we went from living on farms

146
00:12:07,640 --> 00:12:10,920
to living in cities.

147
00:12:10,920 --> 00:12:17,120
In the future, with the autonomous revolution, it is highly likely that we will have a different

148
00:12:17,120 --> 00:12:22,000
physical infrastructure associated with our society.

149
00:12:22,000 --> 00:12:24,440
I'm not talking about fixing sewers.

150
00:12:24,440 --> 00:12:32,740
I'm saying, hey, maybe people will want to repurpose office buildings and shopping centers.

151
00:12:32,740 --> 00:12:39,440
Maybe they will want to return to living in living cities, which will have more parks.

152
00:12:39,440 --> 00:12:44,400
Maybe they will think sitting in traffic for two hours isn't a good idea.

153
00:12:44,400 --> 00:12:49,640
They're going to want more public transportation, all of those things.

154
00:12:49,640 --> 00:12:54,840
Today we spend 5% of the gross national product on construction.

155
00:12:54,840 --> 00:13:01,200
Maybe in the future we'll be spending 10 or 15% on construction because we will be rebuilding

156
00:13:01,200 --> 00:13:06,260
our physical environment to match the needs of the future.

157
00:13:06,260 --> 00:13:08,520
That would create a lot of jobs.

158
00:13:08,520 --> 00:13:10,840
It's a matter of prioritization too, right?

159
00:13:10,840 --> 00:13:17,240
It takes a body, whether a government or a corporation or whoever, to detect problems,

160
00:13:17,240 --> 00:13:19,560
prioritize them, and come up with solutions.

161
00:13:19,560 --> 00:13:24,320
It just seems like this debacle of bureaucracy that is happening, for example, in the United

162
00:13:24,320 --> 00:13:30,720
States, politically speaking, is just counterproductive to the measures that need to be taken appropriately

163
00:13:30,720 --> 00:13:33,720
to the speed of technological growth.

164
00:13:33,720 --> 00:13:40,400
I believe that there are conservative solutions to the problem, and I believe there are liberal

165
00:13:40,400 --> 00:13:42,860
solutions to the problem.

166
00:13:42,860 --> 00:13:53,400
What is important is that you say, I've got a purpose and a goal, and we all get behind

167
00:13:53,400 --> 00:14:03,800
a certain set of solutions and work to make them work.

168
00:14:03,800 --> 00:14:06,880
Maybe we're not going to pick the optimum solution.

169
00:14:06,880 --> 00:14:12,880
When I was in business, we seldom picked the perfect solution, but as a management team

170
00:14:12,880 --> 00:14:18,800
at Intel, we all said, hey, we've done the best we could to find the solution.

171
00:14:18,800 --> 00:14:21,800
Now let's make the solution work.

172
00:14:21,800 --> 00:14:25,340
That's what we did during the Second World War.

173
00:14:25,340 --> 00:14:28,240
That's what we've done when we faced other crises.

174
00:14:28,240 --> 00:14:32,280
We've made the solution we picked work.

175
00:14:32,280 --> 00:14:35,240
I think this is a very interesting time to talk about solutions too.

176
00:14:35,240 --> 00:14:40,800
We're going through this coronavirus pandemic and see how quickly they respond to it with

177
00:14:40,800 --> 00:14:46,760
respect to coming up with solutions.

178
00:14:46,760 --> 00:14:54,420
That's an example of the fact that probably the biggest threats in our future are going

179
00:14:54,420 --> 00:14:58,760
to be pandemics and bioterrorism.

180
00:14:58,760 --> 00:15:07,640
This one should have convinced us that a pandemic can be a pretty devastating thing.

181
00:15:07,640 --> 00:15:16,880
If you look at it, we have had SARS and MERS and things like this that we've gotten control

182
00:15:16,880 --> 00:15:20,520
of because they weren't quite as bad.

183
00:15:20,520 --> 00:15:24,600
Now we're faced with something that's a real challenge.

184
00:15:24,600 --> 00:15:29,280
We've got to put in place new kinds of solutions to deal with this.

185
00:15:29,280 --> 00:15:30,280
Yeah, absolutely.

186
00:15:30,280 --> 00:15:34,640
Would you talk about some of the elements of this autonomous revolution?

187
00:15:34,640 --> 00:15:39,180
For a lot of people, autonomous cars now becoming more and more familiar kind of an aspect,

188
00:15:39,180 --> 00:15:43,480
but obviously this is not something that is going to be limited to autonomous cars.

189
00:15:43,480 --> 00:15:44,480
No.

190
00:15:44,480 --> 00:15:51,920
What I think is going to happen is that our institutions quite broadly are going to be

191
00:15:51,920 --> 00:15:53,920
restructured.

192
00:15:53,920 --> 00:16:01,740
For example, I looked at finance during this, 8 million people are employed in finance.

193
00:16:01,740 --> 00:16:07,160
What finance is, is basically an information application.

194
00:16:07,160 --> 00:16:14,320
If you look at it, you've got a bank and it's a physical building, but it's an information

195
00:16:14,320 --> 00:16:15,320
application.

196
00:16:15,320 --> 00:16:19,500
What goes on inside it is all information processing.

197
00:16:19,500 --> 00:16:24,240
We're matching buyers and sellers and doing things like this and we're transferring money

198
00:16:24,240 --> 00:16:31,280
around and suddenly we can do all of that virtually.

199
00:16:31,280 --> 00:16:36,360
We may even have new forms of money.

200
00:16:36,360 --> 00:16:43,040
There is an example of the autonomous revolution in finance and maybe 8 million people work

201
00:16:43,040 --> 00:16:44,700
in finance today.

202
00:16:44,700 --> 00:16:49,840
In the future, it could be 4 million, it could be 2 million, but it's obvious that a lot

203
00:16:49,840 --> 00:16:52,320
of things are going to be happening.

204
00:16:52,320 --> 00:16:57,800
If you look across the business world, what I tell people is, look at that office building.

205
00:16:57,800 --> 00:17:06,580
It's got offices and elevators and desks and it's got information network.

206
00:17:06,580 --> 00:17:08,200
People talk to one another.

207
00:17:08,200 --> 00:17:18,120
They send around notes and they're on computer files and then it's got a memory or a file

208
00:17:18,120 --> 00:17:19,120
network.

209
00:17:19,120 --> 00:17:24,120
People store stuff in their brains and it's in file cabinets and it's on the network and

210
00:17:24,120 --> 00:17:29,960
it's got an audio response system, people talking on the phone and an audio recognition

211
00:17:29,960 --> 00:17:34,480
system, people listening when phone calls come in and all of that can be a few racks

212
00:17:34,480 --> 00:17:41,080
of electronics and suddenly you move a call center that employs 2,000 people in India

213
00:17:41,080 --> 00:17:44,380
to a few boxes of electronics in North Dakota.

214
00:17:44,380 --> 00:17:48,960
These are the kinds of transmissions that happen when you move things from a physical

215
00:17:48,960 --> 00:17:51,160
world to a virtual world.

216
00:17:51,160 --> 00:17:56,920
I think what freaks people out a lot is this comparison that the job that 2,000 people

217
00:17:56,920 --> 00:18:00,280
are doing, it can be done by a few racks of computation.

218
00:18:00,280 --> 00:18:06,640
They cannot just grasp that, hey, I can be replaced, what I'm offering can be replaced,

219
00:18:06,640 --> 00:18:11,320
but not only that, it's that people are not realizing that we've already started merging

220
00:18:11,320 --> 00:18:15,200
with technology and machines since we started using tools.

221
00:18:15,200 --> 00:18:16,960
They don't see it like that.

222
00:18:16,960 --> 00:18:24,960
Well, look, there are a lot of things that we don't see.

223
00:18:24,960 --> 00:18:34,040
For example, there's a lot of socially useful work that has zero monetary value in our society.

224
00:18:34,040 --> 00:18:39,040
I look at it and if you are raising children and you want to go to the office, you drop

225
00:18:39,040 --> 00:18:43,880
them off with childcare and pay somebody to take care of them.

226
00:18:43,880 --> 00:18:48,600
Did we ever think about paying people to stay at home and raise their children?

227
00:18:48,600 --> 00:18:56,440
Well, we'll give you money to subsidize childcare, but if you have somebody else do it, we won't

228
00:18:56,440 --> 00:19:00,000
give you money to raise your own kids.

229
00:19:00,000 --> 00:19:04,240
We're going to have to be willing to think about adopting different perspectives.

230
00:19:04,240 --> 00:19:09,000
I'm not saying that's the right solution to the problem, but we're going to have to think

231
00:19:09,000 --> 00:19:10,880
differently in the future.

232
00:19:10,880 --> 00:19:15,160
Yeah, absolutely, because it's very obvious that any technology works as well as a society

233
00:19:15,160 --> 00:19:16,880
that adopts it.

234
00:19:16,880 --> 00:19:20,480
One of the reasons that I'm bringing these up, because I think, for example, Google Glass

235
00:19:20,480 --> 00:19:26,080
was a very good example that, and we've talked about this before, that it was not even available

236
00:19:26,080 --> 00:19:27,080
to be bought.

237
00:19:27,080 --> 00:19:31,840
It had to be given to certain people creating this elite group of individuals in San Francisco

238
00:19:31,840 --> 00:19:37,400
and area and in many cases they were getting attacked by people who were not wearing those

239
00:19:37,400 --> 00:19:38,400
glasses.

240
00:19:38,400 --> 00:19:44,160
Yeah, well, I mean, I don't know how to respond to that.

241
00:19:44,160 --> 00:19:49,600
Well one of the responses that I got from the chairman of transhumanist party here in

242
00:19:49,600 --> 00:19:53,800
the US was that people felt they're being left out, that this is something that they

243
00:19:53,800 --> 00:19:54,800
can't even buy.

244
00:19:54,800 --> 00:19:59,680
It has to be given to them by a company and creating this elite group of people was not

245
00:19:59,680 --> 00:20:02,560
something that was received well by the community.

246
00:20:02,560 --> 00:20:19,120
Well I think we have a problem with elitism in the country today and you know, when I

247
00:20:19,120 --> 00:20:29,120
came to Silicon Valley, which was 1959, it was really egalitarian and we valued people

248
00:20:29,120 --> 00:20:38,080
by what they contributed and when I say that, I mean, I think we valued policemen and school

249
00:20:38,080 --> 00:20:52,360
teachers and everybody and it was my experience that the managements of those companies were

250
00:20:52,360 --> 00:20:54,080
really very egalitarian.

251
00:20:54,080 --> 00:21:01,680
I mean, I worked for Hewlett Packard and Dave Packard used to cook the hamburgers at the

252
00:21:01,680 --> 00:21:11,040
company picnic and he was out in the community and he was on the factory floor and Dave Packard

253
00:21:11,040 --> 00:21:20,000
valued people based on their values and what they contributed and not on their net worth.

254
00:21:20,000 --> 00:21:25,920
Yeah, we have to deal with it, absolutely.

255
00:21:25,920 --> 00:21:33,240
Let's talk about use autonomous vehicles as an example because the information that is

256
00:21:33,240 --> 00:21:41,280
being harvested in order to make an autonomous system functional, one of the concerns is

257
00:21:41,280 --> 00:21:46,160
that that set of data will be owned by a centralized structure of authority, whether it's going

258
00:21:46,160 --> 00:21:52,120
to be Google or whether it's Microsoft or any other company and this would just continue

259
00:21:52,120 --> 00:21:57,520
this trend of getting users data without paying them any compensation or the users having

260
00:21:57,520 --> 00:22:00,000
any kind of a control over their data.

261
00:22:00,000 --> 00:22:03,400
How are we going to deal with moving forward?

262
00:22:03,400 --> 00:22:13,600
Well, I fundamentally believe that there is no freedom if you don't have privacy and that

263
00:22:13,600 --> 00:22:21,520
is an issue that as far as I'm concerned, maybe we'll develop technologies like blockchain

264
00:22:21,520 --> 00:22:28,120
that will enable us to do this but I think it's a government issue and I believe that

265
00:22:28,120 --> 00:22:35,480
I should own my own information and if you look at it, let me give you the example.

266
00:22:35,480 --> 00:22:40,640
My capital equipment is my car and my computer and my cell phone.

267
00:22:40,640 --> 00:22:50,440
I am using my capital equipment and the people who are appropriating my data are using the

268
00:22:50,440 --> 00:22:53,880
product that I produced on my capital equipment.

269
00:22:53,880 --> 00:22:59,480
If I was running a factory using my capital equipment, putting product on the shipping

270
00:22:59,480 --> 00:23:06,840
dock and you were taking it off the shipping dock, you'd be in jail and because this stuff

271
00:23:06,840 --> 00:23:15,800
is intangible, we don't deal with it the same way that we do with tangible things but the

272
00:23:15,800 --> 00:23:23,220
world of the future, intangible things have in many cases more valuable than tangible

273
00:23:23,220 --> 00:23:33,000
ones but the law hasn't caught up with it.

274
00:23:33,000 --> 00:23:47,520
If I write a song, people can take it from me and not compensate me now and these new

275
00:23:47,520 --> 00:23:53,320
technologies make a lot of things possible that were not possible in the past and they

276
00:23:53,320 --> 00:24:01,760
make cybercrime possible but they make new kinds of theft possible as well and one of

277
00:24:01,760 --> 00:24:06,880
the things that you're stealing from me is my privacy.

278
00:24:06,880 --> 00:24:10,480
Whenever we talk about universal basic income because that seems to be like an inevitable

279
00:24:10,480 --> 00:24:14,800
kind of step for people who will be losing their job, my opinion is that why can't we

280
00:24:14,800 --> 00:24:22,680
get paid for the data that we produce and share with these companies based on the contribution?

281
00:24:22,680 --> 00:24:30,240
One of the things that we suggested in the book was that we create information fiduciaries

282
00:24:30,240 --> 00:24:37,480
where all of that data would be resident with the information fiduciary kind of like a safety

283
00:24:37,480 --> 00:24:44,120
deposit box and then that as the owner of this, if I wanted free stuff from Google,

284
00:24:44,120 --> 00:24:51,000
I could say you can have this kind of data or if I wanted to buy automobile insurance,

285
00:24:51,000 --> 00:24:56,720
I could say here is my automobile, here's my driver's record, you can have access to

286
00:24:56,720 --> 00:25:04,360
it or other things but I would put me in control of my data and who gets to use it and that

287
00:25:04,360 --> 00:25:06,720
way I could monetize it.

288
00:25:06,720 --> 00:25:11,640
That would probably, I'm guessing, generate a few thousand dollars a year in income for

289
00:25:11,640 --> 00:25:13,680
individuals as well.

290
00:25:13,680 --> 00:25:15,680
And it also be decentralized, right?

291
00:25:15,680 --> 00:25:18,960
Because you're not going to be dependent on, for example, only Google because you can't

292
00:25:18,960 --> 00:25:23,180
decide to share a data with Amazon and then Facebook and what and what and you get money

293
00:25:23,180 --> 00:25:25,440
from all those decentralized sources.

294
00:25:25,440 --> 00:25:29,440
Yeah, and there are other solutions to this.

295
00:25:29,440 --> 00:25:41,520
I've been reading about blockchain and they have a different solution to the problem but

296
00:25:41,520 --> 00:25:50,800
it's obvious that we should do something and when I say one of the things I didn't like

297
00:25:50,800 --> 00:25:56,560
about writing the book is that the publisher said, well, you've got to give us a few solutions

298
00:25:56,560 --> 00:26:04,560
and I said, well, all right, I'll propose something but what I want people to understand

299
00:26:04,560 --> 00:26:14,240
is that there are many solutions to the problem and so I've defined one but there could be

300
00:26:14,240 --> 00:26:21,960
numerous others in case I said an information fiduciary, if you talk to Dan Tapscott, he

301
00:26:21,960 --> 00:26:29,640
would say, hey, blockchain is the way to do it and I don't care how we arrive at the solution,

302
00:26:29,640 --> 00:26:33,680
I just want to see people owning their own information.

303
00:26:33,680 --> 00:26:35,560
But you're optimistic about blockchain?

304
00:26:35,560 --> 00:26:40,920
Well, I think blockchain is a phenomenal technology.

305
00:26:40,920 --> 00:27:01,240
The question is how will we get it embedded in society and it may or may not happen.

306
00:27:01,240 --> 00:27:02,800
I don't take it as a given.

307
00:27:02,800 --> 00:27:08,760
I mean, I think there are going to be lots of important applications but as to whether

308
00:27:08,760 --> 00:27:14,680
it will be the universal solution to a lot of these problems, I'm not equipped to say.

309
00:27:14,680 --> 00:27:19,160
Yeah, perhaps the best known application on blockchain are cryptocurrencies at this point.

310
00:27:19,160 --> 00:27:23,800
Do you see cryptocurrencies as automating a financial system?

311
00:27:23,800 --> 00:27:33,720
I see cryptocurrencies as extremely important and not necessarily Bitcoin but a cryptocurrency

312
00:27:33,720 --> 00:27:47,360
and the reason why is money is to be a unit of account, a store of value and a medium

313
00:27:47,360 --> 00:27:49,520
of exchange.

314
00:27:49,520 --> 00:27:55,220
The worst people I know to create stores of value are governments because what they want

315
00:27:55,220 --> 00:28:01,200
is inflation so that they can pay off their bills.

316
00:28:01,200 --> 00:28:08,560
It turns out that as a medium of exchange, the fees on transferring money are absolutely

317
00:28:08,560 --> 00:28:15,280
horrendous like we have about $3 trillion transferred through credit cards and debit

318
00:28:15,280 --> 00:28:21,800
cards and we pay about $200 billion in fees on those transfers and interest.

319
00:28:21,800 --> 00:28:24,620
That's about 6%.

320
00:28:24,620 --> 00:28:29,640
Why should I spend six cents on every dollar to transfer a dollar?

321
00:28:29,640 --> 00:28:38,640
That makes no sense at all so it looks like there is a better alternative and I can say

322
00:28:38,640 --> 00:28:44,840
I think with a high degree of confidence that cryptocurrencies are going to challenge the

323
00:28:44,840 --> 00:28:52,640
currencies of many developing countries but I also have a feeling that a good cryptocurrency

324
00:28:52,640 --> 00:28:59,360
is going to challenge the euro and the dollar over a period of time and if that happens,

325
00:28:59,360 --> 00:29:01,960
think about the implications of this.

326
00:29:01,960 --> 00:29:12,000
If you talk about form change, one of the things that has enabled our government to

327
00:29:12,000 --> 00:29:17,560
survive and solve problems is that they have had control over their currency.

328
00:29:17,560 --> 00:29:20,160
They've had the ability to print money.

329
00:29:20,160 --> 00:29:24,320
What happens when you take away the ability to print money from the government?

330
00:29:24,320 --> 00:29:30,520
I mean it's a big change and I think that that could happen.

331
00:29:30,520 --> 00:29:35,560
I don't know if it will happen but I think it's a very real possibility.

332
00:29:35,560 --> 00:29:36,560
Oh absolutely.

333
00:29:36,560 --> 00:29:42,000
We're already seeing that government is being challenged in different ways whether it's

334
00:29:42,000 --> 00:29:47,840
Facebook's Libra that they're going through all of these regulatory things.

335
00:29:47,840 --> 00:29:52,320
They haven't even launched it yet or if it's John McAfee who I had on the show and he's

336
00:29:52,320 --> 00:29:57,360
saying I haven't paid taxes for 10 years and anybody who wants to not pay taxes use

337
00:29:57,360 --> 00:30:01,280
cryptocurrencies that are anonymous like Monera and blah, blah, blah, blah.

338
00:30:01,280 --> 00:30:08,720
Governments are being challenged from many different directions already.

339
00:30:08,720 --> 00:30:09,720
Do you have anything to add?

340
00:30:09,720 --> 00:30:15,320
I want to talk about ethics a little because ethics and morality seems to be a very important

341
00:30:15,320 --> 00:30:20,800
yet very blurry aspect of all of this autonomous kind of a world.

342
00:30:20,800 --> 00:30:35,960
Well see we are living in a world where trust has been undermined and I'm going to give

343
00:30:35,960 --> 00:30:46,080
you a real old world example from about 1950 or 60.

344
00:30:46,080 --> 00:30:52,600
When Sonia and I got married in 1965, a lot of my parents and their friends used to go

345
00:30:52,600 --> 00:31:03,000
to Europe and they shopped at a store that sold fine china named Goods and we got china

346
00:31:03,000 --> 00:31:06,940
from people who were at Goods.

347
00:31:06,940 --> 00:31:13,440
One time Sonia and I went to Europe and we visited Goods and it was a great store and

348
00:31:13,440 --> 00:31:20,340
at some point a piece of china got broken and Sonia ordered a piece of china from Goods

349
00:31:20,340 --> 00:31:28,160
which was in England and it arrived and it was not right.

350
00:31:28,160 --> 00:31:33,880
She informed Goods of this and they said we will ship you a replacement and Goods said

351
00:31:33,880 --> 00:31:39,600
to her and she said don't you want to wait until I have returned the item and they said

352
00:31:39,600 --> 00:31:45,560
no you are a customer of Goods.

353
00:31:45,560 --> 00:31:55,500
We today are living in a world where that level of trust does not exist and we're dependent

354
00:31:55,500 --> 00:32:02,080
upon reviews and this and that and the other thing but one of the things that makes the

355
00:32:02,080 --> 00:32:09,120
economy work and one of the things that make relationships between people work is trust.

356
00:32:09,120 --> 00:32:16,280
And currently things like anonymity on the internet and whether it's fake news or fake

357
00:32:16,280 --> 00:32:22,360
truths or things like this but we have to find ways of restoring trust in the economy

358
00:32:22,360 --> 00:32:24,560
or we're going to have real big problems.

359
00:32:24,560 --> 00:32:32,480
Yeah and with respect to for example autonomous vehicles that famous dilemma I guess that

360
00:32:32,480 --> 00:32:37,600
you have an autonomous vehicle that is on that branch of a road and one there's one

361
00:32:37,600 --> 00:32:41,360
baby and the other one there are like five people which one would you kill.

362
00:32:41,360 --> 00:32:47,160
So whatever kind of decision that is going to be made we go back to this lack of trust

363
00:32:47,160 --> 00:32:50,880
that a lot of people will be like well no the other option would have been right.

364
00:32:50,880 --> 00:32:54,880
It seems like a very big kind of a challenge to solve.

365
00:32:54,880 --> 00:33:04,120
Yeah it is because I mean people make mistakes.

366
00:33:04,120 --> 00:33:09,320
My wife and I were talking about autonomous vehicles and autonomous trucks and I said

367
00:33:09,320 --> 00:33:19,160
there's no doubt in my mind that today even at the state of the technology they are safer

368
00:33:19,160 --> 00:33:28,760
than human drivers and I if I look at that objectively I believe it.

369
00:33:28,760 --> 00:33:38,160
If I look at it emotionally I don't and I say you know I know that I'm not going to

370
00:33:38,160 --> 00:33:47,760
make the same mistake that an autonomous vehicle would but I look at my driving today and I'm

371
00:33:47,760 --> 00:33:54,840
not nearly as sharp as I used to be and I'd be much better off with an autonomous chauffeur

372
00:33:54,840 --> 00:34:02,280
even with the technology at this stage.

373
00:34:02,280 --> 00:34:10,960
But it's hard to trust algorithms because algorithms do absolutely stupid things at

374
00:34:10,960 --> 00:34:23,040
times and you know an algorithm is a rigid rule frequently and you know I said I would

375
00:34:23,040 --> 00:34:31,560
love to write rules that say use your best judgment and algorithms don't work that way.

376
00:34:31,560 --> 00:34:35,440
Yeah there's also alignment issue right like best judgment for humans are not necessarily

377
00:34:35,440 --> 00:34:43,680
going to be the best judgment for algorithms just experiencing different realities.

378
00:34:43,680 --> 00:34:49,840
Considering the exponential growth of technology on the basis of Moore's law that was determined

379
00:34:49,840 --> 00:34:54,480
on the basis of silicon and what I'm hearing is that we are reaching the limits of silicon

380
00:34:54,480 --> 00:34:57,360
and we have to enter a new mode of computation.

381
00:34:57,360 --> 00:35:04,640
Quantum computing is one of the candidates but I'm getting from a lot of experts in the

382
00:35:04,640 --> 00:35:10,560
field that they're not that optimistic about the emergence of a functional quantum computing

383
00:35:10,560 --> 00:35:11,560
anytime soon.

384
00:35:11,560 --> 00:35:15,740
What do you think about quantum computing in specific and the next mode of computation

385
00:35:15,740 --> 00:35:17,700
in general after silicon?

386
00:35:17,700 --> 00:35:30,400
I am not an authority on any of this but what I observe is that I am not sure our limitations

387
00:35:30,400 --> 00:35:36,640
are associated anymore with the power of computation.

388
00:35:36,640 --> 00:35:51,120
We are limited not by the power of computation but by our ability to adapt to the applications

389
00:35:51,120 --> 00:35:54,880
as they are currently being structured.

390
00:35:54,880 --> 00:36:06,920
We do not need more technological progress to make vast improvements in our society right

391
00:36:06,920 --> 00:36:12,400
now, more technological improvements in computation.

392
00:36:12,400 --> 00:36:20,240
What would make us better is if we figured out how to apply the improvements we already

393
00:36:20,240 --> 00:36:33,960
have more humanely and what I've told people is that I consider the recent crop of engineers

394
00:36:33,960 --> 00:36:37,920
to be not very good and they look at me and they say, well, they're all technological

395
00:36:37,920 --> 00:36:46,360
geniuses and I look at them and I say, hey, I'm using this stuff and is it making my life

396
00:36:46,360 --> 00:36:49,680
simpler and more efficient?

397
00:36:49,680 --> 00:37:00,160
The answer is that it does not do 100% of what is absolutely critical to me 100% of

398
00:37:00,160 --> 00:37:02,440
the time reliably.

399
00:37:02,440 --> 00:37:08,760
What happens is that I'm constantly getting bogged down in fringe cases where something

400
00:37:08,760 --> 00:37:17,180
surprising happens and I end up spending an incredible amount of time on a fringe case

401
00:37:17,180 --> 00:37:21,120
that I didn't want to have anything to do with anyhow, but I'm trying to get rid of

402
00:37:21,120 --> 00:37:22,120
it.

403
00:37:22,120 --> 00:37:26,520
I'm looking at this and what do I do with my phone?

404
00:37:26,520 --> 00:37:30,640
I just went through it and spent an incredible amount of time turning off alerts.

405
00:37:30,640 --> 00:37:34,640
Is that an iPhone or Android?

406
00:37:34,640 --> 00:37:41,320
I don't want to say.

407
00:37:41,320 --> 00:37:54,460
To me, that's poor engineering and it's not doing for me what I want it to do or I find

408
00:37:54,460 --> 00:38:01,240
that I've got this wonderful electronic calendar system, but for some reason, the software

409
00:38:01,240 --> 00:38:10,480
is not reliable enough for me to use my phone and be 100% certain that it updates all the

410
00:38:10,480 --> 00:38:13,120
other devices.

411
00:38:13,120 --> 00:38:18,040
When I'm out, I don't enter a new event on my phone.

412
00:38:18,040 --> 00:38:24,640
I check for it on my phone, write it down on a piece of paper and come home or make

413
00:38:24,640 --> 00:38:32,480
myself a note on my phone and come home and enter it on my computer so that I show her

414
00:38:32,480 --> 00:38:36,840
that it gets updated throughout the system.

415
00:38:36,840 --> 00:38:45,680
These things are really wonderful conveniences, but if they don't work 100% reliably, 100%

416
00:38:45,680 --> 00:38:48,160
of the time.

417
00:38:48,160 --> 00:38:50,360
Don't upgrade me.

418
00:38:50,360 --> 00:38:55,560
Figure out how to make the stuff you've sold me work reliably.

419
00:38:55,560 --> 00:39:02,320
We just had our home audio system replaced.

420
00:39:02,320 --> 00:39:09,440
The reason was that we had a sophisticated system and I told the guy I wanted the least

421
00:39:09,440 --> 00:39:17,040
sophisticated system and he swore he sold it to me.

422
00:39:17,040 --> 00:39:26,920
It turns out that I ended up with a sophisticated system that worked most of the time or I have

423
00:39:26,920 --> 00:39:37,040
a thermostat that was online and it got software upgraded and my wife couldn't turn on the

424
00:39:37,040 --> 00:39:40,920
furnace on a cold day.

425
00:39:40,920 --> 00:39:47,880
I cut it off from the internet and I'm now probably 30 versions of software behind, but

426
00:39:47,880 --> 00:39:53,000
the furnace always turns on.

427
00:39:53,000 --> 00:39:54,800
These are things that are really important.

428
00:39:54,800 --> 00:39:57,520
You're so right.

429
00:39:57,520 --> 00:40:00,400
How the masses are going to adopt something like that.

430
00:40:00,400 --> 00:40:04,720
Bill's book is called The Autonomous Revolution Reclaiming the Future We've Sold to Machines.

431
00:40:04,720 --> 00:40:07,280
I have two more questions for you.

432
00:40:07,280 --> 00:40:17,440
What do you think about this 5G and the tech rivalry between the United States and China?

433
00:40:17,440 --> 00:40:30,520
I have for a long time thought that we managed our situation with China not well.

434
00:40:30,520 --> 00:40:38,120
This goes back to trade when I was with Intel.

435
00:40:38,120 --> 00:40:50,840
I believe the Chinese are extremely competent and they do a very good job, but their motivations

436
00:40:50,840 --> 00:40:56,080
are frequently not aligned with our best interests.

437
00:40:56,080 --> 00:41:05,280
In that case, we better find a way whether it's with 5G or with the production of drugs

438
00:41:05,280 --> 00:41:08,620
to be not as dependent upon them.

439
00:41:08,620 --> 00:41:15,820
This doesn't say don't do business with them, but we are overly dependent upon them and

440
00:41:15,820 --> 00:41:25,960
we are not demanding enough that they play by the same rules that we play by.

441
00:41:25,960 --> 00:41:29,800
That doesn't mean that they shouldn't have different rules.

442
00:41:29,800 --> 00:41:37,160
That means that when they interface with us, then we've got to get mutual agreement on

443
00:41:37,160 --> 00:41:43,000
what a fair set of rules are because different societies are going to have different rules.

444
00:41:43,000 --> 00:41:49,600
We can't tell China what rules they should have, but we can say, hey, in the places where

445
00:41:49,600 --> 00:41:56,840
we interact, these are the sets of rules you're going to have to abide by and you can't steal

446
00:41:56,840 --> 00:42:02,040
intellectual property, you can't do this, you can't do that, and if you want to do that,

447
00:42:02,040 --> 00:42:04,240
we don't want to do business with you.

448
00:42:04,240 --> 00:42:05,240
Absolutely.

449
00:42:05,240 --> 00:42:07,560
Thank you so much again for your time.

450
00:42:07,560 --> 00:42:09,000
It's been such a pleasure and honor.

451
00:42:09,000 --> 00:42:13,960
Let me ask you the last question I ask all my guests, that if you come across an intelligent

452
00:42:13,960 --> 00:42:18,660
alien from a different civilization, what would you say is the worst thing humanity

453
00:42:18,660 --> 00:42:22,160
has done and what would you say is our greatest achievement?

454
00:42:22,160 --> 00:42:28,760
God, the worst thing?

455
00:42:28,760 --> 00:42:50,560
Well, I was going to say things like genocides, and then I thought, hey, we have also horribly

456
00:42:50,560 --> 00:43:02,280
abused our environment, and we aren't feeling the effects of that, but we will, and I'm

457
00:43:02,280 --> 00:43:04,680
sorry, what was the other thing you wanted to know?

458
00:43:04,680 --> 00:43:07,160
Our greatest achievement.

459
00:43:07,160 --> 00:43:19,920
Well, our greatest achievements are that we've found, I think, a way of adapting to the physical

460
00:43:19,920 --> 00:43:20,920
reality.

461
00:43:20,920 --> 00:43:21,920
Thank you.

462
00:43:21,920 --> 00:43:49,920
Thank you.

463
00:43:51,920 --> 00:43:52,920
Thank you.

