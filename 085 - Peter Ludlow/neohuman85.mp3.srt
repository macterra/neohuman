1
00:00:00,000 --> 00:00:05,600
If my mind is in part a function of what's going on in the external world already, and

2
00:00:05,600 --> 00:00:10,000
if all these amazing things are happening in the internet and artificial intelligence

3
00:00:10,000 --> 00:00:14,520
and big data, and all of those can be construed as part of my mind.

4
00:00:14,520 --> 00:00:17,920
But furthermore, there are these methods of communication in which I can communicate and

5
00:00:17,920 --> 00:00:21,320
exchange ideas with people like you, right?

6
00:00:21,320 --> 00:00:29,840
I am like very rapidly achieving like higher and higher, I don't know, planes of consciousness

7
00:00:29,840 --> 00:00:36,400
is the right word, but certainly I'm learning a lot more and thinking in ways that I couldn't

8
00:00:36,400 --> 00:00:39,560
imagine thinking earlier, right?

9
00:00:39,560 --> 00:00:46,680
So to me that there are these avenues of sort of naturally growing that are much more interesting

10
00:00:46,680 --> 00:00:55,560
to me than to try and find some sort of imaginary wall to our intelligence or some sort of ceiling

11
00:00:55,560 --> 00:00:59,240
to our intelligence is a better way to put it, which if we could break through that then

12
00:00:59,240 --> 00:01:09,160
we can see the world correctly.

13
00:01:09,160 --> 00:01:15,000
Peter Ladlow, welcome to the 85th episode of New Human Podcast.

14
00:01:15,000 --> 00:01:16,000
Thank you.

15
00:01:16,000 --> 00:01:17,000
I've been looking forward to this.

16
00:01:17,000 --> 00:01:18,000
Yeah, absolutely.

17
00:01:18,000 --> 00:01:19,000
Likewise.

18
00:01:19,000 --> 00:01:20,000
And from Mexico.

19
00:01:20,000 --> 00:01:22,360
I am in Mexico, yes.

20
00:01:22,360 --> 00:01:26,200
How are you doing with the pandemic in that part of the world?

21
00:01:26,200 --> 00:01:30,600
You know, they seem to have a better handle on it than we do in the United States.

22
00:01:30,600 --> 00:01:37,600
I mean, like things are really tight here, lots of precautions, people are following

23
00:01:37,600 --> 00:01:38,600
the rules.

24
00:01:38,600 --> 00:01:42,560
So I actually feel safer here than in the United States.

25
00:01:42,560 --> 00:01:43,560
Interesting.

26
00:01:43,560 --> 00:01:49,880
I want to talk a little about, well, before that, let's get to the work you've done,

27
00:01:49,880 --> 00:01:53,000
the lives you've lived and what do you mainly focus on nowadays?

28
00:01:53,000 --> 00:01:58,200
I ask everybody just to establish some kind of a perspective and the context so our audience

29
00:01:58,200 --> 00:02:02,160
know where your thoughts are coming from.

30
00:02:02,160 --> 00:02:05,800
Well, okay, let's start here.

31
00:02:05,800 --> 00:02:07,500
Let's start with graduate school.

32
00:02:07,500 --> 00:02:16,720
So in 1979, I moved to New York to go to grad school in philosophy at Columbia University.

33
00:02:16,720 --> 00:02:24,760
And my thought was I was going to do philosophy of language and I showed up and people were

34
00:02:24,760 --> 00:02:30,920
doing strange things like the philosophers were doing linguistics, by linguistics I mean

35
00:02:30,920 --> 00:02:35,720
linguistics in the sense of Chomsky's program and generative linguistics.

36
00:02:35,720 --> 00:02:42,480
And the linguists in the sort of vicinity were doing philosophy and I thought that was

37
00:02:42,480 --> 00:02:44,080
sort of weird.

38
00:02:44,080 --> 00:02:49,520
And I was a bit distracted from the program and I spent a lot of time downtown because

39
00:02:49,520 --> 00:02:53,800
a very distracting time to be in New York in 1979 when I arrived.

40
00:02:53,800 --> 00:03:01,280
And there was a lot going on and had been going on since like 1977 when punk came and

41
00:03:01,280 --> 00:03:04,400
really the birth of rap was in that period.

42
00:03:04,400 --> 00:03:12,640
But there was a movement that was known then or at least it's known now as No Wave, as

43
00:03:12,640 --> 00:03:15,960
in N-O, No Wave.

44
00:03:15,960 --> 00:03:20,280
And what was going on is that way down in the Lower East Side in Alphabet City, which

45
00:03:20,280 --> 00:03:26,320
at the time was completely bombed out, there were these kids I guess.

46
00:03:26,320 --> 00:03:30,560
And there were a bunch of musicians that decided they wanted to become filmmakers and there

47
00:03:30,560 --> 00:03:35,880
were a bunch of filmmakers that decided they wanted to become musicians.

48
00:03:35,880 --> 00:03:40,120
And so everyone was just doing things they didn't know how to do and the result was fantastic.

49
00:03:40,120 --> 00:03:46,240
So the best example of this is that there was a band that I really liked called Del

50
00:03:46,240 --> 00:03:53,040
Byzantines, Byzantines spelled T-E-N-Z.

51
00:03:53,040 --> 00:03:59,800
And I used to go and watch them a lot and the lead singer slash keyboardist was a guy

52
00:03:59,800 --> 00:04:06,880
by the name of Jim Jarmusch, who we now know as a very famous film director.

53
00:04:06,880 --> 00:04:11,320
I remember when his first movie came out, it was just a little low budget thing called

54
00:04:11,320 --> 00:04:12,720
Permanent Vacation.

55
00:04:12,720 --> 00:04:18,040
This was before his big breakout movie, which was Stranger Than Paradise, I guess.

56
00:04:18,040 --> 00:04:19,400
And it just blew my mind.

57
00:04:19,400 --> 00:04:24,800
I go, wow, that's like Jim Jarmusch, like my hero from Del Byzantines.

58
00:04:24,800 --> 00:04:29,920
And then it was sort of that where I got it that there were these people, everyone was

59
00:04:29,920 --> 00:04:36,560
doing, we have these musicians trying to make movies and filmmakers trying to start bands

60
00:04:36,560 --> 00:04:41,280
and it's not like they were working in a vacuum, they were still surrounded by musicians and

61
00:04:41,280 --> 00:04:43,560
filmmakers and all that stuff.

62
00:04:43,560 --> 00:04:44,560
And so then I got it.

63
00:04:44,560 --> 00:04:49,760
I just said, you know what, I really want to take a deeper dive into linguistics.

64
00:04:49,760 --> 00:04:56,680
And so I applied to go to MIT as a visiting scholar in the linguistics program and I really

65
00:04:56,680 --> 00:04:58,600
got into linguistics when I was there.

66
00:04:58,600 --> 00:05:04,060
I mean, I asked them to put me in housing with linguists or in an office with linguistics

67
00:05:04,060 --> 00:05:05,400
grad students.

68
00:05:05,400 --> 00:05:09,920
And at one point, I went and talked to the chair of the linguistics department.

69
00:05:09,920 --> 00:05:12,000
I said, I am really getting into linguistics.

70
00:05:12,000 --> 00:05:16,920
I'm thinking about like maybe transferring or like after I get my philosophy PhD, maybe

71
00:05:16,920 --> 00:05:19,320
I'll try and get a linguistics PhD.

72
00:05:19,320 --> 00:05:23,360
And he looks at me and goes, well, you're getting a PhD anyway.

73
00:05:23,360 --> 00:05:24,360
What do you care?

74
00:05:24,360 --> 00:05:26,720
It doesn't matter what your PhD says.

75
00:05:26,720 --> 00:05:29,040
If you want to learn linguistics, learn linguistics.

76
00:05:29,040 --> 00:05:30,440
That's it.

77
00:05:30,440 --> 00:05:31,440
And no one cares.

78
00:05:31,440 --> 00:05:32,800
I mean, linguists don't care.

79
00:05:32,800 --> 00:05:35,920
No one cares if your PhD is in philosophy or whatever.

80
00:05:35,920 --> 00:05:38,240
You just do linguistics.

81
00:05:38,240 --> 00:05:45,640
And that came as a kind of a revelation to me and it fueled subsequent work, which was

82
00:05:45,640 --> 00:05:51,280
often work that was not in my official area of expertise.

83
00:05:51,280 --> 00:05:59,200
So my first job out of grad school was in an artificial intelligence group at Honeywell.

84
00:05:59,200 --> 00:06:01,520
What year was this?

85
00:06:01,520 --> 00:06:08,280
So let's see, that would have been 1985, 86.

86
00:06:08,280 --> 00:06:11,640
It was the intelligent interface systems group at Honeywell.

87
00:06:11,640 --> 00:06:15,840
And those were the very early days.

88
00:06:15,840 --> 00:06:20,120
It was really the second iteration of artificial intelligence.

89
00:06:20,120 --> 00:06:23,720
And that was sort of interesting.

90
00:06:23,720 --> 00:06:27,920
And then I was offered a job in philosophy.

91
00:06:27,920 --> 00:06:28,920
Many things happened.

92
00:06:28,920 --> 00:06:29,920
I was offered a job in philosophy.

93
00:06:29,920 --> 00:06:32,920
I took it.

94
00:06:32,920 --> 00:06:35,080
But I retained interest in other things.

95
00:06:35,080 --> 00:06:42,240
And one of the very early things I was interested in was what was then becoming the internet.

96
00:06:42,240 --> 00:06:44,800
I mean, it was very early stages.

97
00:06:44,800 --> 00:06:50,320
And I edited a collection for MIT Press called High Noon on the Electronic Frontier.

98
00:06:50,320 --> 00:06:56,280
And that covered a number of conceptual issues that were already coming into place.

99
00:06:56,280 --> 00:07:00,840
One set of those issues had to do with cryptoanarchy, which I guess is something you want to talk

100
00:07:00,840 --> 00:07:03,560
about a little bit.

101
00:07:03,560 --> 00:07:10,280
And then a few years after that, right around 1999, I assembled the second collection, which

102
00:07:10,280 --> 00:07:14,960
was titled Cryptoanarchy Cyber States and Pyro Utopias.

103
00:07:14,960 --> 00:07:20,400
And while the first collection was very successful, the second one was not, or at least it was

104
00:07:20,400 --> 00:07:28,360
not successful until now, like 20 years later.

105
00:07:28,360 --> 00:07:29,360
Way ahead of its time.

106
00:07:29,360 --> 00:07:34,240
Well, I don't know if it was that or what it was.

107
00:07:34,240 --> 00:07:37,680
Yeah, I don't know what that was.

108
00:07:37,680 --> 00:07:47,240
So it's very interesting that after all these years, that project is getting some uptake.

109
00:07:47,240 --> 00:07:53,240
And I think a lot of it is fueled by what we see going on with cryptocurrencies and

110
00:07:53,240 --> 00:07:56,520
decentralized finance and things like that.

111
00:07:56,520 --> 00:07:57,520
And that's been it.

112
00:07:57,520 --> 00:08:01,200
I mean, right now, I'm involved in two different projects.

113
00:08:01,200 --> 00:08:05,000
One of them is kind of straight philosophical.

114
00:08:05,000 --> 00:08:11,720
That's a project where I've been looking at some work that the medieval logicians were

115
00:08:11,720 --> 00:08:12,720
doing.

116
00:08:12,720 --> 00:08:17,000
And by medieval logicians, I mean people you've probably heard of, like William of Ockham.

117
00:08:17,000 --> 00:08:23,600
He of the Razor and Abelard, he of the romance with Heloise.

118
00:08:23,600 --> 00:08:29,840
And they had this really interesting project, which was, they wanted to take, well, first

119
00:08:29,840 --> 00:08:36,400
of all, I mean, preface this by saying, you know, there's this myth about medieval logic,

120
00:08:36,400 --> 00:08:41,680
which is that they weren't doing anything except singing peons to Aristotle or something.

121
00:08:41,680 --> 00:08:46,040
And this is a myth that was perpetuated by people like Kant and Wittgenstein.

122
00:08:46,040 --> 00:08:47,280
But it's a complete myth.

123
00:08:47,280 --> 00:08:52,840
And what these guys were doing is they wanted to take Aristotle's logic, expand the scope

124
00:08:52,840 --> 00:08:56,600
to cover basically any sort of language that you use, and then they were going to take

125
00:08:56,600 --> 00:09:00,240
all of his rules of logic and reduce them down to two basic rules.

126
00:09:00,240 --> 00:09:02,440
I mean, it's like an incredible project.

127
00:09:02,440 --> 00:09:04,560
It's a mind blowing project.

128
00:09:04,560 --> 00:09:09,720
And it ran aground some, I don't know, like 600 years ago because they didn't have the

129
00:09:09,720 --> 00:09:17,160
tools to study language with the kind of fine logical detail that you need to.

130
00:09:17,160 --> 00:09:22,640
But now here we are at the development of lots of work in logic in the 20th century,

131
00:09:22,640 --> 00:09:26,520
and Chomsky showed how you could apply these tools to natural language.

132
00:09:26,520 --> 00:09:31,720
So the idea is that now we have the tools to explore these questions after they've

133
00:09:31,720 --> 00:09:36,240
been dormant for 600 years or so.

134
00:09:36,240 --> 00:09:43,640
And then the other project is I remain very interested in cryptocurrencies, and in particular

135
00:09:43,640 --> 00:09:51,260
a branch of cryptocurrency that's known as decentralized finance or DeFi, where you basically

136
00:09:51,260 --> 00:09:57,880
do all the things that the banking industry and the insurance industry and all those things

137
00:09:57,880 --> 00:10:03,280
that we consider to be part of serious real world finance, the question is whether you

138
00:10:03,280 --> 00:10:12,480
can replicate all of those functions in the cryptocurrency space.

139
00:10:12,480 --> 00:10:19,240
It seems like the beginning of Bitcoin, which now is how many years ago, like 12 years ago,

140
00:10:19,240 --> 00:10:25,960
it kind of introduced some kind of a practicality to the philosophical perspective towards decentralization

141
00:10:25,960 --> 00:10:28,400
and crypto anarchy.

142
00:10:28,400 --> 00:10:34,680
And it kind of blew up because of that, and the context kind of evolved with it, right?

143
00:10:34,680 --> 00:10:35,760
I think that's right.

144
00:10:35,760 --> 00:10:41,080
And I think that goes some ways to explaining why the initial iteration of my book on crypto

145
00:10:41,080 --> 00:10:46,400
anarchy didn't go anywhere, which was there was talk about digital cash in it.

146
00:10:46,400 --> 00:10:50,160
And there were versions of digital cash available.

147
00:10:50,160 --> 00:10:57,160
But they had not solved the big puzzle, which is called the double spending problem.

148
00:10:57,160 --> 00:10:59,120
And it took Satoshi to do that.

149
00:10:59,120 --> 00:11:03,580
And it required a technological breakthrough for it to really be feasible.

150
00:11:03,580 --> 00:11:06,080
And we have that now.

151
00:11:06,080 --> 00:11:10,760
Sorry, I just mute my microphone because of these dogs.

152
00:11:10,760 --> 00:11:14,600
Sometimes it's just I don't even hear them.

153
00:11:14,600 --> 00:11:16,780
Okay, good.

154
00:11:16,780 --> 00:11:23,760
How has the definition of linguistics changed from the time that you've begun up to this

155
00:11:23,760 --> 00:11:24,760
day?

156
00:11:24,760 --> 00:11:30,880
It seems to me that language is becoming such a frontier, because different political sides,

157
00:11:30,880 --> 00:11:35,320
different philosophical perspectives, they're fighting over how to define certain kind of

158
00:11:35,320 --> 00:11:36,320
terms.

159
00:11:36,320 --> 00:11:40,400
Let's say, for example, racism, racism means something to one side and means something

160
00:11:40,400 --> 00:11:41,400
to other sides.

161
00:11:41,400 --> 00:11:45,600
And they're fighting with each other that, for example, America is a racist country.

162
00:11:45,600 --> 00:11:47,040
No, America is not a racist country.

163
00:11:47,040 --> 00:11:52,760
But they're fighting with two complete different definitions of these terms.

164
00:11:52,760 --> 00:11:53,760
Yeah.

165
00:11:53,760 --> 00:11:55,480
No, that's exactly right.

166
00:11:55,480 --> 00:11:59,560
Actually, I have a book on that that came out a couple years ago.

167
00:11:59,560 --> 00:12:03,400
And it was called Living Words.

168
00:12:03,400 --> 00:12:08,120
And it was, I had some subtitle for it, but now I'm forgetting what the subtitle of my

169
00:12:08,120 --> 00:12:09,120
own book was.

170
00:12:09,120 --> 00:12:12,120
It was called Living Words.

171
00:12:12,120 --> 00:12:15,520
Wow, that's terrible.

172
00:12:15,520 --> 00:12:18,080
You don't even remember the title of your own book.

173
00:12:18,080 --> 00:12:21,120
I can tell you what it says.

174
00:12:21,120 --> 00:12:26,560
So the basic idea is that you don't really learn a language.

175
00:12:26,560 --> 00:12:34,000
It's not this thing that you sort of just take on full stop, but rather what we do is

176
00:12:34,000 --> 00:12:36,220
we build little micro languages together.

177
00:12:36,220 --> 00:12:40,800
So right now, you and I and your listeners are constructing a conversation together.

178
00:12:40,800 --> 00:12:42,200
We're going to introduce terms.

179
00:12:42,200 --> 00:12:45,160
We're going to get to understand each other better.

180
00:12:45,160 --> 00:12:50,400
And the idea is that it's what I called in the book, and presumably this is part of the

181
00:12:50,400 --> 00:12:56,960
subtitle, the dynamic lexicon is what I called it, meaning that word meanings are very malleable.

182
00:12:56,960 --> 00:12:58,200
They're living words.

183
00:12:58,200 --> 00:13:00,880
And so we have to think of them that way.

184
00:13:00,880 --> 00:13:05,720
And then a lot of what we do in a conversation is not, I have something to say, I'm going

185
00:13:05,720 --> 00:13:12,480
to say it, you're going to hear it, but rather we're engaged in this process in which we're

186
00:13:12,480 --> 00:13:17,240
shaping the meanings of the words that we're going to deploy with each other.

187
00:13:17,240 --> 00:13:21,880
So a lot of conversation that takes place is not here's some information, now you have

188
00:13:21,880 --> 00:13:26,440
it, but rather it's sort of us fine tuning the words that we're going to use to carry

189
00:13:26,440 --> 00:13:28,600
on further conversation.

190
00:13:28,600 --> 00:13:33,360
It's almost like lexical grooming that we engage in.

191
00:13:33,360 --> 00:13:39,700
And part of all of this is when we get into these cases where we have to argue about the

192
00:13:39,700 --> 00:13:45,560
meaning of what's really racism and what's really sexism, what is a woman, for example,

193
00:13:45,560 --> 00:13:47,720
does it include trans women?

194
00:13:47,720 --> 00:13:52,960
All of these things are what I called in the book, lexical warfare.

195
00:13:52,960 --> 00:13:56,960
And the idea is, well, what happens, you know, often we're in this position where we are

196
00:13:56,960 --> 00:14:05,080
actually litigating what the meaning of a term is going to be.

197
00:14:05,080 --> 00:14:09,300
And so there's no easy answers to how to do that.

198
00:14:09,300 --> 00:14:14,640
But what I try to do is look for certain norms that govern the right way to do it and the

199
00:14:14,640 --> 00:14:16,320
wrong way to do it.

200
00:14:16,320 --> 00:14:21,600
And hopefully, hopefully we are going to get better or have a better understanding of how

201
00:14:21,600 --> 00:14:26,800
to litigate these word meanings, especially when the cost is extremely high, because basically

202
00:14:26,800 --> 00:14:33,080
every ethical issue we engage in is on some level, a kind of debate about what a word

203
00:14:33,080 --> 00:14:40,120
is supposed to mean, you know, what is a person, you know, what is what is terrorism, what

204
00:14:40,120 --> 00:14:46,800
is what is freedom, you know, all of these things, basically everything you when you

205
00:14:46,800 --> 00:14:51,360
look at it, what you're doing is you're arguing about what the meaning of the word should

206
00:14:51,360 --> 00:14:52,360
be.

207
00:14:52,360 --> 00:14:56,880
Now, that doesn't mean any definition is equally good, because there are rules that sort of

208
00:14:56,880 --> 00:15:03,080
dictate or norms that dictate whether we are adjusting the meanings of those words correctly.

209
00:15:03,080 --> 00:15:08,600
I mean, Bill Clinton himself said that it depends on what is the definition of the word

210
00:15:08,600 --> 00:15:09,600
is.

211
00:15:09,600 --> 00:15:17,200
Yeah, I think he was, I think he was asking what they meant by the tense of that.

212
00:15:17,200 --> 00:15:20,320
But yeah, yeah, I mean, it can sound kind of shady.

213
00:15:20,320 --> 00:15:23,680
I mean, well, that's the thing, once you, you know, once you understand that you're

214
00:15:23,680 --> 00:15:29,000
going to be negotiating the meanings of terms, then you have to then you have to be careful

215
00:15:29,000 --> 00:15:32,120
because people can pull some really shady stuff with you.

216
00:15:32,120 --> 00:15:33,120
And people do it all the time.

217
00:15:33,120 --> 00:15:36,640
I mean, the government is the government for well, I don't mean to pick on a government

218
00:15:36,640 --> 00:15:41,020
or the government, but they're always trying to shift the meaning of a term, you know.

219
00:15:41,020 --> 00:15:48,960
So now, Antifa people are terrorists, you know, you know, it's always it's always going

220
00:15:48,960 --> 00:15:49,960
on.

221
00:15:49,960 --> 00:15:52,640
It's like an ongoing battle and you have to stay on your toes.

222
00:15:52,640 --> 00:15:55,920
This fluidity of words and terms.

223
00:15:55,920 --> 00:15:58,080
This is not a modern phenomenon.

224
00:15:58,080 --> 00:16:01,080
This must have existed throughout history, right?

225
00:16:01,080 --> 00:16:02,640
Yeah, yeah, yeah.

226
00:16:02,640 --> 00:16:08,040
We are where we are in this warfare that you're talking about, maybe because this process

227
00:16:08,040 --> 00:16:13,540
of litigation has been democratized by the use of different kind of devices that now

228
00:16:13,540 --> 00:16:18,940
people have their own voices that can be heard, that it didn't really matter maybe 100 years

229
00:16:18,940 --> 00:16:24,380
ago that there were different definitions of racism, the definition that would turn

230
00:16:24,380 --> 00:16:30,900
into a status quo was determined and established by a centralized structure of authority that

231
00:16:30,900 --> 00:16:35,180
had control over newspapers or different kind of media and that was the term.

232
00:16:35,180 --> 00:16:40,120
But now with these, you know, I always refer to these that we're like monkeys with machine

233
00:16:40,120 --> 00:16:41,120
guns.

234
00:16:41,120 --> 00:16:47,440
We don't really know the power of these things, but we are using them for our own biologically

235
00:16:47,440 --> 00:16:49,480
driven intentions.

236
00:16:49,480 --> 00:16:56,520
Yeah, I mean, I think I mean, it's hard to know exactly what the consequence of new media

237
00:16:56,520 --> 00:16:58,080
is going to be for this.

238
00:16:58,080 --> 00:17:04,440
For sure with the advent of television and so forth in the previous century, you might

239
00:17:04,440 --> 00:17:10,100
have thought, oh, wow, this is really great because now we can discuss alternative meanings.

240
00:17:10,100 --> 00:17:13,800
But what you got instead was more of a system of control.

241
00:17:13,800 --> 00:17:21,040
That is one understanding or one meaning of the term became quasi-official.

242
00:17:21,040 --> 00:17:29,400
I mean, it's not like we had some academy that was telling us what a word should mean,

243
00:17:29,400 --> 00:17:35,920
but because of corporate control of media in the United States, there was a kind of

244
00:17:35,920 --> 00:17:40,160
control of what words could mean in accepted media.

245
00:17:40,160 --> 00:17:45,720
Now that raises the question, now that we're all monkeys with our little computers in our

246
00:17:45,720 --> 00:17:47,920
hands, what is that going to mean?

247
00:17:47,920 --> 00:17:53,840
Is it going to mean we get a kind of fractionalization of different word meanings?

248
00:17:53,840 --> 00:17:55,560
And that could happen.

249
00:17:55,560 --> 00:18:01,800
And what you could get is a bunch of people descending into different bubbles where, you

250
00:18:01,800 --> 00:18:05,400
know, this cluster understands a word like freedom in this way.

251
00:18:05,400 --> 00:18:09,040
This cluster understands a word like freedom in a very different way.

252
00:18:09,040 --> 00:18:12,320
And now we're just talking past each other.

253
00:18:12,320 --> 00:18:19,640
So the real trick, I mean, it's easy to have a unified meaning of what a term is.

254
00:18:19,640 --> 00:18:23,160
And it's easy to have little clusters of different meanings.

255
00:18:23,160 --> 00:18:31,560
The hard part is getting these clusters to find common ground with each other and negotiate

256
00:18:31,560 --> 00:18:33,880
what the words ought to mean, right?

257
00:18:33,880 --> 00:18:40,720
And that is the skill that I think perhaps we have lost somewhere along the way.

258
00:18:40,720 --> 00:18:46,560
I mean, I don't know, I mean, any sort of historical story is going to be somewhat fantastical

259
00:18:46,560 --> 00:18:47,560
here.

260
00:18:47,560 --> 00:18:52,520
But I can imagine an earlier history and an earlier time when people got together in the

261
00:18:52,520 --> 00:18:59,800
bazaar or whatever, and they're not just negotiating the price for spices or whatever, right?

262
00:18:59,800 --> 00:19:03,720
Let's say we're on the Silk Road or something and we're negotiating prices, but we're also

263
00:19:03,720 --> 00:19:05,960
negotiating word meanings.

264
00:19:05,960 --> 00:19:12,240
And I tend to think that there wasn't necessarily set understandings of what a word is supposed

265
00:19:12,240 --> 00:19:14,840
to mean, right?

266
00:19:14,840 --> 00:19:20,400
But maybe with the 20th century, we began to have those kinds of set understandings.

267
00:19:20,400 --> 00:19:29,600
And even now, when we ghettoize ourselves into different bubbles inside of Facebook,

268
00:19:29,600 --> 00:19:35,560
I'm not sure that we are addressing important questions about not just what words mean to

269
00:19:35,560 --> 00:19:37,640
us and our friends, but what they ought to mean.

270
00:19:37,640 --> 00:19:42,360
Yeah, it was also a matter of documentation of them too, right?

271
00:19:42,360 --> 00:19:47,640
The idea that history is written by the victor, that what we are reading is not necessarily

272
00:19:47,640 --> 00:19:51,760
what had happened, is what has been written down and documented.

273
00:19:51,760 --> 00:19:56,040
And the difference right now is that this process of documentation or litigation that

274
00:19:56,040 --> 00:19:57,680
you're saying, it's been democratized.

275
00:19:57,680 --> 00:20:03,840
So it's not that there is any kind of a singular kind of a foundation, because there are different

276
00:20:03,840 --> 00:20:08,200
kind of needs, different kind of desires, different kind of...

277
00:20:08,200 --> 00:20:09,200
People are different.

278
00:20:09,200 --> 00:20:14,600
Equality, objectively, has no meaning, naturally speaking, at least to me.

279
00:20:14,600 --> 00:20:18,120
Equity is a different kind of a story, but the fact that you want to start everything

280
00:20:18,120 --> 00:20:21,240
at point zero, what are we going to do?

281
00:20:21,240 --> 00:20:26,360
Are we going to eventually genetically engineer everybody to start at the same ground with

282
00:20:26,360 --> 00:20:27,480
the same kind of...

283
00:20:27,480 --> 00:20:28,480
It's just impossible.

284
00:20:28,480 --> 00:20:29,480
Right.

285
00:20:29,480 --> 00:20:30,480
Impossible.

286
00:20:30,480 --> 00:20:31,480
We all start from different places, right?

287
00:20:31,480 --> 00:20:37,440
And so given that we're starting from different places, how do we engage with each other and

288
00:20:37,440 --> 00:20:42,400
create, in my terminology, micro-languages with each other so that we can communicate

289
00:20:42,400 --> 00:20:46,000
even though we begin from different starting places?

290
00:20:46,000 --> 00:20:51,880
And you mentioned a point earlier where when we look back historically, we often don't

291
00:20:51,880 --> 00:20:59,600
really understand how variegated language was and all the different forms in which it could

292
00:20:59,600 --> 00:21:07,080
be found, ranging from how things were written to the different understandings of word meanings.

293
00:21:07,080 --> 00:21:13,120
And hopefully, if nothing else, what the internet is going to give us is a vast repository where

294
00:21:13,120 --> 00:21:14,120
we're...

295
00:21:14,120 --> 00:21:18,520
A hundred years from now or a thousand years from now, we're going to be able to study

296
00:21:18,520 --> 00:21:24,120
the different ways in which people were using language in our time.

297
00:21:24,120 --> 00:21:28,200
And this depository now is being used for development of artificial intelligence as

298
00:21:28,200 --> 00:21:29,200
well, right?

299
00:21:29,200 --> 00:21:33,600
And let's think about this context in the big picture of what Marshall McLuhan said,

300
00:21:33,600 --> 00:21:36,880
that man becomes sex organs of the machines.

301
00:21:36,880 --> 00:21:41,520
We're basically feeding the machines to become whatever that they're going to become.

302
00:21:41,520 --> 00:21:44,880
And it seems like more and more aspects of our lives are becoming digitized.

303
00:21:44,880 --> 00:21:47,480
So this is not a process they're going to reverse at any point.

304
00:21:47,480 --> 00:21:48,480
It's just going to accelerate.

305
00:21:48,480 --> 00:21:54,920
I mean, it's very interesting to think about what is the nature of the symbiotic relationship

306
00:21:54,920 --> 00:22:05,360
we have with the internet, because it is taking on a big chunk of our cognitive function.

307
00:22:05,360 --> 00:22:08,680
I mean, it's onloading lots of our memory.

308
00:22:08,680 --> 00:22:14,000
And so you might ask a question, I mean, there's certain philosophers, like a guy named David

309
00:22:14,000 --> 00:22:16,560
Chalmers has asked this question, where exactly is...

310
00:22:16,560 --> 00:22:17,560
Extension.

311
00:22:17,560 --> 00:22:18,560
Extended mind.

312
00:22:18,560 --> 00:22:19,560
Exactly.

313
00:22:19,560 --> 00:22:23,280
And so, well, where is my mind?

314
00:22:23,280 --> 00:22:25,080
Well, where's my memory?

315
00:22:25,080 --> 00:22:28,440
I mean, that's an easy, you know, because like you asked me a question, I don't remember

316
00:22:28,440 --> 00:22:29,440
the...

317
00:22:29,440 --> 00:22:33,400
Well, a great example is you asked me the title of my book and I can't remember the

318
00:22:33,400 --> 00:22:34,400
subtitle.

319
00:22:34,400 --> 00:22:35,400
Well, I can't remember it.

320
00:22:35,400 --> 00:22:38,480
I just don't have my phone handy and I don't want to be rude and pick it up.

321
00:22:38,480 --> 00:22:39,600
But that's it.

322
00:22:39,600 --> 00:22:46,120
I just like offload a bunch of my memory onto my telephone or into the internet.

323
00:22:46,120 --> 00:22:55,320
And so we begin to create this very weird symbiotic relationship with a machine, but

324
00:22:55,320 --> 00:22:59,160
not just with our physical bodies, right?

325
00:22:59,160 --> 00:23:02,200
But actually our minds are...

326
00:23:02,200 --> 00:23:05,440
Much of our mental processing is being done externally.

327
00:23:05,440 --> 00:23:10,200
You know, people always talk about, oh, I want to get a computer inserted in my head.

328
00:23:10,200 --> 00:23:11,760
It's too late for all that.

329
00:23:11,760 --> 00:23:12,960
In effect, that's already...

330
00:23:12,960 --> 00:23:15,040
You don't have to put it in your head, right?

331
00:23:15,040 --> 00:23:25,200
The fact that you're relying on the internet and on Google or DuckDuckGo or whatever search

332
00:23:25,200 --> 00:23:32,400
engine you use, the fact that you're relying on that to reason and to carry on your cognitive

333
00:23:32,400 --> 00:23:38,080
functions shows that that fusion of your mind with the internet is to some extent already

334
00:23:38,080 --> 00:23:45,800
happened and you don't need to do sci-fi things like implant things into your head.

335
00:23:45,800 --> 00:23:49,280
So you're not on board with Elon Musk's Neuralink?

336
00:23:49,280 --> 00:23:50,280
No.

337
00:23:50,280 --> 00:23:53,960
I mean, it would be interesting to see what happens.

338
00:23:53,960 --> 00:23:58,480
I would expect something interesting to happen.

339
00:23:58,480 --> 00:24:04,000
And from an engineering problem, it'll be really fascinating.

340
00:24:04,000 --> 00:24:09,400
But from a philosophical issue, it's almost like, well, yeah, but it's kind of...

341
00:24:09,400 --> 00:24:14,400
We've already done that in a way, in the most interesting way, at the most philosophical

342
00:24:14,400 --> 00:24:16,480
level, it's already happened.

343
00:24:16,480 --> 00:24:17,480
One of the arguments...

344
00:24:17,480 --> 00:24:21,300
I've talked to a number of people about this.

345
00:24:21,300 --> 00:24:26,480
One of the arguments is that in order to find who we are, which is the way to find about

346
00:24:26,480 --> 00:24:35,240
the universe and everything else to look within inwardly, we require to develop higher levels

347
00:24:35,240 --> 00:24:41,960
of cognition, cognition ability, because we're doing really good compared to monkeys, but

348
00:24:41,960 --> 00:24:43,720
we could do way better.

349
00:24:43,720 --> 00:24:48,120
Whatever the definition of that better is, that's debatable.

350
00:24:48,120 --> 00:24:51,520
But the fact that what we're experiencing is not all there is.

351
00:24:51,520 --> 00:24:57,080
We can experience far more, let's say like in matrix kind of an environment.

352
00:24:57,080 --> 00:25:04,720
And to get there, we cannot rely on biology and trial and error driven process of evolution.

353
00:25:04,720 --> 00:25:06,520
Now that's very arrogant.

354
00:25:06,520 --> 00:25:10,920
Some people would say, well, you're playing God and you don't know how this going to affect

355
00:25:10,920 --> 00:25:14,080
and influence your environment and you can mess everything up.

356
00:25:14,080 --> 00:25:21,020
But at the same time, it has a spiritual kind of aspect to it too, that if I can know more

357
00:25:21,020 --> 00:25:26,080
and be more, why wouldn't I do it?

358
00:25:26,080 --> 00:25:30,120
I've never been on board with this kind of project.

359
00:25:30,120 --> 00:25:35,360
Because I feel that you can change your cognitive makeup.

360
00:25:35,360 --> 00:25:39,560
People think, well, all right, there must be a higher plane of reality that I can achieve.

361
00:25:39,560 --> 00:25:41,600
I could know things better.

362
00:25:41,600 --> 00:25:46,920
But it's always occurred to me that you might achieve a different way of thinking, but I

363
00:25:46,920 --> 00:25:51,760
don't know that...

364
00:25:51,760 --> 00:25:55,440
There's an interesting sense in which it's a higher plane.

365
00:25:55,440 --> 00:25:58,920
It's a different plane, right?

366
00:25:58,920 --> 00:26:03,360
And you might get there and you might look down at your previous life before you enhanced

367
00:26:03,360 --> 00:26:08,360
your mind with implants and so forth and you say, wow, that was really stupid.

368
00:26:08,360 --> 00:26:16,080
But that's not really evidence that you've achieved a higher plane, right?

369
00:26:16,080 --> 00:26:21,060
It may be evidence that you've achieved a plane of higher hubris or something like that.

370
00:26:21,060 --> 00:26:28,400
And the other element to it is I'm not convinced that we are even close to bumping up against

371
00:26:28,400 --> 00:26:32,120
natural limits to our own intelligence.

372
00:26:32,120 --> 00:26:35,680
Part of that is because I don't think our own intelligence is solely a function of what's

373
00:26:35,680 --> 00:26:37,880
going on inside of our heads.

374
00:26:37,880 --> 00:26:43,320
And this brings us back to the business about the extended mind.

375
00:26:43,320 --> 00:26:48,640
That my mind is in part a function of what's going on in the external world already.

376
00:26:48,640 --> 00:26:53,040
And if all these amazing things are happening in the internet and artificial intelligence

377
00:26:53,040 --> 00:26:57,580
and big data, and all of those can be construed as part of my mind.

378
00:26:57,580 --> 00:27:00,960
But furthermore, there are these methods of communication in which I can communicate and

379
00:27:00,960 --> 00:27:04,600
exchange ideas with people like you, right?

380
00:27:04,600 --> 00:27:13,320
I am very rapidly achieving higher and higher, I don't know, planes of consciousness is the

381
00:27:13,320 --> 00:27:19,440
right word, but certainly I'm learning a lot more and thinking in ways that I couldn't

382
00:27:19,440 --> 00:27:22,600
imagine thinking earlier, right?

383
00:27:22,600 --> 00:27:29,720
So to me that there are these avenues of sort of naturally growing that are much more interesting

384
00:27:29,720 --> 00:27:38,600
to me than to try and find some sort of imaginary wall to our intelligence or some sort of ceiling

385
00:27:38,600 --> 00:27:40,880
to our intelligence is a better way to put it.

386
00:27:40,880 --> 00:27:43,720
Which if we could break through that, then we can see the world correctly.

387
00:27:43,720 --> 00:27:45,160
Yeah, I totally agree.

388
00:27:45,160 --> 00:27:51,960
And it's very interesting, I mean, I would consider myself a transhumanist in a sense

389
00:27:51,960 --> 00:27:57,100
that I believe that humanity and technology, we have evolved together in a sense the way

390
00:27:57,100 --> 00:28:02,440
that Kubrick portrayed it in 2001 Space Odyssey.

391
00:28:02,440 --> 00:28:07,680
We found a tool and then we used it in order to obtain things for ourselves and then protecting

392
00:28:07,680 --> 00:28:11,120
those things against a different tribe and then a different tribe looked at us and they

393
00:28:11,120 --> 00:28:15,720
improved that tool a little bit and now we have International Space Station and reusable

394
00:28:15,720 --> 00:28:17,320
rockets and all that.

395
00:28:17,320 --> 00:28:22,420
But you're absolutely, I think, spot on with saying that we haven't even tapped into the

396
00:28:22,420 --> 00:28:29,160
full potential of our own cognition and ability and we cannot really make the claim otherwise

397
00:28:29,160 --> 00:28:36,760
when we have a variety of psychedelics completely illegal and hey, you cannot do that, don't

398
00:28:36,760 --> 00:28:40,200
you dare doing that and this is an interesting time we're talking about it because it's

399
00:28:40,200 --> 00:28:43,480
the second revolution of psychedelics that are happening right now.

400
00:28:43,480 --> 00:28:48,720
If really we're in the second revolution of, well, you know, it's funny you say that

401
00:28:48,720 --> 00:28:53,760
because I'm living in Mexico now and so, I mean, there are a lot of people that are

402
00:28:53,760 --> 00:29:01,680
experimenting with psychedelics but they have been for, you know, a thousand years, like

403
00:29:01,680 --> 00:29:07,880
the huichol with the peyote and so forth or if you go down to Brazil, lots of tribes and

404
00:29:07,880 --> 00:29:17,480
I mean, I actually have thoughts about psychedelics and their history with people which is that

405
00:29:17,480 --> 00:29:24,520
to some extent the human mind evolved to not really function that well, it's on its own

406
00:29:24,520 --> 00:29:30,540
like in isolation but we evolved to be able to function with the help of the things we

407
00:29:30,540 --> 00:29:35,640
find around us and in some sense that might be magic mushrooms or whatever the case might

408
00:29:35,640 --> 00:29:36,640
be.

409
00:29:36,640 --> 00:29:39,040
You're talking about the stoned ape theory?

410
00:29:39,040 --> 00:29:40,040
What's that?

411
00:29:40,040 --> 00:29:41,040
I don't know that theory.

412
00:29:41,040 --> 00:29:42,040
What?

413
00:29:42,040 --> 00:29:43,040
The stoned ape?

414
00:29:43,040 --> 00:29:49,960
That like almost every animal on the planet has a way of managing its mental health with

415
00:29:49,960 --> 00:29:58,920
the help of some form of drug, you know, from reindeers with their little red cap mushrooms

416
00:29:58,920 --> 00:30:04,240
to really, is there an actual theory with the name about stoned apes?

417
00:30:04,240 --> 00:30:09,400
Yeah, Terrence McKenna, Terrence McKenna's theory, stoned ape theory is that we basically,

418
00:30:09,400 --> 00:30:12,780
our brain evolved faster than the rest of our body because we started experimenting

419
00:30:12,780 --> 00:30:17,080
with psychedelics.

420
00:30:17,080 --> 00:30:22,200
I would say that we co-evolved with these substances, okay?

421
00:30:22,200 --> 00:30:23,200
So I don't know.

422
00:30:23,200 --> 00:30:27,600
I mean if you went back to early, early history, I believe we actually co-evolved with some

423
00:30:27,600 --> 00:30:33,280
of these substances and we can find them around on planet Earth right now because early human

424
00:30:33,280 --> 00:30:35,720
ancestors found them useful.

425
00:30:35,720 --> 00:30:44,520
And so to function correctly, cognitively, we needed these plants.

426
00:30:44,520 --> 00:30:45,520
We don't need them now.

427
00:30:45,520 --> 00:30:49,600
You know, we have Big Pharma that can help us, I suppose.

428
00:30:49,600 --> 00:30:56,160
But you know, if you try and, here again we get into this issue of the externalization

429
00:30:56,160 --> 00:30:57,520
of the human mind, right?

430
00:30:57,520 --> 00:31:02,440
It's not just what's in here and it's not just what we can connect with the internet.

431
00:31:02,440 --> 00:31:08,600
But once you start passing laws that prohibit people from using these substances, like peyote

432
00:31:08,600 --> 00:31:16,720
or whatever it might be, in a certain sense you're passing laws that are walling off parts

433
00:31:16,720 --> 00:31:19,640
of people's very own minds, if that makes sense to you.

434
00:31:19,640 --> 00:31:20,640
Yeah, absolutely.

435
00:31:20,640 --> 00:31:25,520
Yeah, I'm saying this is the second, I mean it's well established that we're going through

436
00:31:25,520 --> 00:31:31,800
a second psychedelic revolution because of organizations like MAPS, which is a multi-discipline

437
00:31:31,800 --> 00:31:37,080
multi-disciplinary association with psychedelic studies that they're pushing to legalize MDMA

438
00:31:37,080 --> 00:31:41,920
for example, to deal with depression and PTSD for, because there are more, for example,

439
00:31:41,920 --> 00:31:47,400
veterans are dying inside the United States than they're dying in the battlefield.

440
00:31:47,400 --> 00:31:51,840
And a lot of them get hooked on like opioids and stuff, but then they do like two sessions

441
00:31:51,840 --> 00:31:56,280
of ayahuasca and you know, it does magic for example for them.

442
00:31:56,280 --> 00:31:57,280
Interesting.

443
00:31:57,280 --> 00:31:58,280
Yeah.

444
00:31:58,280 --> 00:31:59,280
Interesting.

445
00:31:59,280 --> 00:32:01,880
It's a moral kind of argument.

446
00:32:01,880 --> 00:32:08,120
Yeah, I mean I'm not any sort of expert on this and I'm not, I mean I have my own way

447
00:32:08,120 --> 00:32:14,040
of managing my mental life, which involves a lot of time and the internet and with computers

448
00:32:14,040 --> 00:32:19,600
and so forth and gaming, but it's very clear to me that there are people that really need

449
00:32:19,600 --> 00:32:25,680
these substances and access to them and it's critical, it's a critical part of their mental

450
00:32:25,680 --> 00:32:26,680
lives.

451
00:32:26,680 --> 00:32:27,680
Yeah.

452
00:32:27,680 --> 00:32:30,400
And it's, it's, it's alien to deny that to people.

453
00:32:30,400 --> 00:32:31,400
Absolutely.

454
00:32:31,400 --> 00:32:33,400
Like say, well, you can't use this part of your brain.

455
00:32:33,400 --> 00:32:34,400
Yeah.

456
00:32:34,400 --> 00:32:35,400
Or explore it.

457
00:32:35,400 --> 00:32:36,400
Yeah.

458
00:32:36,400 --> 00:32:37,400
Or explore it.

459
00:32:37,400 --> 00:32:38,400
Yeah.

460
00:32:38,400 --> 00:32:39,400
Yeah.

461
00:32:39,400 --> 00:32:40,400
Yeah.

462
00:32:40,400 --> 00:32:41,400
Yeah.

463
00:32:41,400 --> 00:32:42,400
And so, but coming back to your point, I mean, yeah.

464
00:32:42,400 --> 00:32:45,760
So these are, I don't think of them as even tools.

465
00:32:45,760 --> 00:32:51,840
I think they are these, these, these substances, these natural substances are parts, critical

466
00:32:51,840 --> 00:32:53,320
parts of our mental lives.

467
00:32:53,320 --> 00:33:00,400
And so obviously if you deny yourself those elements to your mental life, you're going

468
00:33:00,400 --> 00:33:06,920
to set up certain limits to what you can and cannot think.

469
00:33:06,920 --> 00:33:12,680
And you start, you start placing, you start placing limits on, on your mental life.

470
00:33:12,680 --> 00:33:17,480
And that's when people start asking for things like, you know, I want to, I want implants

471
00:33:17,480 --> 00:33:18,820
or something like that.

472
00:33:18,820 --> 00:33:19,820
You don't need implants.

473
00:33:19,820 --> 00:33:23,720
You just, you just need the things that are already there and that's going to open up

474
00:33:23,720 --> 00:33:26,200
business that are so vast.

475
00:33:26,200 --> 00:33:33,960
I mean, we can't even begin to, we can't even begin to quantify the vastness of the spaces

476
00:33:33,960 --> 00:33:36,320
that are already available to us.

477
00:33:36,320 --> 00:33:40,120
I think the bridge that connects what we were talking about to back to the digital world

478
00:33:40,120 --> 00:33:45,560
and cyberspace, and I want to talk about crypto-anarchy, it really comes down to individual liberty

479
00:33:45,560 --> 00:33:50,840
that you, you will be able to determine who you are and what you're going to do with that

480
00:33:50,840 --> 00:33:57,600
information or data or whatever that determines you, that you represent in a cyberspace, for

481
00:33:57,600 --> 00:33:58,600
example.

482
00:33:58,600 --> 00:34:05,120
And I want to ask you about what is happening to the concept of individuality in the digital

483
00:34:05,120 --> 00:34:12,400
realm or cyberspace or metaverse and whether or not crypto-anarchy from your perspective

484
00:34:12,400 --> 00:34:17,840
is the soul, and I would argue that it should be decentralized crypto-anarchy, right, because

485
00:34:17,840 --> 00:34:22,440
I know that anarchy philosophically means that it's decentralized, but it seems like

486
00:34:22,440 --> 00:34:26,160
for humans you have to emphasize on key terms.

487
00:34:26,160 --> 00:34:35,000
Decentralized crypto-anarchy is the only way and only avenue in order to protect this sense

488
00:34:35,000 --> 00:34:41,240
of individuality, however it's going to be defined based on our evolution.

489
00:34:41,240 --> 00:34:48,600
Let me say that I am a bit concerned about all the talk of individualism that we get

490
00:34:48,600 --> 00:34:54,240
in the crypto sphere because it's onboarded a lot of libertarian talk and I'm sure that

491
00:34:54,240 --> 00:35:01,300
many of your listeners are libertarians and I have issues with libertarianism and talk

492
00:35:01,300 --> 00:35:07,320
of individualism and I would much rather frame all of this in terms of limits on what we

493
00:35:07,320 --> 00:35:12,600
could think, right, using a plural pronoun there, like, you know, what we can do, what

494
00:35:12,600 --> 00:35:25,400
we could think, and I don't – I think it's more helpful to think about what crypto-anarchy

495
00:35:25,400 --> 00:35:34,800
opens up to us in terms of possibilities for collaboration that otherwise couldn't happen

496
00:35:34,800 --> 00:35:40,800
because it has to be – in certain cases, it has to be hidden from maybe from the state,

497
00:35:40,800 --> 00:35:43,480
but the state is not the only problem here.

498
00:35:43,480 --> 00:35:51,800
It might have to be hidden from society as a whole because of certain negative attitudes

499
00:35:51,800 --> 00:35:55,740
that society has to the ideas that you're exchanging, right?

500
00:35:55,740 --> 00:36:01,680
So for me, if crypto-anarchy doesn't really open up a space for me to be me and I can

501
00:36:01,680 --> 00:36:05,880
do whatever the hell I want and I can think whatever I want, but it's more like it's

502
00:36:05,880 --> 00:36:12,720
opening up a space for us to engage in collective activities that we otherwise not have been

503
00:36:12,720 --> 00:36:20,120
able to do and that's going to benefit me because it's not just me sitting here in

504
00:36:20,120 --> 00:36:25,060
a vacuum but when I'm engaging with other people who now have this freedom as well,

505
00:36:25,060 --> 00:36:33,800
the freedom to negotiate new meanings and terms freely together, right, then that opens

506
00:36:33,800 --> 00:36:37,040
up another canvas for us to explore, right?

507
00:36:37,040 --> 00:36:41,580
Another unlimited canvas or at least a very vast canvas that's going to open up to us

508
00:36:41,580 --> 00:36:43,600
and allow us to explore.

509
00:36:43,600 --> 00:36:48,360
So individualism, I'd rather use a term like freedom and that would include not just

510
00:36:48,360 --> 00:36:56,000
individual freedom but collective freedom or community freedom, family freedom, you

511
00:36:56,000 --> 00:36:57,000
know.

512
00:36:57,000 --> 00:37:00,800
Freedom of association maybe would be a good term, umbrella term.

513
00:37:00,800 --> 00:37:04,160
Freedom of association?

514
00:37:04,160 --> 00:37:08,840
Not just freedom of association but I mean there's more to it than that because if we

515
00:37:08,840 --> 00:37:15,000
have, I mean freedom to associate is key, right, and I mean the internet allows us to

516
00:37:15,000 --> 00:37:24,000
associate from across distances and what Crypto Anarchy does is allows people to associate

517
00:37:24,000 --> 00:37:28,520
with us that might not otherwise be able to, right, because their government might not

518
00:37:28,520 --> 00:37:34,040
even think they should be online so they have to find other ways of getting online.

519
00:37:34,040 --> 00:37:40,000
I mean they might have to use Tor or something to not be spied on when they're online talking

520
00:37:40,000 --> 00:37:44,960
with us and then once and so that freedom as you say, freedom of association is going

521
00:37:44,960 --> 00:37:49,760
to be critical but then once we get there we also have other freedoms.

522
00:37:49,760 --> 00:37:55,760
We have these freedoms to exchange these ideas once we're associating and we have the freedom

523
00:37:55,760 --> 00:38:09,600
to not just associate but to work together and to keep modifying or I like the word modulate,

524
00:38:09,600 --> 00:38:13,360
to modulate word meanings with each other and that's key too.

525
00:38:13,360 --> 00:38:18,680
That is that language should not be a prison to us but now language is going to be something

526
00:38:18,680 --> 00:38:25,240
that we can, that's going to work for us instead of us working inside of a little cage that

527
00:38:25,240 --> 00:38:29,920
language has provided for us and getting together with other people is the first step to that,

528
00:38:29,920 --> 00:38:33,840
right, because if you're alone or you're with the same group of people all the time

529
00:38:33,840 --> 00:38:38,040
then you don't have those options.

530
00:38:38,040 --> 00:38:42,840
It's not going to be obvious how you should be expanding your language but once you have

531
00:38:42,840 --> 00:38:49,680
the opportunity to sort of interact with people in other places, people from other cultures,

532
00:38:49,680 --> 00:38:55,280
people with different ideas and different micro-languages, that's when things really

533
00:38:55,280 --> 00:38:57,740
start to happen.

534
00:38:57,740 --> 00:39:01,520
This relationship between language and us that you're saying that we're reverting in

535
00:39:01,520 --> 00:39:06,280
a way that language will work for us rather than we work for the pre-established, there

536
00:39:06,280 --> 00:39:12,120
are people who are benefiting from those pre-existed definitions of language.

537
00:39:12,120 --> 00:39:15,040
They are, yes they are, yes.

538
00:39:15,040 --> 00:39:19,800
Maybe government is one of the best examples of them, churches are another one, education

539
00:39:19,800 --> 00:39:20,800
system.

540
00:39:20,800 --> 00:39:25,480
People will make the case that patriarchy, people will say that you and I benefit because

541
00:39:25,480 --> 00:39:30,880
of the sort of the patriarchal nature of language and so there's a kind of sexism built into

542
00:39:30,880 --> 00:39:38,000
language itself and then there is this project which is what they would call, I think the

543
00:39:38,000 --> 00:39:43,960
fancy word that philosophers use is conceptual ameliorization but what they mean is we want

544
00:39:43,960 --> 00:39:50,280
to change the – people have imposed these word meanings on us and they're not helpful

545
00:39:50,280 --> 00:39:55,560
and what we have to do is sort of break these definitions or bend the definitions because

546
00:39:55,560 --> 00:39:58,340
we can't make any progress until we do that.

547
00:39:58,340 --> 00:40:02,400
So yeah, basically in any sphere it's not just government, it could be cultural, it

548
00:40:02,400 --> 00:40:06,160
could be religious, right?

549
00:40:06,160 --> 00:40:10,780
What is a religious person, what is a moral person?

550
00:40:10,780 --> 00:40:19,040
People use language to imprison other people and so a big part of freeing yourself and

551
00:40:19,040 --> 00:40:26,040
freeing your family and freeing your friends is to resist this.

552
00:40:26,040 --> 00:40:32,080
First of all, identify when people are using language to imprison you and secondly, finding

553
00:40:32,080 --> 00:40:39,560
the tools that allow you to resist it and to make the case for your view of how language

554
00:40:39,560 --> 00:40:41,560
actually should be shaped.

555
00:40:41,560 --> 00:40:45,840
So what's going to happen to those benefactors of this language prison in the coming years

556
00:40:45,840 --> 00:40:49,880
as information is becoming more and more democratized?

557
00:40:49,880 --> 00:40:53,760
Yeah, well, they're going to freak out, obviously.

558
00:40:53,760 --> 00:40:59,160
They're going to freak out and you could see this, I mean there's going to be a kind

559
00:40:59,160 --> 00:41:05,000
of linguistic conservatism, I don't mean that like Republican versus Democrat because they

560
00:41:05,000 --> 00:41:06,000
can both be conservative.

561
00:41:06,000 --> 00:41:08,000
Just hold on to what is.

562
00:41:08,000 --> 00:41:10,000
Yeah, yeah.

563
00:41:10,000 --> 00:41:14,200
And they're going to be pounding on tables saying, look, I have a dictionary right here

564
00:41:14,200 --> 00:41:19,520
and it says like, this is what a woman is, it can't be a trans person.

565
00:41:19,520 --> 00:41:24,940
No, I'm not making a case that trans women are women, I'm just saying the minute people

566
00:41:24,940 --> 00:41:29,120
open up a dictionary and says that's how you resolve it, those are the people that

567
00:41:29,120 --> 00:41:32,880
are falling back on a conservative sort of definition.

568
00:41:32,880 --> 00:41:39,520
And people are going to be saying, oh, this new definition is degeneracy.

569
00:41:39,520 --> 00:41:40,760
You're destroying language.

570
00:41:40,760 --> 00:41:42,120
This is another one you're going to hear.

571
00:41:42,120 --> 00:41:43,880
You're destroying language.

572
00:41:43,880 --> 00:41:46,720
Everything is going to fall apart because you're destroying language and you're not

573
00:41:46,720 --> 00:41:48,560
destroying language.

574
00:41:48,560 --> 00:41:52,680
You're destroying language, you keep it in a prison and you don't let it evolve, right?

575
00:41:52,680 --> 00:41:56,680
I mean, you keep it in a prison, it's like putting a tiger in a cage and just locking

576
00:41:56,680 --> 00:42:00,800
it up there and thinking that you are somehow preserving it by doing that.

577
00:42:00,800 --> 00:42:01,800
Well, you're not.

578
00:42:01,800 --> 00:42:04,680
You have to turn it loose.

579
00:42:04,680 --> 00:42:10,240
Do you think this point of authority to determine the definition and meanings of language is

580
00:42:10,240 --> 00:42:16,000
shifting from human governance and authority to artificial intelligence?

581
00:42:16,000 --> 00:42:23,200
I had not thought about that.

582
00:42:23,200 --> 00:42:30,160
Only artificial intelligence can inform these sorts of decisions.

583
00:42:30,160 --> 00:42:37,440
What would happen if an AI started telling us that, you know, you're using the definition

584
00:42:37,440 --> 00:42:41,440
of woman wrong or you're using the definition of freedom wrong?

585
00:42:41,440 --> 00:42:43,040
I don't know.

586
00:42:43,040 --> 00:42:52,080
The thing is that the way I perceive all of this is playing out is that there's certain

587
00:42:52,080 --> 00:42:59,040
people are going to come up and try and establish authority to you or establish their linguistic

588
00:42:59,040 --> 00:43:01,680
authority, okay?

589
00:43:01,680 --> 00:43:06,520
And there's a question, well, does that linguistic authority work here?

590
00:43:06,520 --> 00:43:13,400
So, you know, we get stranded on Gilligan's Island and the professor comes to us and says,

591
00:43:13,400 --> 00:43:19,160
well, no, actually a tomato is not a vegetable, it's a fruit, right?

592
00:43:19,160 --> 00:43:23,360
Well, okay, maybe he has some knowledge about science there.

593
00:43:23,360 --> 00:43:32,080
The question is, is that kind of authority relevant to how we talk about fruits and vegetables?

594
00:43:32,080 --> 00:43:36,680
Likewise, if an artificial intelligence came to us and said, well, I have all these reasons

595
00:43:36,680 --> 00:43:44,800
for thinking that, you know, a woman can't be a trans woman or that, you know, freedom

596
00:43:44,800 --> 00:43:46,960
had some sort of definition of freedom for us.

597
00:43:46,960 --> 00:43:52,600
He says that you have to serve the machine, but to be truly free, you have to serve the

598
00:43:52,600 --> 00:43:53,600
machine.

599
00:43:53,600 --> 00:43:59,000
And we're like, well, you know, you're a very smart AI, but like, you know, we don't

600
00:43:59,000 --> 00:44:01,360
necessarily have to take that on, right?

601
00:44:01,360 --> 00:44:09,720
And so it's still, even if there are smarter people out there, you know, we don't merely

602
00:44:09,720 --> 00:44:14,120
have the freedom to do this or we should have the freedom to do it, we also have the responsibility

603
00:44:14,120 --> 00:44:20,760
to say, well, look, I don't really, I don't know if I should onboard your definition just

604
00:44:20,760 --> 00:44:25,720
because you're smarter than me because I have my own reasons for defining words as I do.

605
00:44:25,720 --> 00:44:32,560
So maybe, you know, I have my own reasons for saying a tomato is a vegetable.

606
00:44:32,560 --> 00:44:36,280
And I don't really care what the artificial intelligence thinks about that.

607
00:44:36,280 --> 00:44:42,320
So at the end, so I guess my point is at the end of the day, we are going to have to still

608
00:44:42,320 --> 00:44:47,920
be responsible for deciding which authorities we're going to accept.

609
00:44:47,920 --> 00:44:54,160
And you know, I wrote a paper on this, but I actually never got around to writing.

610
00:44:54,160 --> 00:45:00,920
I gave a talk on this and it's one of my favorite things to think about, but I'm just inherently

611
00:45:00,920 --> 00:45:05,720
lazy about writing it down for some reason, which is about the issue of when you defer

612
00:45:05,720 --> 00:45:07,360
to authority, right?

613
00:45:07,360 --> 00:45:09,440
So it's a paper about deference.

614
00:45:09,440 --> 00:45:13,480
And so when you should defer to someone and when you shouldn't, but deference, not in

615
00:45:13,480 --> 00:45:20,040
like, should I cut my hair or should I, you know, salute a flag or something, but deference

616
00:45:20,040 --> 00:45:23,800
to somebody about word meaning, about what a word ought to mean.

617
00:45:23,800 --> 00:45:26,240
And it's not an easy topic.

618
00:45:26,240 --> 00:45:31,640
And so part of the idea I had is that no matter what the authority is, you have to issue a

619
00:45:31,640 --> 00:45:34,960
series of challenges to that authority, always.

620
00:45:34,960 --> 00:45:40,560
It doesn't matter even if it's like some cool kid in school, right, that's using a term

621
00:45:40,560 --> 00:45:44,640
and you think you should defer to that person about the meaning of a cool, a meaning of

622
00:45:44,640 --> 00:45:45,640
a term.

623
00:45:45,640 --> 00:45:47,960
Like suppose the meaning of the term is cool, what's cool?

624
00:45:47,960 --> 00:45:52,000
So there's some guy who looks like Arthur Frans Larell Fonsi from Happy Days.

625
00:45:52,000 --> 00:45:53,960
And we start, we defer to him, why?

626
00:45:53,960 --> 00:45:55,320
Is it because of his haircut?

627
00:45:55,320 --> 00:45:57,240
Is it because of his leather jacket?

628
00:45:57,240 --> 00:46:03,760
Well, even a character like Richie in Happy Days at certain points has got to challenge

629
00:46:03,760 --> 00:46:04,960
Fonsi.

630
00:46:04,960 --> 00:46:09,060
And so the idea is, well, when do you challenge these linguistic authorities?

631
00:46:09,060 --> 00:46:11,280
How do you challenge those linguistic authorities?

632
00:46:11,280 --> 00:46:15,360
And this is something that fascinates me because I have no idea how it's supposed to work or

633
00:46:15,360 --> 00:46:17,280
even how about we go about it.

634
00:46:17,280 --> 00:46:20,760
But it's a skill we have to acquire because it's critical.

635
00:46:20,760 --> 00:46:24,000
Because we're being surrounded by all these linguistic authorities.

636
00:46:24,000 --> 00:46:26,200
They don't always have our best interests in mind.

637
00:46:26,200 --> 00:46:28,520
They don't even have their own best interests in mind.

638
00:46:28,520 --> 00:46:32,800
And we have to know when they're like, you know, they're jumping the shark or something

639
00:46:32,800 --> 00:46:36,200
and sort of using a word like cool and correctly.

640
00:46:36,200 --> 00:46:43,920
We have to be able to, we need to defer to individuals at certain times, like doctors

641
00:46:43,920 --> 00:46:48,440
on the meaning of certain medical terms, for example, but we also have to be in a position

642
00:46:48,440 --> 00:46:50,200
where we can challenge them.

643
00:46:50,200 --> 00:46:52,880
And that is an art.

644
00:46:52,880 --> 00:46:57,220
And if I had rules for doing that, I would tell you what they are, but I'm not even sure

645
00:46:57,220 --> 00:46:58,220
there are rules.

646
00:46:58,220 --> 00:46:59,760
I think it's just some practice.

647
00:46:59,760 --> 00:47:00,760
Yeah.

648
00:47:00,760 --> 00:47:01,760
Exactly.

649
00:47:01,760 --> 00:47:05,280
And with this information, we have more uncertainty we are dealing with.

650
00:47:05,280 --> 00:47:09,400
And I want to go back to what you were saying about New York, that it was a very chaotic

651
00:47:09,400 --> 00:47:13,600
kind of an era that people were doing whatever they wanted to do and cool stuff were happening.

652
00:47:13,600 --> 00:47:18,460
It seems like we're going through the same kind of a period right now in a different

653
00:47:18,460 --> 00:47:25,280
kind of a way that people have access to devices to express themselves and challenge each other.

654
00:47:25,280 --> 00:47:29,240
And we don't, as you said, we don't really know what the rules are, but maybe there are

655
00:47:29,240 --> 00:47:33,600
no rules and good stuff happen as a consequence along the way.

656
00:47:33,600 --> 00:47:40,320
And we just have to learn to notice and realize those moments of harmony, enjoy them and just

657
00:47:40,320 --> 00:47:41,320
move on.

658
00:47:41,320 --> 00:47:48,640
Yeah, I agree with that, except I would say it is never the case that there are no rules.

659
00:47:48,640 --> 00:47:52,720
Sometimes you think there are no rules, but that's when you have to be really careful

660
00:47:52,720 --> 00:47:56,160
because that means you're not seeing the rules, right?

661
00:47:56,160 --> 00:48:03,520
It's like the, you know, the fish in the bowl thinks there's nothing out there, thinks it's

662
00:48:03,520 --> 00:48:07,120
completely free because it's just swimming around wherever it wants to go.

663
00:48:07,120 --> 00:48:10,240
And then those moments when you think that you're completely free, that's when you need

664
00:48:10,240 --> 00:48:12,000
to be most on guard.

665
00:48:12,000 --> 00:48:13,000
Yeah.

666
00:48:13,000 --> 00:48:14,000
That's awesome.

667
00:48:14,000 --> 00:48:15,000
Yeah, absolutely.

668
00:48:15,000 --> 00:48:19,920
What are your thoughts on simulation theory?

669
00:48:19,920 --> 00:48:22,600
I know I don't understand what these guys are talking about.

670
00:48:22,600 --> 00:48:29,240
And some of them are my best friends, honestly, because maybe they just are putting it in

671
00:48:29,240 --> 00:48:35,280
a way that I don't understand because I wouldn't call it simulation theory, meaning do we live

672
00:48:35,280 --> 00:48:36,280
in a simulation?

673
00:48:36,280 --> 00:48:37,280
Is that the question?

674
00:48:37,280 --> 00:48:38,280
Simulation hypothesis.

675
00:48:38,280 --> 00:48:39,280
I better say no theory.

676
00:48:39,280 --> 00:48:40,280
Yeah.

677
00:48:40,280 --> 00:48:41,280
Yeah.

678
00:48:41,280 --> 00:48:48,880
I mean, because the thing is simulations typically don't have the relevant properties of the

679
00:48:48,880 --> 00:48:50,400
thing they are simulating.

680
00:48:50,400 --> 00:48:54,960
So for example, I can simulate hurricanes using a computer, but it's not like if I open

681
00:48:54,960 --> 00:48:59,360
up the computer, there's going to be lots of wind and rain in there, right?

682
00:48:59,360 --> 00:49:06,240
Likewise, I might be able to simulate some anything using artificial intelligence, but

683
00:49:06,240 --> 00:49:13,160
it doesn't mean that anything like whatever consciousness would be happening in there.

684
00:49:13,160 --> 00:49:20,680
I think a better way to make the same point would be, well, what if we are synthesizing

685
00:49:20,680 --> 00:49:21,960
worlds?

686
00:49:21,960 --> 00:49:28,480
So you're not merely simulating them, but you're synthesizing them somehow using, maybe

687
00:49:28,480 --> 00:49:33,480
using computers, maybe you can synthesize a reality using computers or something like

688
00:49:33,480 --> 00:49:34,480
that.

689
00:49:34,480 --> 00:49:40,320
And at that point, my attitude is, well, it doesn't matter if it's synthetic or nonsynthetic

690
00:49:40,320 --> 00:49:47,160
and maybe it's just synthetic worlds all the way down, who knows?

691
00:49:47,160 --> 00:49:56,000
So I've never really been grabbed by that simulation theory stuff.

692
00:49:56,000 --> 00:50:01,840
It's interesting to me because you've written about consciousness as well, right?

693
00:50:01,840 --> 00:50:02,840
Very little.

694
00:50:02,840 --> 00:50:09,280
I mean, I've read a lot about consciousness and some of my very best friends write a lot

695
00:50:09,280 --> 00:50:10,840
about consciousness.

696
00:50:10,840 --> 00:50:15,800
And when they give these arguments about zombies and so forth, I worry because I think I must

697
00:50:15,800 --> 00:50:23,200
be a zombie because I don't know what the missing ingredient is supposed to be.

698
00:50:23,200 --> 00:50:28,420
And I've always been suspicious of talks about, you know, philosophers have these fancy words

699
00:50:28,420 --> 00:50:34,160
for that sort of ethereal extra secret ingredient X.

700
00:50:34,160 --> 00:50:40,200
Sometimes they call it qualia, sometimes they call it sense data, it's sort of like, well,

701
00:50:40,200 --> 00:50:50,200
when you see something green, there is this green mental patch that exists somehow in

702
00:50:50,200 --> 00:50:54,840
your mind in addition to the green thing itself.

703
00:50:54,840 --> 00:50:58,040
And I've always been a bit suspicious of that.

704
00:50:58,040 --> 00:51:02,000
You know, the philosopher Gilbert Ryle talked about this and he says, well, you know, people

705
00:51:02,000 --> 00:51:09,440
talk about seeing a mental image with the mind's eye, but, you know, people never talk

706
00:51:09,440 --> 00:51:17,240
about smelling mental images of smells with the mind's nose, for example.

707
00:51:17,240 --> 00:51:22,520
We imagine popcorn, right, the smell of popcorn or buttered popcorn or whatever, but we don't

708
00:51:22,520 --> 00:51:28,540
fall into this thing like, oh, there must be this thing that the mind's nose is smelling

709
00:51:28,540 --> 00:51:30,640
in my head or something like that.

710
00:51:30,640 --> 00:51:42,160
And I think, I can't get past the feeling that when we start talking about these sense

711
00:51:42,160 --> 00:51:51,160
data or qualia or conscious experience stuff, that we aren't inventing something that isn't

712
00:51:51,160 --> 00:51:52,160
there.

713
00:51:52,160 --> 00:51:56,200
I mean, I guess Dan Dennett has a view that's similar to this.

714
00:51:56,200 --> 00:51:58,800
So I mean, this might be exactly his view.

715
00:51:58,800 --> 00:52:05,400
So maybe I'm a Danettian about this.

716
00:52:05,400 --> 00:52:09,520
Another interesting way to look at it, I have a guest, I really like having him, Chris Nebar,

717
00:52:09,520 --> 00:52:13,400
who is a neuroscientist, but he's also a Taoist.

718
00:52:13,400 --> 00:52:19,400
And his whole thing is that we have a right side mind, left side mind, and all this need

719
00:52:19,400 --> 00:52:25,120
to categorize and make sense out of stuff is the product of the left side brain.

720
00:52:25,120 --> 00:52:30,640
And the right side is more of a curious childlike kind of a brain that it doesn't matter what

721
00:52:30,640 --> 00:52:31,640
it means.

722
00:52:31,640 --> 00:52:36,360
It doesn't matter in what category fits, it's just that you're experiencing it.

723
00:52:36,360 --> 00:52:42,000
So try to make sense out of something with half of our brain, it just means that we're

724
00:52:42,000 --> 00:52:47,920
just making sense of maybe half of the thing that exists based on our own categorization

725
00:52:47,920 --> 00:52:49,920
of it.

726
00:52:49,920 --> 00:52:53,200
Yeah, that's right.

727
00:52:53,200 --> 00:52:57,400
I think you need the interplay of the two things, right?

728
00:52:57,400 --> 00:53:06,640
That is, you know, I'm immediately thinking of talking about concepts without percepts

729
00:53:06,640 --> 00:53:09,640
are empty, percepts without concepts are blind.

730
00:53:09,640 --> 00:53:15,040
So it's sort of like, and so you need that kind of organizational element, or you don't

731
00:53:15,040 --> 00:53:17,040
have any experience at all.

732
00:53:17,040 --> 00:53:23,160
But then once you get things organized, right, conceptually, perceptually, and so forth,

733
00:53:23,160 --> 00:53:29,000
you have to you have to keep interrogating the conceptual apparatus you've built for

734
00:53:29,000 --> 00:53:30,000
yourself.

735
00:53:30,000 --> 00:53:35,320
And you have to keep tearing it down and challenging it with I forget which side left or right

736
00:53:35,320 --> 00:53:36,400
is going to do that.

737
00:53:36,400 --> 00:53:40,400
But I guess that's the left, the right side of the brain is going to do that.

738
00:53:40,400 --> 00:53:42,360
But that doesn't matter.

739
00:53:42,360 --> 00:53:48,680
So you have, you know, so you need the two, this kind of interplay between the two.

740
00:53:48,680 --> 00:53:51,480
You can't let either side sort of get lazy on you.

741
00:53:51,480 --> 00:53:56,560
Yeah, now, when I talked to Chris, the whole thing is that the West has been dominant by

742
00:53:56,560 --> 00:54:01,320
the left side of the brain that, you know, we just make sense out of everything and be

743
00:54:01,320 --> 00:54:03,000
convinced that this is it.

744
00:54:03,000 --> 00:54:06,360
But obviously, we have huge amount of problems here.

745
00:54:06,360 --> 00:54:13,120
It seems like Western cultures and societies have figured the surface really well.

746
00:54:13,120 --> 00:54:16,800
But don't you dare look beneath that because we have no answer for it.

747
00:54:16,800 --> 00:54:21,960
And you're going to go rogue if you if you'd be curious a little too much about them.

748
00:54:21,960 --> 00:54:25,240
Yeah, no, people get very uncomfortable, right?

749
00:54:25,240 --> 00:54:29,840
Because people and myself included, I mean, I like, I mean, I like it that much.

750
00:54:29,840 --> 00:54:34,960
But I do take some security when things are neatly organized, I know.

751
00:54:34,960 --> 00:54:37,760
And the concepts we're throwing around, we know what we mean.

752
00:54:37,760 --> 00:54:39,360
And, you know, we know what freedom means.

753
00:54:39,360 --> 00:54:43,640
We know what a woman is, we know what this and that is.

754
00:54:43,640 --> 00:54:50,200
And when those concepts start breaking down, it's, it's an uncomfortable feeling.

755
00:54:50,200 --> 00:55:00,360
And it is true that in the West, we do get we do like to lock down concepts, right?

756
00:55:00,360 --> 00:55:01,720
You know, it's a very weird thing.

757
00:55:01,720 --> 00:55:03,800
I mean, this is I think this is related.

758
00:55:03,800 --> 00:55:06,320
But if it's not, you could say this is not related.

759
00:55:06,320 --> 00:55:13,520
But I was at one point, I was reading literature from Japanese business schools.

760
00:55:13,520 --> 00:55:16,120
I just just because I thought it was interesting.

761
00:55:16,120 --> 00:55:19,880
And there were the people that were working for 7-Eleven in Japan.

762
00:55:19,880 --> 00:55:27,200
And the question was, how do you categorize the things that you have in your store?

763
00:55:27,200 --> 00:55:28,520
What you're going to order?

764
00:55:28,520 --> 00:55:33,520
What I mean, you think about it, it would never occur to you that that is a puzzle or

765
00:55:33,520 --> 00:55:36,080
a problem that needs to be solved, right?

766
00:55:36,080 --> 00:55:41,400
But you know, if you think, say you're in 7-Eleven USA, you know, these are the things

767
00:55:41,400 --> 00:55:45,760
that we're going to keep in our store, you're going to be left behind pretty quickly.

768
00:55:45,760 --> 00:55:50,240
Because the fact that we like things need to be organized in this way today, there's

769
00:55:50,240 --> 00:55:54,580
no guarantee that you know, the next generation of people coming along or this crop of immigrants

770
00:55:54,580 --> 00:56:00,320
or whatever, or however people are changing, these people using Hiawaska are going to want

771
00:56:00,320 --> 00:56:03,960
to categorize things in the same way.

772
00:56:03,960 --> 00:56:13,240
So what the business, these business school guys were looking at 7-Eleven Japan and looking

773
00:56:13,240 --> 00:56:20,440
at ways in which they maintain fluid categorization of the concepts of the things they were keeping

774
00:56:20,440 --> 00:56:24,680
on inventory and the things they were selling and the things they were ordering.

775
00:56:24,680 --> 00:56:29,680
And a lot of that involved not just having, you know, it involved having multiple people

776
00:56:29,680 --> 00:56:33,680
with different categorizations and throwing those people together.

777
00:56:33,680 --> 00:56:36,880
And so that when you have people with very different ideas, like I mean, I guess this

778
00:56:36,880 --> 00:56:42,560
brings us back to the Lower East Side of New York in 1979, when you get people with different

779
00:56:42,560 --> 00:56:47,600
ideas and different ways of categorizing the world, something new and interesting is going

780
00:56:47,600 --> 00:56:48,960
to come out of that.

781
00:56:48,960 --> 00:56:52,360
Yeah, Japan is such an, have you been?

782
00:56:52,360 --> 00:56:59,240
Yeah, yeah, but not, I mean, not nearly enough to really be any sort of authority on it.

783
00:56:59,240 --> 00:57:06,240
Yeah, I've been once and like three years ago and all my childhood, you know, in Iran,

784
00:57:06,240 --> 00:57:10,480
I watched a lot of anime and Japan for me was like India for hippies.

785
00:57:10,480 --> 00:57:13,600
I always wanted to go and it did not disappoint.

786
00:57:13,600 --> 00:57:19,360
And I always use it as an example of how organically they are adapting to technology.

787
00:57:19,360 --> 00:57:22,920
Like you go to a temple that's been built like thousand years ago and there's a robot

788
00:57:22,920 --> 00:57:26,600
priest in the corner that you can go and talk to and everybody's like, yeah, that's how

789
00:57:26,600 --> 00:57:27,600
it is.

790
00:57:27,600 --> 00:57:35,080
Yeah, no, I mean, that's my experience too, is they have this way of blending technology

791
00:57:35,080 --> 00:57:41,240
with culture that I don't necessarily see in the United States.

792
00:57:41,240 --> 00:57:49,720
I see it in some places, but technology seems more alien to it in the United States for

793
00:57:49,720 --> 00:57:50,720
some reason.

794
00:57:50,720 --> 00:57:51,720
It's something different.

795
00:57:51,720 --> 00:57:53,920
It's something alien to broader culture.

796
00:57:53,920 --> 00:58:00,400
I mean, with exceptions like in California and techno pagans and things like that.

797
00:58:00,400 --> 00:58:01,720
Yeah, absolutely.

798
00:58:01,720 --> 00:58:06,280
How do you see this coming decade shaping our civilization?

799
00:58:06,280 --> 00:58:10,920
Because what we're going through right now this year, it seems like a dent to our civilization

800
00:58:10,920 --> 00:58:14,960
and the sense of certainty and big changes are coming.

801
00:58:14,960 --> 00:58:15,960
Seems like it.

802
00:58:15,960 --> 00:58:17,600
How do you see it?

803
00:58:17,600 --> 00:58:23,160
I see it that like every 50 years we get a revolutionary period.

804
00:58:23,160 --> 00:58:27,600
Now I hear your dog is like that.

805
00:58:27,600 --> 00:58:34,920
The last round was in the late 60s and before that around 1917 and then you go back I suppose

806
00:58:34,920 --> 00:58:39,320
1848.

807
00:58:39,320 --> 00:58:45,520
Basically twice a century we get into these revolutionary periods and then people onboard

808
00:58:45,520 --> 00:58:46,520
new ideas.

809
00:58:46,520 --> 00:58:47,520
You get these conflicts.

810
00:58:47,520 --> 00:58:49,600
I think they're healthy conflicts.

811
00:58:49,600 --> 00:58:53,000
Well, I mean, I need to be careful.

812
00:58:53,000 --> 00:58:55,240
I mean, they're not always healthy conflicts.

813
00:58:55,240 --> 00:58:59,520
Sometimes they lead to like massive scale genocides.

814
00:58:59,520 --> 00:59:00,720
I know exactly what you mean.

815
00:59:00,720 --> 00:59:01,920
I totally agree with you.

816
00:59:01,920 --> 00:59:02,920
Big picture-wise.

817
00:59:02,920 --> 00:59:03,920
Yeah.

818
00:59:03,920 --> 00:59:04,920
Yeah.

819
00:59:04,920 --> 00:59:05,920
Yeah.

820
00:59:05,920 --> 00:59:06,920
Yeah.

821
00:59:06,920 --> 00:59:07,920
Yeah.

822
00:59:07,920 --> 00:59:08,920
Yeah.

823
00:59:08,920 --> 00:59:09,920
Yeah.

824
00:59:09,920 --> 00:59:16,680
We're seeing a lot of changes and right now I'm spending a lot of time in the decentralized

825
00:59:16,680 --> 00:59:26,000
finance little subculture of the cryptocurrency business.

826
00:59:26,000 --> 00:59:33,240
These are the people that are basically building platforms that are going to eliminate banks.

827
00:59:33,240 --> 00:59:36,120
They're going to eliminate insurance companies.

828
00:59:36,120 --> 00:59:38,560
They are going to eliminate lending institutions.

829
00:59:38,560 --> 00:59:43,720
They are going to eliminate the money transfer.

830
00:59:43,720 --> 00:59:48,400
You want to mention the name of some of them is XRP one of them?

831
00:59:48,400 --> 00:59:49,400
No.

832
00:59:49,400 --> 00:59:50,400
XRP is-

833
00:59:50,400 --> 00:59:51,400
Not really decentralized.

834
00:59:51,400 --> 00:59:52,400
Yeah.

835
00:59:52,400 --> 01:00:01,280
It's the evil one because they sold their soul and they're providing a platform for

836
01:00:01,280 --> 01:00:05,440
transferring information between big banks.

837
01:00:05,440 --> 01:00:15,440
So let's start with everyone knows Bitcoin and then the second one people might know

838
01:00:15,440 --> 01:00:19,360
is Ethereum and the coin is called ETH.

839
01:00:19,360 --> 01:00:24,680
What is interesting about Ethereum is that- let me start with Bitcoin.

840
01:00:24,680 --> 01:00:30,120
If you think of Bitcoin as being this ledger that we all share, it's all over the world.

841
01:00:30,120 --> 01:00:34,080
We can see who has what or what account number has what.

842
01:00:34,080 --> 01:00:37,320
So it's this giant shared ledger and if you think about it, what it does is it gives us

843
01:00:37,320 --> 01:00:43,560
a way of keeping track of money, doing the function that a bank does with that and also

844
01:00:43,560 --> 01:00:46,240
allows us to transfer money.

845
01:00:46,240 --> 01:00:55,520
So it eliminates one institution of human trust, so that's the key thing.

846
01:00:55,520 --> 01:00:56,520
Absolutely.

847
01:00:56,520 --> 01:01:02,600
Centralized trust, typically involving a big corporation or a big bank like Chase or JPMorgan

848
01:01:02,600 --> 01:01:04,720
and Chase or whatever.

849
01:01:04,720 --> 01:01:12,200
Now when Ethereum came along, this kid who was then 18 years old named Vitalik Buterin-

850
01:01:12,200 --> 01:01:15,600
Who I met once in Toronto.

851
01:01:15,600 --> 01:01:16,600
You met him?

852
01:01:16,600 --> 01:01:17,680
I met him once.

853
01:01:17,680 --> 01:01:26,200
I used to co-organize Toronto transhumanist meetups and I came across this man, Corey

854
01:01:26,200 --> 01:01:27,200
something.

855
01:01:27,200 --> 01:01:28,200
He passed away unfortunately.

856
01:01:28,200 --> 01:01:32,840
He was one of the early investors in Ethereum and they had this place called Decentral on

857
01:01:32,840 --> 01:01:40,360
Spadina and King and I just stopped by to see the place and he invited us to organize

858
01:01:40,360 --> 01:01:45,800
it there and Vitalik was there and this was like a year before Ethereum was released and

859
01:01:45,800 --> 01:01:50,440
really quickly just said hi, shook his hand and that was it.

860
01:01:50,440 --> 01:01:51,440
Yeah.

861
01:01:51,440 --> 01:01:55,600
No, I mean you should listen to his podcast if you could get him on.

862
01:01:55,600 --> 01:01:56,600
He's amazing.

863
01:01:56,600 --> 01:02:01,280
If you look at some of the podcasts he does, he's very kind of somewhat robotic in style

864
01:02:01,280 --> 01:02:02,760
but everything he says is like-

865
01:02:02,760 --> 01:02:04,960
I would love to have them.

866
01:02:04,960 --> 01:02:07,560
Yeah, absolutely.

867
01:02:07,560 --> 01:02:11,680
So I actually got in on there.

868
01:02:11,680 --> 01:02:13,080
I had some Bitcoin.

869
01:02:13,080 --> 01:02:17,080
I was Bitcoin mining at the time and then I read about this project and it said, well

870
01:02:17,080 --> 01:02:23,400
like if you send us a Bitcoin, we'll give you 2000 ETH.

871
01:02:23,400 --> 01:02:27,160
I said, all right, what the hell, because like a Bitcoin was not worth anything at the

872
01:02:27,160 --> 01:02:28,160
time.

873
01:02:28,160 --> 01:02:29,160
It was like $250.

874
01:02:29,160 --> 01:02:37,760
I said, what the hell, I'll get sent $250, I get 2000 ETH.

875
01:02:37,760 --> 01:02:43,840
Because the project sounded amazing because the idea is just as you can record this information

876
01:02:43,840 --> 01:02:48,800
on a Bitcoin blockchain and it's there, the whole history of this ledger is there for

877
01:02:48,800 --> 01:02:56,840
everyone to look at, set in stone, Vitalik pointed out that, well, I can encode any amount

878
01:02:56,840 --> 01:02:57,840
of information.

879
01:02:57,840 --> 01:03:02,000
I'm going to build a blockchain where you could encode enough information so that you

880
01:03:02,000 --> 01:03:05,040
could encode in any computer program.

881
01:03:05,040 --> 01:03:06,920
Tokenize everything basically.

882
01:03:06,920 --> 01:03:11,800
Tokenize everything and not only that, but you could activate these things so we could

883
01:03:11,800 --> 01:03:15,320
build a contract that would execute itself.

884
01:03:15,320 --> 01:03:16,320
Smart contracts.

885
01:03:16,320 --> 01:03:17,320
You wouldn't need, right.

886
01:03:17,320 --> 01:03:18,320
So there you go.

887
01:03:18,320 --> 01:03:23,800
You don't need an organization of trust in order to hold the money for us or even make

888
01:03:23,800 --> 01:03:26,240
sure that the contract is executed.

889
01:03:26,240 --> 01:03:29,080
It just automatically gets executed.

890
01:03:29,080 --> 01:03:37,160
And given that the language is, as we say, Turing complete, any contract that you can

891
01:03:37,160 --> 01:03:42,040
write that's computable can be put into the blockchain.

892
01:03:42,040 --> 01:03:50,040
And people for the, I guess, Ethereum just had its fifth birthday a couple of weeks ago

893
01:03:50,040 --> 01:03:53,640
and it's taken some time for it to find its feet.

894
01:03:53,640 --> 01:04:02,480
But right now there is a segment of crypto called decentralized finance or DeFi in which

895
01:04:02,480 --> 01:04:07,280
people are now taking these tools and they are really going to town with these tools

896
01:04:07,280 --> 01:04:10,520
and really doing things like issuing loans.

897
01:04:10,520 --> 01:04:14,360
People are now putting mortgages on the blockchain.

898
01:04:14,360 --> 01:04:22,000
People are engaged in, well, obviously fancy things like futures and futures markets and

899
01:04:22,000 --> 01:04:24,280
so forth.

900
01:04:24,280 --> 01:04:26,740
Insurance.

901
01:04:26,740 --> 01:04:34,560
So the thinking is that eventually people are going to wake up to the idea that we don't

902
01:04:34,560 --> 01:04:42,920
need to pay some middleman to sit in a big fancy bank in a giant building on Fifth Avenue

903
01:04:42,920 --> 01:04:48,160
to do all this stuff and to hold our money and to invest that money in a carbon bubble

904
01:04:48,160 --> 01:04:51,720
or the military industrial complex or anything like that.

905
01:04:51,720 --> 01:04:57,520
You and I and all of our friends and people like us can just submit small amounts of money

906
01:04:57,520 --> 01:05:03,800
into these contracts and then we basically build our own hedge fund, for example.

907
01:05:03,800 --> 01:05:06,980
And we have like computers that are doing arbitrage for us.

908
01:05:06,980 --> 01:05:10,080
We don't need big fancy companies to do that.

909
01:05:10,080 --> 01:05:16,040
And so just as you don't see at Tower Records anymore, you don't have to go to Tower Records

910
01:05:16,040 --> 01:05:26,600
to buy music, it's very rare to see giant bookstores these days, you buy all that stuff

911
01:05:26,600 --> 01:05:39,440
on Amazon, Lord willing, these corporations, giant banks, giant insurance companies, they're

912
01:05:39,440 --> 01:05:42,040
all going to go the way of Tower Records as well.

913
01:05:42,040 --> 01:05:45,080
I totally agree.

914
01:05:45,080 --> 01:05:48,000
I had just briefly, I had John McAfee on.

915
01:05:48,000 --> 01:05:51,560
I saw that interview, yeah, that was hilarious.

916
01:05:51,560 --> 01:05:52,560
That was a trip.

917
01:05:52,560 --> 01:06:03,160
I was amazed that you were able to get into a word edgewise though because he'll keep

918
01:06:03,160 --> 01:06:04,160
rolling.

919
01:06:04,160 --> 01:06:05,160
Oh yeah.

920
01:06:05,160 --> 01:06:12,480
And in the beginning, he was like, just so you know, I keep drinking throughout the interview.

921
01:06:12,480 --> 01:06:19,640
So his thing, his thesis was that Bitcoin is going to go down and will do any other

922
01:06:19,640 --> 01:06:23,760
cryptocurrency that does not have anonymity into it.

923
01:06:23,760 --> 01:06:27,000
So I think he has a new coin.

924
01:06:27,000 --> 01:06:28,600
I'm not recommending it.

925
01:06:28,600 --> 01:06:29,600
He said Monero.

926
01:06:29,600 --> 01:06:34,280
Ghost also is something that I think he has developed himself or with a bunch of people

927
01:06:34,280 --> 01:06:38,800
he's been backing, but Monero, because of anonymity part of it.

928
01:06:38,800 --> 01:06:40,160
But I guess that goes back to libertarianism.

929
01:06:40,160 --> 01:06:43,480
That's more important to him than it is to us.

930
01:06:43,480 --> 01:06:45,920
But yeah, well, that'll be interesting.

931
01:06:45,920 --> 01:06:47,440
That's entirely possible.

932
01:06:47,440 --> 01:06:58,120
That's entirely possible, in which case, yeah, these coins that do a better job of mixing

933
01:06:58,120 --> 01:06:59,920
up the transactions.

934
01:06:59,920 --> 01:07:07,680
So for example, right now, it's very easy to decipher our transactions using Bitcoin

935
01:07:07,680 --> 01:07:09,180
because here's an account.

936
01:07:09,180 --> 01:07:14,320
It went from me to this account and people figure out it's mine, they figure out it's

937
01:07:14,320 --> 01:07:18,280
yours, and pretty soon they can reconstruct a network.

938
01:07:18,280 --> 01:07:23,120
But when you get to things like Monero, it's basically mixing the transactions up.

939
01:07:23,120 --> 01:07:26,760
So you don't really know who it's coming from or who it's going to.

940
01:07:26,760 --> 01:07:27,760
Excellent.

941
01:07:27,760 --> 01:07:29,240
Well, Peter, this was awesome.

942
01:07:29,240 --> 01:07:34,160
I was looking forward to this conversation and definitely didn't disappoint.

943
01:07:34,160 --> 01:07:40,560
What is next for you and where can our audience follow your work?

944
01:07:40,560 --> 01:07:42,760
You mentioned you were working on two projects.

945
01:07:42,760 --> 01:07:45,800
Yeah, let's see.

946
01:07:45,800 --> 01:08:03,240
You know, I have some basic website that just keeps track of the basic stuff.

947
01:08:03,240 --> 01:08:07,840
And I've just created a new Twitter account.

948
01:08:07,840 --> 01:08:12,680
And again, my extended mine is kind of offline here right now.

949
01:08:12,680 --> 01:08:15,080
So let's just have people start there.

950
01:08:15,080 --> 01:08:20,400
I think, you know, they know how to use the internet, they can find me.

951
01:08:20,400 --> 01:08:21,400
Excellent.

952
01:08:21,400 --> 01:08:26,200
Let me ask you, I actually look for you on Twitter because the final product you've seen

953
01:08:26,200 --> 01:08:30,760
my podcast, I put the Twitter handle on the bottom, I couldn't find yours.

954
01:08:30,760 --> 01:08:34,560
I think I found a post from a couple of years ago, but your account was deleted.

955
01:08:34,560 --> 01:08:37,320
So I'm going to add that to the notes after the show.

956
01:08:37,320 --> 01:08:41,800
Yeah, I think the Twitter account I'm using right now is brand new.

957
01:08:41,800 --> 01:08:45,520
I have like 10 followers, so you guys can help me out with this.

958
01:08:45,520 --> 01:08:46,520
Let me see.

959
01:08:46,520 --> 01:08:47,520
I think it's...

960
01:08:47,520 --> 01:08:49,880
Wait, I'm going to check right now.

961
01:08:49,880 --> 01:08:57,360
But the name is EJ Spode, as in letter E, letter J, Spode, S-P-O-D-E.

962
01:08:57,360 --> 01:09:09,440
And I'm just checking here right now to see if there is some underscore involved in that.

963
01:09:09,440 --> 01:09:11,960
EJ underscores Spode.

964
01:09:11,960 --> 01:09:22,960
Also, I have a couple of papers online under that pseudonym as well.

965
01:09:22,960 --> 01:09:34,200
I have a book review of the Kingdom of Language by Tom Wolfe and a paper actually, it's on

966
01:09:34,200 --> 01:09:41,640
Ethereum and this issue of trust and the limits of decentralized trust.

967
01:09:41,640 --> 01:09:48,280
And a couple of other papers under that name, but I've just fired up this Twitter account,

968
01:09:48,280 --> 01:09:50,840
EJ underscores Spode.

969
01:09:50,840 --> 01:09:55,000
Yeah, tell your listeners or watchers to add it so I have...

970
01:09:55,000 --> 01:10:00,200
It's really embarrassing because I only have like 10 followers at this point.

971
01:10:00,200 --> 01:10:05,080
Well, and Rome wasn't built overnight, so...

972
01:10:05,080 --> 01:10:09,720
Let me ask you the final question I ask all my guests, that if you come across an intelligent

973
01:10:09,720 --> 01:10:14,680
alien from a different civilization, what would you say is the worst thing humanity

974
01:10:14,680 --> 01:10:19,640
has done and what would you say is our greatest achievement?

975
01:10:19,640 --> 01:10:28,520
See, I need to watch your podcast to the very end because then I would be prepared for this.

976
01:10:28,520 --> 01:10:34,680
I'm glad you didn't because just getting caught off guard is one of the best moments of each

977
01:10:34,680 --> 01:10:35,680
episode.

978
01:10:35,680 --> 01:10:36,680
Yeah.

979
01:10:36,680 --> 01:10:37,680
So, okay.

980
01:10:37,680 --> 01:10:40,360
So, it's the best and worst of humanity.

981
01:10:40,360 --> 01:10:44,520
That's what I have to explain to this alien.

982
01:10:44,520 --> 01:10:56,560
Well, I think that the worst thing is that we imagine ourselves as being somehow above

983
01:10:56,560 --> 01:11:04,160
the natural realm and that we are somehow alien to the natural realm or we're somehow

984
01:11:04,160 --> 01:11:09,960
different from it.

985
01:11:09,960 --> 01:11:20,600
And maybe that's also the best thing because it means we're questioning who we are and

986
01:11:20,600 --> 01:11:26,100
even if we're getting it wrong and even if we're thinking we're special in a way that

987
01:11:26,100 --> 01:11:29,920
we're not, I mean, at least we're asking the question.

988
01:11:29,920 --> 01:11:35,560
So, the best thing is also the worst thing and let the alien ponder that and sort it

989
01:11:35,560 --> 01:11:36,560
out.

990
01:11:36,560 --> 01:11:57,360
Awesome.

991
01:12:06,560 --> 01:12:08,620
you

