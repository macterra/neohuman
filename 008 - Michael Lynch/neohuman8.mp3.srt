1
00:00:00,000 --> 00:00:01,780
One problem that we saw with the social bot

2
00:00:01,780 --> 00:00:03,580
is that if we allow it to teach itself,

3
00:00:04,580 --> 00:00:05,820
it becomes a disaster.

4
00:00:05,820 --> 00:00:06,900
It could be a disaster.

5
00:00:06,900 --> 00:00:09,780
It'll end up like the rest of us bastards, right?

6
00:00:09,780 --> 00:00:14,780
Hello, and welcome to the eighth episode

7
00:00:19,240 --> 00:00:20,660
of Neo Human Podcast.

8
00:00:20,660 --> 00:00:24,260
I'm Aga Bahari at Agologist on Twitter and Instagram,

9
00:00:24,260 --> 00:00:29,260
and you can follow the show on iTunes or liveinlimbo.com.

10
00:00:29,260 --> 00:00:32,140
With me today is Professor Michael Lynch.

11
00:00:32,140 --> 00:00:34,380
Thank you so much for being with us, Michael.

12
00:00:34,380 --> 00:00:35,660
Well, thanks for having me.

13
00:00:35,660 --> 00:00:36,500
My pleasure.

14
00:00:37,380 --> 00:00:39,780
I think it would be much better than me introducing you

15
00:00:39,780 --> 00:00:41,220
if you introduce yourself,

16
00:00:41,220 --> 00:00:43,020
and tell us a little bit about your background

17
00:00:43,020 --> 00:00:44,980
and what you're doing now.

18
00:00:44,980 --> 00:00:48,940
So I'm a philosopher by training and by profession.

19
00:00:48,940 --> 00:00:51,300
I teach at the University of Connecticut,

20
00:00:51,300 --> 00:00:54,220
where I also direct the Humanities Institute,

21
00:00:54,220 --> 00:00:59,020
and I'm also the PI on a large research project

22
00:00:59,020 --> 00:01:03,500
on public discourse and the barriers to public discourse

23
00:01:03,500 --> 00:01:05,740
and concepts like intellectual humility.

24
00:01:06,740 --> 00:01:08,460
And I'm also the author of this recent book,

25
00:01:08,460 --> 00:01:09,460
The Internet of Us,

26
00:01:09,460 --> 00:01:14,180
which is the book that I just put out from Norton.

27
00:01:15,140 --> 00:01:17,220
Yeah, and that's the reason that I got to know you.

28
00:01:17,220 --> 00:01:18,820
Right, right.

29
00:01:18,820 --> 00:01:23,820
So yeah, and that book is what we might,

30
00:01:25,180 --> 00:01:28,500
say it's a bit of philosophy on a topic

31
00:01:28,500 --> 00:01:30,540
that I think philosophers haven't,

32
00:01:30,540 --> 00:01:31,700
well, they haven't ignored,

33
00:01:31,700 --> 00:01:35,700
they haven't really focused on as much as I think they might,

34
00:01:35,700 --> 00:01:39,340
which is the philosophical implications

35
00:01:39,340 --> 00:01:41,900
of information technology, you might say.

36
00:01:41,900 --> 00:01:44,020
That's what this book is about.

37
00:01:44,020 --> 00:01:46,380
And philosophical implications,

38
00:01:46,380 --> 00:01:50,100
particularly with regard to how we understand knowledge,

39
00:01:50,100 --> 00:01:55,100
because not only do we live in the so-called information age,

40
00:01:55,260 --> 00:01:57,700
we also live, as many people say, in the knowledge age.

41
00:01:57,700 --> 00:01:59,100
We talk about the knowledge economy

42
00:01:59,100 --> 00:02:00,500
and knowledge workers, right?

43
00:02:00,500 --> 00:02:04,980
And we think about the fact that people's ability

44
00:02:04,980 --> 00:02:08,460
to get knowledge and become trained as knowledge seekers,

45
00:02:08,460 --> 00:02:10,100
that is, have a higher education,

46
00:02:10,100 --> 00:02:14,260
impacts their life in the economy economy.

47
00:02:14,260 --> 00:02:17,900
So knowledge is sort of a central topic for us in our lives.

48
00:02:17,900 --> 00:02:19,340
And it's partly a central topic,

49
00:02:19,340 --> 00:02:22,820
just because we do have these incredible devices

50
00:02:22,820 --> 00:02:26,500
that we all use for accessing information.

51
00:02:26,500 --> 00:02:28,260
So it seemed to me that,

52
00:02:29,380 --> 00:02:31,300
as somebody who works on epistemology,

53
00:02:31,300 --> 00:02:32,660
which is the study of knowledge,

54
00:02:32,660 --> 00:02:36,020
that it was high time that we started thinking

55
00:02:36,020 --> 00:02:39,300
as philosophers and as intellectuals

56
00:02:39,300 --> 00:02:44,300
about what these changes and how we distribute,

57
00:02:44,580 --> 00:02:47,260
access and produce knowledge,

58
00:02:47,260 --> 00:02:49,460
because of information technology, these changes,

59
00:02:49,460 --> 00:02:52,580
how these would actually affect our understanding

60
00:02:52,580 --> 00:02:54,420
of knowledge itself and its value.

61
00:02:54,420 --> 00:02:57,300
Would you say that our philosophy in general

62
00:02:57,300 --> 00:03:01,140
is in need of updates because of our technology?

63
00:03:01,140 --> 00:03:02,820
Yeah, I think it is.

64
00:03:02,820 --> 00:03:07,820
I mean, the book is divided into two parts.

65
00:03:07,900 --> 00:03:10,020
In one part, I talk about how certain old,

66
00:03:10,020 --> 00:03:12,420
classic philosophical problems,

67
00:03:12,420 --> 00:03:14,900
problems that have been around for thousands of years,

68
00:03:14,900 --> 00:03:16,260
or as my students say,

69
00:03:16,260 --> 00:03:19,420
that I've been discussed for thousands of years,

70
00:03:19,420 --> 00:03:21,620
sort of old philosophical problems,

71
00:03:21,620 --> 00:03:26,620
have been reached, sort of given new life

72
00:03:27,500 --> 00:03:30,020
by our digital form of life.

73
00:03:30,020 --> 00:03:32,540
I talk about our life in the infosphere

74
00:03:32,540 --> 00:03:35,220
as a sort of what Wittgenstein would call form of life.

75
00:03:35,220 --> 00:03:37,540
That is, we have a sort of ways of acting now

76
00:03:37,540 --> 00:03:40,460
due to how we interact with information technology

77
00:03:40,460 --> 00:03:43,700
that we don't even sort of notice.

78
00:03:43,700 --> 00:03:45,180
These are sort of behaviors and habits

79
00:03:45,180 --> 00:03:46,700
that we engage in on a daily basis,

80
00:03:46,700 --> 00:03:48,500
having to do with access and information

81
00:03:48,500 --> 00:03:50,100
that we almost never notice anymore.

82
00:03:50,100 --> 00:03:52,780
And that fact, the fact that these things

83
00:03:52,780 --> 00:03:54,780
have become so much part of our life

84
00:03:54,780 --> 00:03:57,180
that is these devices and so forth,

85
00:03:57,180 --> 00:04:00,340
and our analytic techniques

86
00:04:00,340 --> 00:04:02,260
that allow us to use these devices,

87
00:04:03,500 --> 00:04:05,420
the fact that they've become so much part of our life

88
00:04:05,420 --> 00:04:10,020
has made these old problems new again.

89
00:04:10,020 --> 00:04:12,020
And so let me give you an example of that.

90
00:04:13,340 --> 00:04:16,220
An old classic philosophical problem is

91
00:04:16,220 --> 00:04:21,220
how do we rationally resolve disagreements

92
00:04:23,020 --> 00:04:28,020
between each other over competing values and facts?

93
00:04:30,060 --> 00:04:32,380
That's something that's been around for a long time.

94
00:04:32,380 --> 00:04:35,380
How do we rationally resolve our differences

95
00:04:35,380 --> 00:04:37,740
of opinion over values, for example?

96
00:04:37,740 --> 00:04:39,340
Can we do that?

97
00:04:39,340 --> 00:04:40,340
Some people might say yes,

98
00:04:40,340 --> 00:04:43,460
some people might say cynically no, we can't.

99
00:04:43,460 --> 00:04:44,620
Politics at the end of the day

100
00:04:44,620 --> 00:04:47,500
has to become war by other means and so forth.

101
00:04:47,500 --> 00:04:49,100
Well, the way in which information technology

102
00:04:49,100 --> 00:04:52,780
is impacting this is really sort of funny and curious.

103
00:04:52,780 --> 00:04:55,140
We have all, we have, you and I,

104
00:04:55,140 --> 00:04:58,700
by virtue of just accessing our phone,

105
00:04:58,700 --> 00:05:01,460
have tremendous amounts of information at our disposal,

106
00:05:01,460 --> 00:05:05,620
more than most humans have ever had combined.

107
00:05:05,620 --> 00:05:06,460
Right? Absolutely.

108
00:05:06,460 --> 00:05:07,980
Is that absolutely the case?

109
00:05:07,980 --> 00:05:12,180
You're doubling the knowledge of humanity every year.

110
00:05:12,180 --> 00:05:14,580
Indeed, indeed, and maybe even,

111
00:05:14,580 --> 00:05:17,740
that might even be underestimating it.

112
00:05:19,420 --> 00:05:20,740
Just simply because the,

113
00:05:20,740 --> 00:05:22,540
I mean, partly this is a sort of inside,

114
00:05:22,540 --> 00:05:24,740
but the use of data analytics,

115
00:05:26,300 --> 00:05:28,700
our sophistication in those methods

116
00:05:28,700 --> 00:05:33,700
is really just shot up in the last couple of years,

117
00:05:33,700 --> 00:05:37,060
I think, and that is,

118
00:05:37,060 --> 00:05:39,220
it may be increasing even more than that,

119
00:05:39,220 --> 00:05:42,860
but let's say it's doubling it.

120
00:05:42,860 --> 00:05:47,220
Now, what you might think that that sort of from a sort of,

121
00:05:47,220 --> 00:05:48,540
if you just sort of heard that,

122
00:05:48,540 --> 00:05:50,220
you heard somebody like living

123
00:05:51,620 --> 00:05:53,980
just even 50 years ago heard that, right?

124
00:05:53,980 --> 00:05:55,820
And they could believe it.

125
00:05:55,820 --> 00:05:57,100
They might think, wow,

126
00:05:58,140 --> 00:06:01,500
then we should be closer to solving certain problems, right?

127
00:06:02,380 --> 00:06:03,900
We should be closer to solving

128
00:06:03,900 --> 00:06:05,380
not just certain technological problems,

129
00:06:05,380 --> 00:06:08,380
but also maybe policy problems.

130
00:06:08,380 --> 00:06:09,900
I mean, if we have all this information,

131
00:06:09,900 --> 00:06:13,340
then surely if we put our best minds at it,

132
00:06:13,340 --> 00:06:15,060
we'll come to greater agreement on matters

133
00:06:15,060 --> 00:06:17,180
like climate change or the environment,

134
00:06:17,180 --> 00:06:20,100
as they might've said years ago, right?

135
00:06:21,940 --> 00:06:24,420
But what we're finding actually is that

136
00:06:24,420 --> 00:06:28,980
the disagreements in policy

137
00:06:28,980 --> 00:06:31,620
and disagreements over matters of fact even

138
00:06:31,620 --> 00:06:34,260
are becoming more and more entrenched,

139
00:06:34,260 --> 00:06:37,820
even though we have access to greater amounts of information.

140
00:06:37,820 --> 00:06:38,780
And why is that?

141
00:06:38,780 --> 00:06:40,540
Well, partly it's due to a fact

142
00:06:40,540 --> 00:06:43,660
that a lot of people have commented on in the digital age,

143
00:06:43,660 --> 00:06:46,060
which is that when we access information online,

144
00:06:48,500 --> 00:06:50,180
it's not just all the information.

145
00:06:50,180 --> 00:06:53,180
Because there's so much information, we have to sort it.

146
00:06:54,140 --> 00:06:56,660
We have to make decisions, who we're gonna check,

147
00:06:56,660 --> 00:06:58,300
what sources we're gonna rely on,

148
00:06:59,740 --> 00:07:02,180
what friends we're gonna accept, so to speak.

149
00:07:03,380 --> 00:07:04,700
And that puts us in,

150
00:07:04,700 --> 00:07:06,460
and this is the point that a lot of people comment on,

151
00:07:06,460 --> 00:07:09,580
is that puts us in sort of information bubbles.

152
00:07:09,580 --> 00:07:11,180
But the point that I think is new,

153
00:07:11,180 --> 00:07:12,660
and this relates to this sort of perennial

154
00:07:12,660 --> 00:07:15,460
philosophical problem, is that these information bubbles

155
00:07:15,460 --> 00:07:17,700
have gotten to the point where our disagreements

156
00:07:17,700 --> 00:07:21,420
are no longer just over values.

157
00:07:21,420 --> 00:07:23,660
They're no longer just over the facts.

158
00:07:23,660 --> 00:07:26,020
They're actually over whose sources

159
00:07:27,860 --> 00:07:30,860
are the most reliable for deciding what the facts are,

160
00:07:30,860 --> 00:07:35,180
and what even a fact is, is I think an issue right now.

161
00:07:35,180 --> 00:07:38,220
So that's essentially an old philosophical problem,

162
00:07:38,220 --> 00:07:40,900
is being given new life by the very technology

163
00:07:40,900 --> 00:07:43,620
that we might have thought would have solved it.

164
00:07:43,620 --> 00:07:45,300
It's the concept of the more we know,

165
00:07:45,300 --> 00:07:47,700
the more we know we don't know.

166
00:07:47,700 --> 00:07:50,820
Well, in a sense, in one sense, that's a good,

167
00:07:50,820 --> 00:07:53,020
yeah, I mean, in one way, that's right.

168
00:07:53,020 --> 00:07:54,500
Another way of thinking about it, though,

169
00:07:54,500 --> 00:07:57,500
is that, and this is something, the subtitle of my book

170
00:07:57,500 --> 00:07:59,740
is Knowing More and Understanding Less

171
00:07:59,740 --> 00:08:01,060
in the Age of Big Data.

172
00:08:01,060 --> 00:08:02,820
And I think we actually do know more.

173
00:08:02,820 --> 00:08:03,940
We don't just have more information,

174
00:08:03,940 --> 00:08:05,540
we do actually know more.

175
00:08:05,540 --> 00:08:08,860
I often feel without my phone, I'm just 100% dumber.

176
00:08:09,740 --> 00:08:14,100
But the sense in which we know more

177
00:08:14,100 --> 00:08:15,620
is a very minimal sense.

178
00:08:15,620 --> 00:08:18,740
When we know from, most of the times,

179
00:08:18,740 --> 00:08:21,820
by using apps, for example, not just Google searching,

180
00:08:21,820 --> 00:08:24,260
but actually using an app, let's say,

181
00:08:24,260 --> 00:08:26,740
to perhaps an interactive app,

182
00:08:30,300 --> 00:08:32,460
the sense in which I know is the minimal sense

183
00:08:32,460 --> 00:08:35,420
in which, if I'm lucky enough to get accurate information,

184
00:08:35,420 --> 00:08:37,940
form an accurate belief from using that app

185
00:08:37,940 --> 00:08:40,380
about some subject matter, let's say the weather.

186
00:08:42,540 --> 00:08:45,260
Then that belief will turn out to be knowledge.

187
00:08:45,260 --> 00:08:48,820
If it's accurate, it's based on a sort of reliable source.

188
00:08:51,500 --> 00:08:54,580
And that's the minimal sense of knowledge

189
00:08:54,580 --> 00:08:55,940
in which we gain knowledge

190
00:08:56,860 --> 00:08:59,700
from interacting with digital sources.

191
00:08:59,700 --> 00:09:02,020
And in that sense, we know more.

192
00:09:02,020 --> 00:09:07,020
But the problem is that, because we do know more in that way,

193
00:09:07,340 --> 00:09:09,060
we have more reliable sources checked,

194
00:09:09,060 --> 00:09:12,580
that actually makes us overconfident

195
00:09:12,580 --> 00:09:14,380
that we actually know more than we do.

196
00:09:15,540 --> 00:09:17,780
I think sometimes we don't actually think,

197
00:09:17,780 --> 00:09:21,580
and we should think, that we know less than we think we do.

198
00:09:21,580 --> 00:09:22,820
But I don't think that often happens.

199
00:09:22,820 --> 00:09:24,900
I think we do know a little bit more,

200
00:09:24,900 --> 00:09:27,060
but a little bit of knowledge is a dangerous thing.

201
00:09:27,060 --> 00:09:29,060
And what happens is that we don't know

202
00:09:29,060 --> 00:09:30,260
what happens is that people think

203
00:09:30,260 --> 00:09:31,820
that they know even more than they do,

204
00:09:31,820 --> 00:09:34,540
just because they have this information right at hand

205
00:09:34,540 --> 00:09:36,380
and they've been able to use it so far

206
00:09:36,380 --> 00:09:39,700
to find out, let's say, what the weather is,

207
00:09:39,700 --> 00:09:44,700
or even what the level on a particular,

208
00:09:48,140 --> 00:09:49,940
I have an app on my phone that allows me

209
00:09:49,940 --> 00:09:53,980
to detect the level of a piece of wood,

210
00:09:53,980 --> 00:09:55,740
let's say, it's a carpentry app.

211
00:09:55,740 --> 00:09:58,100
That's an amazing little app, by the way.

212
00:09:58,100 --> 00:09:59,580
What is it called?

213
00:09:59,580 --> 00:10:01,060
Carpentry level.

214
00:10:01,060 --> 00:10:03,660
There's about 10 of them, but there's,

215
00:10:03,660 --> 00:10:07,060
so I can know things, like this one piece of wood is level,

216
00:10:07,060 --> 00:10:08,660
by using that app.

217
00:10:08,660 --> 00:10:11,220
But I shouldn't think that that fact

218
00:10:11,220 --> 00:10:13,380
makes me an expert carpenter either.

219
00:10:14,820 --> 00:10:16,660
What do you think it takes for a knowledge

220
00:10:16,660 --> 00:10:18,780
based on evidence to become a policy?

221
00:10:18,780 --> 00:10:21,860
I'm living in Canada, and what we're witnessing

222
00:10:21,860 --> 00:10:24,140
in the United States is that there are senators

223
00:10:24,140 --> 00:10:26,900
and congressmen who are arguing that

224
00:10:26,900 --> 00:10:29,980
there is no global warming because there's a snow outside.

225
00:10:29,980 --> 00:10:30,820
Yes.

226
00:10:30,820 --> 00:10:34,420
So those are the people who are basically

227
00:10:34,420 --> 00:10:36,900
are affecting the progress of technology

228
00:10:36,900 --> 00:10:39,820
and knowledge and everything because of public policy.

229
00:10:39,820 --> 00:10:43,100
What do you think it will take for the people

230
00:10:43,100 --> 00:10:46,900
like yourself who are creating the new ways

231
00:10:46,900 --> 00:10:50,460
of understanding knowledge to convince the politicians

232
00:10:50,460 --> 00:10:53,620
who are mostly conservative and business oriented

233
00:10:53,620 --> 00:10:57,180
to accept the facts so we can all move forward?

234
00:10:57,180 --> 00:11:00,180
Well, that's a really, really good question.

235
00:11:00,180 --> 00:11:01,780
I guess it's a sort of great follow up

236
00:11:01,780 --> 00:11:04,380
to the point that I had raised is that right now

237
00:11:04,380 --> 00:11:07,940
we are disagreeing over our sources of evidence

238
00:11:07,940 --> 00:11:10,180
and even over whether evidence is important.

239
00:11:11,380 --> 00:11:13,020
So how do we move forward from there?

240
00:11:13,020 --> 00:11:15,260
I mean, on the surface, at first glance,

241
00:11:15,260 --> 00:11:18,020
it seems impossible because once we disagree

242
00:11:18,020 --> 00:11:20,100
over whether evidence is important,

243
00:11:20,100 --> 00:11:22,300
what sort of evidence could I possibly give you

244
00:11:22,300 --> 00:11:23,220
to think that it is?

245
00:11:23,220 --> 00:11:24,420
Yes, exactly.

246
00:11:24,420 --> 00:11:25,260
All right.

247
00:11:25,260 --> 00:11:28,420
So I think what we need to do is shift

248
00:11:28,420 --> 00:11:29,660
slightly different tactic.

249
00:11:29,660 --> 00:11:33,380
I think what we need to do is to convince people

250
00:11:33,380 --> 00:11:36,820
that share at least some sense of democratic commitment

251
00:11:37,900 --> 00:11:41,940
to a sense of sovereignty of the public.

252
00:11:41,940 --> 00:11:43,740
And not all the folks that I'm gonna,

253
00:11:43,740 --> 00:11:46,020
what you might have in mind might be committed to this.

254
00:11:46,020 --> 00:11:47,700
But some of them, let's say are,

255
00:11:47,700 --> 00:11:50,500
they do think that democracy is all things in equal,

256
00:11:50,500 --> 00:11:55,500
better form of government than alternative forms.

257
00:11:55,620 --> 00:11:58,340
If they do think that, then I think what I can do

258
00:11:58,340 --> 00:12:00,980
is make an argument that appealing to evidence

259
00:12:00,980 --> 00:12:05,180
is actually an essential decision-making procedure

260
00:12:05,180 --> 00:12:06,540
for democracy.

261
00:12:06,540 --> 00:12:10,860
Because it's only by paying attention to your evidence,

262
00:12:10,860 --> 00:12:12,140
as opposed to your moral,

263
00:12:12,140 --> 00:12:14,700
you know, your spiritual character, for example.

264
00:12:14,700 --> 00:12:16,260
It's only by paying attention to the evidence

265
00:12:16,260 --> 00:12:17,860
you give me for your beliefs

266
00:12:17,860 --> 00:12:22,860
that I truly respect you as an independent thinker,

267
00:12:23,420 --> 00:12:25,300
somebody who's capable of making up their own mind

268
00:12:25,300 --> 00:12:26,940
about how to vote.

269
00:12:26,940 --> 00:12:28,900
And the same goes for me on the other side, right?

270
00:12:28,900 --> 00:12:31,420
I mean, I've got to be willing to take the evidence

271
00:12:31,420 --> 00:12:33,700
of the conservatives insofar as, you know,

272
00:12:33,700 --> 00:12:35,900
if somebody has some evidence,

273
00:12:35,900 --> 00:12:36,940
and take it seriously.

274
00:12:36,940 --> 00:12:39,380
Because only by then I can respect them

275
00:12:39,380 --> 00:12:43,700
as full participants in the democratic process.

276
00:12:43,700 --> 00:12:45,140
And without that respect,

277
00:12:45,140 --> 00:12:48,820
without the sense that we can mutually respect one another,

278
00:12:48,820 --> 00:12:52,180
then there is no point of talking about a democratic process.

279
00:12:52,180 --> 00:12:56,100
Because democracy is founded on the idea of equal respect,

280
00:12:56,100 --> 00:13:01,100
that we treat each other as fellow participants

281
00:13:01,380 --> 00:13:05,060
in the project of democracy.

282
00:13:05,060 --> 00:13:06,820
So would you suggest that we perhaps,

283
00:13:06,820 --> 00:13:10,700
in need of upgrading our democracy and political system

284
00:13:10,700 --> 00:13:12,420
along with our philosophical system

285
00:13:12,420 --> 00:13:15,940
because of the growth and advancement of our technology?

286
00:13:15,940 --> 00:13:18,140
I think, yeah, I mean, I think our growth

287
00:13:18,140 --> 00:13:21,500
and advancement in technology has been tremendous aid

288
00:13:21,500 --> 00:13:24,540
in some ways to democratic practices.

289
00:13:24,540 --> 00:13:26,780
It's given us, because it has given us the ability

290
00:13:26,780 --> 00:13:28,940
to acquire more information and data

291
00:13:28,940 --> 00:13:30,420
about things like climate change.

292
00:13:30,420 --> 00:13:31,580
I agree.

293
00:13:31,580 --> 00:13:35,860
Without the techniques that mathematicians

294
00:13:35,860 --> 00:13:37,060
have been able to give us,

295
00:13:37,060 --> 00:13:40,500
applied mathematicians for data analytics,

296
00:13:40,500 --> 00:13:42,260
some of the data that we've been able to collect

297
00:13:42,260 --> 00:13:44,660
on climate change wouldn't have never been collected.

298
00:13:44,660 --> 00:13:47,580
So it's only because of technology and our advances

299
00:13:48,820 --> 00:13:50,220
that we're able to do that.

300
00:13:51,940 --> 00:13:54,180
I think what we do need to do is to make sure

301
00:13:54,180 --> 00:13:58,300
that our personal citizens' uses of technology,

302
00:13:58,300 --> 00:14:02,140
the uses of, let's say, social media,

303
00:14:02,140 --> 00:14:05,820
that we understand the ways in which we're using social media

304
00:14:05,820 --> 00:14:12,820
both can reflect our biases and our prejudices.

305
00:14:12,820 --> 00:14:15,100
It's, you know, when we talk about social media,

306
00:14:15,100 --> 00:14:17,060
the social part is really important.

307
00:14:17,060 --> 00:14:19,620
Absolutely, yes.

308
00:14:19,620 --> 00:14:22,260
And that could be really great. It could connect us.

309
00:14:22,260 --> 00:14:23,460
It's awesome to be able to connect

310
00:14:23,460 --> 00:14:25,300
with your old friends on Facebook.

311
00:14:25,300 --> 00:14:27,780
But it's really important to realize

312
00:14:27,780 --> 00:14:31,260
that the biases and prejudices

313
00:14:31,260 --> 00:14:35,620
that infect our social relationships offline,

314
00:14:35,620 --> 00:14:38,580
also affect our life online.

315
00:14:38,580 --> 00:14:40,740
And in a sense, that's trivially obvious.

316
00:14:40,740 --> 00:14:42,260
We think, oh, we all know that.

317
00:14:42,260 --> 00:14:44,620
And yet, we often ignore that fact

318
00:14:44,620 --> 00:14:49,420
when we draw conclusions from, let's say,

319
00:14:49,420 --> 00:14:52,700
about what to believe from something

320
00:14:52,700 --> 00:14:54,620
that somebody posted on Facebook

321
00:14:54,620 --> 00:14:58,940
because they're in our circle of friends, right?

322
00:14:58,940 --> 00:15:02,020
I mean, we tend to, you know, there's a lot of evidence

323
00:15:02,020 --> 00:15:04,220
to show that the sorts of interactions

324
00:15:04,220 --> 00:15:08,900
that we engage in, social media in particular,

325
00:15:08,900 --> 00:15:13,980
tend to lead our perfect situation

326
00:15:13,980 --> 00:15:17,740
in which for us to reflect our quick, implicit biases,

327
00:15:17,740 --> 00:15:19,900
partly because the interactions are so quick

328
00:15:19,900 --> 00:15:21,460
and not often reflective.

329
00:15:21,460 --> 00:15:23,980
They're not Twitter, for example.

330
00:15:23,980 --> 00:15:25,220
I use Twitter. I love it.

331
00:15:25,220 --> 00:15:30,420
But it's not a medium that is really,

332
00:15:30,420 --> 00:15:32,900
it's not the best medium to use

333
00:15:32,900 --> 00:15:37,100
if you're interested in another truism,

334
00:15:37,100 --> 00:15:41,740
reflective, critical engagement.

335
00:15:41,740 --> 00:15:43,580
It doesn't lead to these sorts of conversations.

336
00:15:43,580 --> 00:15:45,700
It's true. It's true.

337
00:15:45,700 --> 00:15:48,660
Well, I mean, with 140 characters, how far you can go?

338
00:15:48,660 --> 00:15:50,820
Yeah. I mean, some people have been incredibly creative

339
00:15:50,820 --> 00:15:53,100
at that, right? There are people that can,

340
00:15:53,100 --> 00:15:55,140
but most of the time, you can't go very far.

341
00:15:55,140 --> 00:15:59,260
But it's interesting, when you mention your life online

342
00:15:59,260 --> 00:16:02,140
and your life offline, Twitter is a very good example

343
00:16:02,140 --> 00:16:04,500
how the 140 limit characters

344
00:16:04,500 --> 00:16:07,420
has affected the news programs.

345
00:16:07,420 --> 00:16:08,260
Indeed.

346
00:16:08,260 --> 00:16:12,340
And our ability to absorb information,

347
00:16:12,340 --> 00:16:14,540
we need something fast and to the point.

348
00:16:14,540 --> 00:16:17,980
And that can be manipulated very easy in that way.

349
00:16:19,300 --> 00:16:22,900
How difficult do you think it's becoming

350
00:16:22,900 --> 00:16:27,500
to differentiate the life online and life offline?

351
00:16:27,500 --> 00:16:31,940
Because on one side is the software advancement

352
00:16:31,940 --> 00:16:35,380
like machine learning, big data and hardware advancement,

353
00:16:35,380 --> 00:16:38,540
like a virtual reality and augmented reality.

354
00:16:38,540 --> 00:16:42,060
How difficult it will become in the coming years

355
00:16:42,060 --> 00:16:43,980
to tell the difference between the two?

356
00:16:44,940 --> 00:16:47,860
Well, I think it's gonna become even more difficult.

357
00:16:47,860 --> 00:16:50,020
And I think it already is.

358
00:16:50,020 --> 00:16:55,020
You're right that there is for many people,

359
00:16:55,260 --> 00:16:57,820
for many of us, I mean, many people listening,

360
00:16:57,820 --> 00:17:00,700
not just some people over there, for all of us,

361
00:17:00,700 --> 00:17:03,900
there are ways in which the two lives

362
00:17:03,900 --> 00:17:05,580
have become very blurred.

363
00:17:06,900 --> 00:17:09,300
And that's particularly, that blurriness happens

364
00:17:09,300 --> 00:17:11,460
right out of the area that I'm interested in,

365
00:17:11,460 --> 00:17:13,260
in knowledge acquisition.

366
00:17:13,260 --> 00:17:14,220
You think about how you know

367
00:17:14,220 --> 00:17:17,340
most of the things you know now, right?

368
00:17:17,340 --> 00:17:19,180
And what's the first thing that you're gonna do

369
00:17:19,180 --> 00:17:20,820
if you have a question?

370
00:17:20,820 --> 00:17:22,620
First thing you're gonna do is Google it,

371
00:17:22,620 --> 00:17:23,460
at least if you're me.

372
00:17:23,460 --> 00:17:25,740
Yeah, you use a term called Google knowing.

373
00:17:25,740 --> 00:17:27,020
Yes, exactly.

374
00:17:27,020 --> 00:17:28,740
So would you expand on that?

375
00:17:28,740 --> 00:17:33,740
Sure, so Google knowing is not just knowledge by web search,

376
00:17:34,540 --> 00:17:36,140
although it's certainly that.

377
00:17:36,140 --> 00:17:38,740
It's the sort of knowledge that we acquire,

378
00:17:38,740 --> 00:17:41,540
what I earlier called this sort of minimal knowledge,

379
00:17:42,740 --> 00:17:46,100
that by digital interface.

380
00:17:46,100 --> 00:17:47,660
So it's the sort of knowledge we might acquire

381
00:17:47,660 --> 00:17:49,300
from my little level app,

382
00:17:49,300 --> 00:17:51,380
but it also might acquire from,

383
00:17:51,380 --> 00:17:55,260
we might acquire from a smartwatch and from a web search.

384
00:17:55,260 --> 00:17:59,220
And I think it's right there at that,

385
00:17:59,220 --> 00:18:03,700
that the concept of knowledge is where right now already

386
00:18:03,700 --> 00:18:07,460
our online life and our offline life is fuzzy,

387
00:18:07,460 --> 00:18:10,180
because what we do in our offline life

388
00:18:10,180 --> 00:18:14,940
is often dictated by what we know or think we know, right?

389
00:18:14,940 --> 00:18:17,060
So what we do in our ordinary everyday life,

390
00:18:17,060 --> 00:18:18,820
I mean, from mundane examples,

391
00:18:18,820 --> 00:18:21,300
like we don't have to talk about something important

392
00:18:21,300 --> 00:18:22,140
like climate change.

393
00:18:22,140 --> 00:18:23,100
We just talk about like, you know,

394
00:18:23,100 --> 00:18:28,100
which restaurant we go to is dictated by what we know.

395
00:18:29,140 --> 00:18:34,140
And how do we figure out what we believe we go online?

396
00:18:34,980 --> 00:18:39,580
So much of our offline life is already heavily impacted

397
00:18:39,580 --> 00:18:41,620
by what we do online.

398
00:18:41,620 --> 00:18:44,340
I mean, and the reason that Google knowing,

399
00:18:44,340 --> 00:18:47,460
and well, the return to your earlier question,

400
00:18:47,460 --> 00:18:48,620
I mean, is that gonna increase?

401
00:18:48,620 --> 00:18:50,300
Well, sure, I mean, you know,

402
00:18:50,300 --> 00:18:53,660
Larry Page, the CEO, or Google,

403
00:18:53,660 --> 00:18:56,780
one of the founders of Google, certainly thinks that,

404
00:18:57,660 --> 00:19:00,940
I mean, he's in the book, I quote him in saying that,

405
00:19:00,940 --> 00:19:04,940
no, one day we'll have, he says, he hopes,

406
00:19:04,940 --> 00:19:06,940
that we'll have an implant that will allow us,

407
00:19:06,940 --> 00:19:09,340
a neural implant that will allow us to

408
00:19:11,740 --> 00:19:13,260
just think of a question, he says,

409
00:19:13,260 --> 00:19:15,540
and then get the answer, right?

410
00:19:15,540 --> 00:19:20,540
And if he's right that that was to come around someday,

411
00:19:22,180 --> 00:19:25,660
or even something closer, like an actual Google Contact,

412
00:19:25,660 --> 00:19:27,300
which they have been working on, right?

413
00:19:27,300 --> 00:19:29,580
They have a project, it's not very far along,

414
00:19:29,580 --> 00:19:30,980
but they have a project in which they'd like

415
00:19:30,980 --> 00:19:34,540
to take Google Glass and put it farther closer to the eye.

416
00:19:34,540 --> 00:19:35,380
Right.

417
00:19:39,380 --> 00:19:42,220
So even if that neural implant never comes to be,

418
00:19:42,220 --> 00:19:45,340
I mean, once you get the Google Contact

419
00:19:45,340 --> 00:19:47,940
as a possibility, and I think that could come along,

420
00:19:49,220 --> 00:19:52,100
then I think things will be even blurrier.

421
00:19:53,420 --> 00:19:55,220
I mean, one way to think about it, so,

422
00:19:57,180 --> 00:20:00,180
Sergey Brin once said, when they introduced Google Glass,

423
00:20:00,180 --> 00:20:04,260
he said, the greatness of this technology is that

424
00:20:05,300 --> 00:20:07,620
it's going to get technology out of the way,

425
00:20:07,620 --> 00:20:09,380
and what he meant by that, I know what he meant,

426
00:20:09,380 --> 00:20:11,660
he meant we won't have to fumble with our phones, right,

427
00:20:11,660 --> 00:20:13,300
to get it up on the internet,

428
00:20:13,300 --> 00:20:14,660
so I understood what he meant.

429
00:20:14,660 --> 00:20:16,060
But it's sort of ironic, isn't it,

430
00:20:16,060 --> 00:20:19,620
because Google Glass or Google Contact

431
00:20:19,620 --> 00:20:21,700
get technology out of the way

432
00:20:21,700 --> 00:20:25,900
by literally putting it in the way of innovation.

433
00:20:25,900 --> 00:20:27,260
Right.

434
00:20:27,260 --> 00:20:30,780
And if that isn't a metaphor for this blurriness

435
00:20:30,780 --> 00:20:32,540
that you're talking about online and off,

436
00:20:32,540 --> 00:20:33,660
I don't know what is.

437
00:20:33,660 --> 00:20:36,380
Isn't it the kind of a natural evolution, though,

438
00:20:36,380 --> 00:20:41,100
becoming a biological and technological hybrid,

439
00:20:41,100 --> 00:20:44,340
or in other words, human 2.0?

440
00:20:44,340 --> 00:20:45,780
Right.

441
00:20:45,780 --> 00:20:48,060
Yeah, so, I mean, the cyborgian of the human

442
00:20:48,060 --> 00:20:49,580
is certainly something that a lot of folks

443
00:20:49,580 --> 00:20:51,900
are interested in, and I think have been writing about,

444
00:20:51,900 --> 00:20:53,220
and I'm interested in, too,

445
00:20:53,220 --> 00:20:56,380
and I think we are moving towards that, human 2.0.

446
00:20:56,380 --> 00:20:57,980
And there could be a lot of benefits from that.

447
00:20:57,980 --> 00:20:59,180
I think there's a lot of benefits

448
00:20:59,180 --> 00:21:00,980
from information technology,

449
00:21:00,980 --> 00:21:04,780
including certain implants of various sorts

450
00:21:04,780 --> 00:21:08,660
that are wired to the internet in healthcare, right?

451
00:21:08,660 --> 00:21:10,580
I mean, there's lots of benefits that are,

452
00:21:10,580 --> 00:21:12,180
I think, could come from that.

453
00:21:12,180 --> 00:21:15,300
So I'm not saying that I'm just blatantly

454
00:21:15,300 --> 00:21:16,420
antagonistic to this.

455
00:21:16,420 --> 00:21:18,220
What I'm interested in, at all,

456
00:21:18,220 --> 00:21:20,220
what I'm interested in is thinking through

457
00:21:20,220 --> 00:21:22,420
the consequences of some of this technology,

458
00:21:22,420 --> 00:21:25,580
both ethically and epistemologically,

459
00:21:25,580 --> 00:21:27,580
and it would be great if once in our life,

460
00:21:27,580 --> 00:21:29,700
and once in human history,

461
00:21:29,700 --> 00:21:31,620
we actually thought through some of these consequences

462
00:21:31,620 --> 00:21:34,420
before we actually went and made the technology,

463
00:21:34,420 --> 00:21:35,620
but I don't really know about that.

464
00:21:35,620 --> 00:21:37,060
Yes, me neither.

465
00:21:37,060 --> 00:21:41,540
That would be, that's never happened before, so.

466
00:21:41,540 --> 00:21:44,700
What do you make of the court case between Apple and FBI?

467
00:21:44,700 --> 00:21:46,940
They dropped it, and then they came back and said,

468
00:21:46,940 --> 00:21:49,140
no, we need Apple, because-

469
00:21:49,140 --> 00:21:51,140
They dropped it, they said, oh, no, just kidding.

470
00:21:51,140 --> 00:21:52,780
They actually didn't.

471
00:21:52,780 --> 00:21:54,020
What do you make of that?

472
00:21:54,020 --> 00:21:57,100
But first of all, do you think the concept of privacy

473
00:21:57,100 --> 00:22:00,260
does even exist anymore?

474
00:22:00,260 --> 00:22:04,380
And if so, how we can protect it, and if not,

475
00:22:04,380 --> 00:22:08,540
what is the option going forward?

476
00:22:08,540 --> 00:22:09,700
Yeah.

477
00:22:09,700 --> 00:22:12,500
Okay, that's a bunch of really difficult questions

478
00:22:12,500 --> 00:22:13,820
and good ones.

479
00:22:13,820 --> 00:22:18,820
So, yeah, let's put off the Apple case for a second.

480
00:22:19,300 --> 00:22:22,300
I think that raises a bunch of interesting questions,

481
00:22:23,620 --> 00:22:26,620
but let's talk about, maybe I'll talk about your more,

482
00:22:26,620 --> 00:22:28,660
the general questions about privacy first.

483
00:22:29,740 --> 00:22:31,940
So does the concept of privacy,

484
00:22:31,940 --> 00:22:34,140
is it even relevant anymore?

485
00:22:34,140 --> 00:22:36,820
That's one way of understanding what you were asking me.

486
00:22:36,820 --> 00:22:38,140
I think yes, it is.

487
00:22:38,140 --> 00:22:40,940
So I don't think it's an outdated value.

488
00:22:42,340 --> 00:22:44,860
I think Jeremy Rifkin and others have argued

489
00:22:44,860 --> 00:22:48,940
that it's a bourgeois value, it was an architect

490
00:22:48,940 --> 00:22:52,060
of the Victorian age and so forth and so on.

491
00:22:52,060 --> 00:22:56,140
I think that's just a mistake.

492
00:22:56,140 --> 00:22:58,500
Yeah, Jeremy Rifkin is the author of the book,

493
00:22:58,500 --> 00:23:02,380
Zero Margin Society, a very interesting concept.

494
00:23:02,380 --> 00:23:03,220
Yeah, please continue.

495
00:23:03,220 --> 00:23:05,300
Yeah, indeed, and I think it is a very interesting book,

496
00:23:05,300 --> 00:23:07,780
but I disagree with him on this point.

497
00:23:07,780 --> 00:23:12,780
I think that one reason to think that it's not an outmoded

498
00:23:16,980 --> 00:23:20,180
notion is to realize what the roots of the notion are.

499
00:23:24,060 --> 00:23:26,580
If we think about why privacy matters

500
00:23:26,580 --> 00:23:29,300
and its philosophical roots,

501
00:23:29,300 --> 00:23:31,420
you'll see that it actually is based, I think,

502
00:23:31,420 --> 00:23:34,740
in our very psychology as human beings.

503
00:23:34,740 --> 00:23:37,980
So part of what we think makes a difference

504
00:23:37,980 --> 00:23:41,220
between you and I and minds in general

505
00:23:41,220 --> 00:23:44,580
is that our thoughts are, in a sense, our own.

506
00:23:47,260 --> 00:23:48,540
We can have this conversation

507
00:23:48,540 --> 00:23:51,900
and we can sort of infer what we're thinking

508
00:23:51,900 --> 00:23:53,300
and so forth as you do.

509
00:23:54,460 --> 00:23:57,260
But of course, what makes you different than I

510
00:23:57,260 --> 00:23:58,340
is that your thoughts are your thoughts,

511
00:23:58,340 --> 00:23:59,300
my thoughts are my thoughts,

512
00:23:59,300 --> 00:24:01,380
and you can share those thoughts with me,

513
00:24:01,380 --> 00:24:05,820
but if you don't, then I'm not necessarily

514
00:24:05,820 --> 00:24:07,620
gonna be able to feel or experience

515
00:24:07,620 --> 00:24:09,580
what you've felt or experienced.

516
00:24:09,580 --> 00:24:10,980
They're private in that sense,

517
00:24:10,980 --> 00:24:12,180
and that's the fact that very term

518
00:24:12,180 --> 00:24:14,220
that philosophers have used for a long time,

519
00:24:14,220 --> 00:24:17,660
that a mark of a human being's psychology

520
00:24:17,660 --> 00:24:20,460
is that we have a special relationship to our psychology

521
00:24:20,460 --> 00:24:23,660
that we don't have to us, to other people's psychology.

522
00:24:24,660 --> 00:24:25,500
And we value that.

523
00:24:25,500 --> 00:24:28,420
We value our sort of subjectivity, our individuality.

524
00:24:28,420 --> 00:24:30,980
And as a result, we value the sort of privacy,

525
00:24:30,980 --> 00:24:33,540
the control and protection

526
00:24:33,540 --> 00:24:34,980
that we can give to our own thoughts.

527
00:24:34,980 --> 00:24:35,820
And that's really, I think,

528
00:24:35,820 --> 00:24:37,780
the philosophical root of privacy.

529
00:24:37,780 --> 00:24:40,300
And I don't think that the notion of,

530
00:24:40,300 --> 00:24:42,980
I don't think individuality or subjectivity

531
00:24:42,980 --> 00:24:46,180
is something that was just recently invented.

532
00:24:46,180 --> 00:24:48,420
I think it's a human being's been around

533
00:24:48,420 --> 00:24:49,660
for longer than that.

534
00:24:50,900 --> 00:24:52,380
But why does it matter?

535
00:24:52,380 --> 00:24:53,860
That's a different question.

536
00:24:53,860 --> 00:24:54,780
Why does privacy matter?

537
00:24:54,780 --> 00:24:58,700
I think privacy matters both for, still today,

538
00:24:58,700 --> 00:25:03,700
for certain obvious reasons and less obvious reasons.

539
00:25:04,580 --> 00:25:06,420
The obvious reasons are that,

540
00:25:06,420 --> 00:25:10,300
the reason I don't want the NSA poking around in my emails

541
00:25:10,300 --> 00:25:12,340
is partly because they might do,

542
00:25:12,340 --> 00:25:13,660
they might, I don't know what they're gonna do

543
00:25:13,660 --> 00:25:15,620
with that information, right?

544
00:25:15,620 --> 00:25:18,340
And we, in general, we don't want people

545
00:25:18,340 --> 00:25:19,460
breaking into our phone,

546
00:25:19,460 --> 00:25:21,900
whether it's the FBI or otherwise,

547
00:25:21,900 --> 00:25:26,260
because that gives them some measure of control.

548
00:25:26,260 --> 00:25:28,260
If they know certain information about me,

549
00:25:28,260 --> 00:25:30,140
they could control me in some way.

550
00:25:30,140 --> 00:25:32,740
And so, that's a sort of obvious point, right?

551
00:25:34,380 --> 00:25:35,740
But even that obvious point

552
00:25:35,740 --> 00:25:39,660
is sometimes missed by certain politicians.

553
00:25:39,660 --> 00:25:41,420
So, Mike Rogers, for example,

554
00:25:41,420 --> 00:25:43,660
a Republican, conservative Republican

555
00:25:44,940 --> 00:25:48,180
from Pennsylvania in the United States,

556
00:25:48,180 --> 00:25:49,660
at one point in a congressional hearing said,

557
00:25:49,660 --> 00:25:52,100
look, I don't know what this problem with the NSA is.

558
00:25:52,100 --> 00:25:55,380
If people don't know about the fact that they're being,

559
00:25:55,380 --> 00:25:58,420
their privacy is being, then what's the harm?

560
00:25:58,420 --> 00:25:59,900
If you don't know, there's no harm.

561
00:25:59,900 --> 00:26:02,380
Which was really only consoling

562
00:26:02,380 --> 00:26:04,300
to peeping Toms everywhere, right?

563
00:26:04,300 --> 00:26:07,020
Do you think he, for example, really believes in that,

564
00:26:07,020 --> 00:26:12,020
or he's using that as an excuse to push his agenda,

565
00:26:12,820 --> 00:26:14,380
which is a party agenda?

566
00:26:14,380 --> 00:26:16,140
Well, that's a perennial, I mean,

567
00:26:17,580 --> 00:26:18,540
let me put it this way.

568
00:26:18,540 --> 00:26:20,580
I think almost the more charitable reading

569
00:26:20,580 --> 00:26:22,460
is that he actually believed it when he said it,

570
00:26:22,460 --> 00:26:24,980
and he just hadn't thought it through.

571
00:26:24,980 --> 00:26:26,380
That's the more charitable reason.

572
00:26:26,380 --> 00:26:27,220
That's a great quality for a politician.

573
00:26:27,220 --> 00:26:28,900
Because if he's using that sort of reasoning

574
00:26:28,900 --> 00:26:31,620
to push his party's agenda, then good luck to him.

575
00:26:31,620 --> 00:26:32,460
Yeah.

576
00:26:33,220 --> 00:26:36,060
So, I think it might've been he just,

577
00:26:36,060 --> 00:26:38,860
he hadn't thought through the logic of what he was saying.

578
00:26:38,860 --> 00:26:41,220
Of course, we think in some ways that if we don't,

579
00:26:41,220 --> 00:26:44,100
if you break it in my house and into my computer

580
00:26:44,100 --> 00:26:46,060
and steal all my financial information,

581
00:26:46,060 --> 00:26:48,060
then even if I don't know about it until later,

582
00:26:48,060 --> 00:26:50,020
like when you drain my bank account,

583
00:26:50,020 --> 00:26:52,460
it's still bad when you did it, right?

584
00:26:53,740 --> 00:26:55,820
So, partly because of consequences.

585
00:26:55,820 --> 00:26:57,420
But there are even less obvious,

586
00:26:57,420 --> 00:27:00,020
but more fundamental reasons to be concerned about privacy

587
00:27:00,020 --> 00:27:01,780
and information privacy.

588
00:27:01,780 --> 00:27:03,700
And that has to do with our autonomy.

589
00:27:03,700 --> 00:27:05,020
It gets back to my first point

590
00:27:05,020 --> 00:27:07,060
about the origin of the notion.

591
00:27:07,060 --> 00:27:12,060
Look, if you, suppose you, again, break into my phone

592
00:27:15,220 --> 00:27:16,780
or whatever, you hack my phone,

593
00:27:16,780 --> 00:27:19,860
and you get the content of my phone,

594
00:27:20,780 --> 00:27:22,980
various contents, the emails and the like.

595
00:27:24,540 --> 00:27:27,980
But suppose you do that only as a scientific experiment,

596
00:27:27,980 --> 00:27:29,500
but you don't do anything with it.

597
00:27:29,500 --> 00:27:33,340
You just, you don't use it to blackmail me or anything,

598
00:27:33,340 --> 00:27:34,580
right, or whatever.

599
00:27:39,340 --> 00:27:40,500
Is it still wrong?

600
00:27:40,500 --> 00:27:42,300
Well, it is.

601
00:27:42,300 --> 00:27:43,820
And a side to that is still wrong

602
00:27:43,820 --> 00:27:45,060
is that we find things like that,

603
00:27:45,060 --> 00:27:47,660
or people, an older example would be somebody reading

604
00:27:47,660 --> 00:27:49,300
your diary or something, right?

605
00:27:49,300 --> 00:27:51,260
But not doing anything with it.

606
00:27:51,260 --> 00:27:53,660
Would you, if you found that out later, right?

607
00:27:53,660 --> 00:27:55,540
Even though that later, like, you know,

608
00:27:55,540 --> 00:27:58,340
you realize that person never did anything,

609
00:27:58,340 --> 00:27:59,940
you would still be creeped out by that.

610
00:27:59,940 --> 00:28:00,980
Of course.

611
00:28:00,980 --> 00:28:01,820
You shouldn't do that.

612
00:28:01,820 --> 00:28:03,140
Well, why not?

613
00:28:03,140 --> 00:28:06,260
Well, because, because you can imagine the other person saying,

614
00:28:06,260 --> 00:28:07,660
well, I never did anything, right?

615
00:28:07,660 --> 00:28:09,140
Well, that wouldn't matter.

616
00:28:09,140 --> 00:28:11,020
Because you violated my autonomy.

617
00:28:11,020 --> 00:28:14,100
I didn't choose to share that information, right?

618
00:28:14,100 --> 00:28:16,220
In this case, or you didn't choose to share it.

619
00:28:16,220 --> 00:28:18,780
So what I did is I sort of made the decision

620
00:28:18,780 --> 00:28:20,540
to share or not to share moot.

621
00:28:20,540 --> 00:28:24,020
It would be, it's similar to if a doctor gives you

622
00:28:24,020 --> 00:28:25,540
a drug without your permission,

623
00:28:25,540 --> 00:28:28,740
even though the drug helps you out, there's a problem there.

624
00:28:28,740 --> 00:28:30,100
Why do we think it's a problem?

625
00:28:30,100 --> 00:28:33,820
Because I didn't get, I wasn't in on the decision.

626
00:28:33,820 --> 00:28:35,900
The decision was made for me.

627
00:28:35,900 --> 00:28:37,940
Now, the doctor case shows us that, in fact,

628
00:28:37,940 --> 00:28:40,500
there are times where we think that privacy invasion

629
00:28:40,500 --> 00:28:42,580
are justified, just as we think that there are times

630
00:28:42,580 --> 00:28:44,220
in which the doctor giving you a shot

631
00:28:44,220 --> 00:28:46,540
or doing a medical procedure is justified

632
00:28:46,540 --> 00:28:48,500
even if you're not conscious, right?

633
00:28:48,500 --> 00:28:50,260
Because in an emergency, right?

634
00:28:50,260 --> 00:28:52,620
We don't, the emergency physicians don't, you know,

635
00:28:52,620 --> 00:28:55,940
look, if you're dying of some problem

636
00:28:55,940 --> 00:28:59,820
or you're in grave medical condition,

637
00:28:59,820 --> 00:29:01,980
they have, they're justified in acting

638
00:29:01,980 --> 00:29:03,780
even if they don't have your consent.

639
00:29:04,660 --> 00:29:07,260
But, and that's the case.

640
00:29:07,260 --> 00:29:09,180
And sometimes we think of privacy cases, right?

641
00:29:09,180 --> 00:29:11,620
We think that other matters, other values

642
00:29:11,620 --> 00:29:13,900
might supersede it, right?

643
00:29:13,900 --> 00:29:16,020
And so that's understandable.

644
00:29:16,020 --> 00:29:18,460
But it's still the case that there is this fundamental

645
00:29:18,460 --> 00:29:20,620
reason to be worried about privacy

646
00:29:20,620 --> 00:29:23,460
that has nothing to do with consequences, right?

647
00:29:23,460 --> 00:29:28,340
Even in the case where the invasion of my privacy

648
00:29:28,340 --> 00:29:30,140
has no bad effects on me,

649
00:29:30,140 --> 00:29:32,980
there's still something wrong about it.

650
00:29:32,980 --> 00:29:34,740
And that is a point that's often missed,

651
00:29:34,740 --> 00:29:36,060
I think, in these debates.

652
00:29:36,060 --> 00:29:37,340
Absolutely.

653
00:29:37,340 --> 00:29:39,380
The book is called The Internet of Us,

654
00:29:39,380 --> 00:29:42,660
Knowing More and Understanding Less in the Age of Big Data.

655
00:29:42,660 --> 00:29:45,780
It came out in March 21st, I think.

656
00:29:45,780 --> 00:29:47,860
That's right. This year, yeah.

657
00:29:47,860 --> 00:29:50,260
In Canada, I think next week.

658
00:29:50,260 --> 00:29:52,300
Oh, it's not available in Canada yet?

659
00:29:52,300 --> 00:29:53,140
I don't know.

660
00:29:53,140 --> 00:29:55,980
I think it comes out in Canada next week

661
00:29:55,980 --> 00:29:58,340
or maybe, right, maybe this week.

662
00:29:58,340 --> 00:29:59,420
Oh, excellent.

663
00:30:01,020 --> 00:30:04,820
Mark Goodman, in his book, The Future Crimes,

664
00:30:04,820 --> 00:30:08,020
make an argument that we have two kinds of computers,

665
00:30:08,020 --> 00:30:09,940
the computers that have been hacked already

666
00:30:09,940 --> 00:30:12,460
or haven't been hacked yet.

667
00:30:12,460 --> 00:30:16,780
So speaking of privacy, what...

668
00:30:16,780 --> 00:30:18,500
That's almost, yeah.

669
00:30:18,500 --> 00:30:21,620
Yeah, what kinds of measures do you think we can take

670
00:30:21,620 --> 00:30:26,620
as members of general public to secure ourself

671
00:30:26,740 --> 00:30:30,780
from intrusion of, well, police is one side of the story,

672
00:30:30,780 --> 00:30:32,860
but the other side are hackers.

673
00:30:32,860 --> 00:30:36,180
The other side is what's going on in deep web and darknet.

674
00:30:36,180 --> 00:30:37,980
Yeah, darknet and deep web.

675
00:30:41,060 --> 00:30:44,700
Well, as a philosopher, first, let me say,

676
00:30:44,700 --> 00:30:47,060
because that's a practical question.

677
00:30:47,060 --> 00:30:49,260
I am not, you know, if you want to know

678
00:30:50,500 --> 00:30:52,700
how to protect yourself in a practical way,

679
00:30:52,700 --> 00:30:55,340
don't ask a philosopher because that's a practical question

680
00:30:55,340 --> 00:30:59,100
and practical questions are not our forte.

681
00:31:00,500 --> 00:31:02,180
What I do think, I mean,

682
00:31:02,180 --> 00:31:03,820
but I don't want to punt completely on the question.

683
00:31:03,820 --> 00:31:08,820
I think actually, as Snowden pointed out,

684
00:31:10,860 --> 00:31:13,380
probably the easiest thing to do to protect yourself

685
00:31:13,380 --> 00:31:17,260
is to try to use sort of reasonable,

686
00:31:17,260 --> 00:31:20,340
sensible policies on passwords,

687
00:31:20,340 --> 00:31:23,020
the sort of passwords, policies that people have.

688
00:31:23,020 --> 00:31:25,580
I mean, that's not gonna protect you completely, obviously.

689
00:31:25,580 --> 00:31:27,260
I mean, nothing can protect you completely.

690
00:31:27,260 --> 00:31:28,940
There is no such thing as complete protection.

691
00:31:28,940 --> 00:31:29,780
That's right.

692
00:31:29,780 --> 00:31:31,020
Not anymore, I guess, yes.

693
00:31:31,020 --> 00:31:34,460
Not anymore, and probably even with any technology, right?

694
00:31:34,460 --> 00:31:39,140
Any technology can be broken.

695
00:31:39,140 --> 00:31:43,300
The Enigma code was broken by Alan Turing and others,

696
00:31:43,300 --> 00:31:46,940
and that was a pretty damn good technology for encryption.

697
00:31:47,860 --> 00:31:49,820
So there's just more or less difficult,

698
00:31:51,580 --> 00:31:52,420
there's techniques.

699
00:31:52,420 --> 00:31:55,580
Now, I mean, there might be logical limits to,

700
00:31:55,580 --> 00:31:58,660
but I mean, quantum, I mean, yeah.

701
00:31:58,660 --> 00:32:00,020
Let's just leave it at that.

702
00:32:00,020 --> 00:32:02,100
Practically speaking, there's no safeguard

703
00:32:02,100 --> 00:32:04,660
that can't be gotten around.

704
00:32:04,660 --> 00:32:09,660
However, you can protect yourself by engaging in more,

705
00:32:11,660 --> 00:32:13,900
and pretty much just following the things

706
00:32:13,900 --> 00:32:16,660
that the IT people will tell you to do on day one.

707
00:32:17,820 --> 00:32:20,060
I referenced Snowden because he was on,

708
00:32:21,700 --> 00:32:24,740
at one point interviewed, I think on 60 Minutes,

709
00:32:24,740 --> 00:32:25,980
so don't quote me on that.

710
00:32:25,980 --> 00:32:27,860
You can Google it and find out, I suppose.

711
00:32:27,860 --> 00:32:31,940
But just telling people the very basic things

712
00:32:31,940 --> 00:32:35,060
about encryption and password protection,

713
00:32:35,060 --> 00:32:35,900
and those things, I think,

714
00:32:35,900 --> 00:32:38,260
are probably the best things you can do.

715
00:32:38,260 --> 00:32:40,140
Now, I think for those of you,

716
00:32:40,140 --> 00:32:41,260
for those of your listeners

717
00:32:41,260 --> 00:32:43,180
that actually know something about encryption,

718
00:32:43,180 --> 00:32:44,980
there's a whole bunch of much more sophisticated things

719
00:32:44,980 --> 00:32:47,940
to do, but they don't need a philosopher to tell them that.

720
00:32:47,940 --> 00:32:49,740
But for folks who don't know anything,

721
00:32:50,780 --> 00:32:55,780
not giving your middle name as your password

722
00:32:55,780 --> 00:32:58,180
and using different passwords for different sites,

723
00:32:58,180 --> 00:32:59,620
those sort of basic things,

724
00:33:01,140 --> 00:33:03,740
it's sort of like locking your door on your apartment.

725
00:33:03,740 --> 00:33:05,700
That's right, exactly.

726
00:33:05,700 --> 00:33:08,340
Can the person who wants to get in your apartment

727
00:33:08,340 --> 00:33:11,220
through that lock and who is a professional do it?

728
00:33:11,220 --> 00:33:12,060
Yep.

729
00:33:13,460 --> 00:33:18,140
Is it gonna be dissuaded, however, some folks,

730
00:33:18,140 --> 00:33:22,060
if there's just a lock or two on your apartment?

731
00:33:22,060 --> 00:33:23,380
Yep.

732
00:33:23,380 --> 00:33:25,060
Both things are true.

733
00:33:25,060 --> 00:33:26,540
You can't stop everybody,

734
00:33:26,540 --> 00:33:28,860
but you can slow people down

735
00:33:28,860 --> 00:33:31,620
and make it more of a pain in the butt

736
00:33:31,620 --> 00:33:33,180
for them to evade your privacy.

737
00:33:33,180 --> 00:33:34,580
Yes, very true.

738
00:33:34,580 --> 00:33:36,580
Speaking of your book,

739
00:33:36,580 --> 00:33:37,940
another concept that you talk about

740
00:33:37,940 --> 00:33:40,220
is extended knowledge hypothesis,

741
00:33:40,220 --> 00:33:44,980
and I'll be very interested for you to expand a little bit

742
00:33:44,980 --> 00:33:46,780
on that concept as well.

743
00:33:46,780 --> 00:33:49,660
Sure, that I think relates in some ways in my own mind

744
00:33:49,660 --> 00:33:54,500
to the privacy debate as well.

745
00:33:54,500 --> 00:33:58,980
Extended knowledge hypothesis is a variation

746
00:34:00,500 --> 00:34:03,460
or an implementation of another hypothesis,

747
00:34:03,460 --> 00:34:05,940
which is called the extended mind hypothesis

748
00:34:05,940 --> 00:34:08,500
due to the philosophers Andy Clark and Ann Burrow

749
00:34:08,500 --> 00:34:10,300
and David Chalmers, who's in New York.

750
00:34:11,620 --> 00:34:14,420
Those philosophers 20 years ago suggested

751
00:34:14,420 --> 00:34:17,580
that it's possible that our mental states,

752
00:34:17,580 --> 00:34:24,860
like the state of memory, might already be extended

753
00:34:24,860 --> 00:34:28,180
to things like books or notepads.

754
00:34:28,180 --> 00:34:31,740
Their example, this was pre-smartphone days,

755
00:34:31,740 --> 00:34:33,820
was that if you just took a shopping list,

756
00:34:33,820 --> 00:34:37,940
you were extending your memory to the list.

757
00:34:37,940 --> 00:34:39,980
What they meant by that is

758
00:34:39,980 --> 00:34:44,100
there's sort of an innocent way to read that,

759
00:34:44,100 --> 00:34:46,620
and one innocent way to read that is that they're just saying

760
00:34:46,620 --> 00:34:49,780
that, oh, shopping lists are good aids to memory,

761
00:34:49,780 --> 00:34:51,900
or the iPhone is a good aid to your memory.

762
00:34:51,900 --> 00:34:52,940
No, no, no, no, no.

763
00:34:52,940 --> 00:34:55,420
They mean something much more radical than that.

764
00:34:55,420 --> 00:34:59,980
What they mean is your memory state itself,

765
00:34:59,980 --> 00:35:02,580
the actual state of your memory, your memory,

766
00:35:02,580 --> 00:35:05,540
actually part of it is that piece of paper.

767
00:35:05,540 --> 00:35:06,780
In other words, your mental states,

768
00:35:06,780 --> 00:35:09,940
your psychological states are partly composed of paper.

769
00:35:11,060 --> 00:35:13,300
That's the extended mind thesis.

770
00:35:13,300 --> 00:35:19,020
Your mind extends beyond the bounds of your skull.

771
00:35:19,020 --> 00:35:20,940
The biological, the structure.

772
00:35:20,940 --> 00:35:24,580
Yeah, exactly, beyond the biological structure.

773
00:35:24,580 --> 00:35:28,340
Now, the extended knowledge hypothesis

774
00:35:28,340 --> 00:35:32,460
is something similar to that, but could be held independently.

775
00:35:32,460 --> 00:35:34,740
I'm sort of agnostic about the extended mind thesis.

776
00:35:34,740 --> 00:35:35,740
I think it might be true,

777
00:35:35,740 --> 00:35:37,620
but there's some good objections against it,

778
00:35:37,620 --> 00:35:42,100
which might not be relevant to mention here.

779
00:35:42,100 --> 00:35:45,180
But I'm like, oh, well, let's see how it goes.

780
00:35:45,180 --> 00:35:47,060
Let's see how things sort of unfold,

781
00:35:47,060 --> 00:35:49,500
and maybe it will turn out to be true,

782
00:35:49,500 --> 00:35:50,820
or maybe it's true about some states,

783
00:35:50,820 --> 00:35:52,700
but not others, and so forth.

784
00:35:52,700 --> 00:35:54,300
I think that's probably the case.

785
00:35:55,500 --> 00:35:57,420
But you could still, independently of that,

786
00:35:57,420 --> 00:36:01,540
hold that our knowledge processing is extended.

787
00:36:01,540 --> 00:36:02,940
That is, you might think that,

788
00:36:02,940 --> 00:36:05,500
even if my mind isn't completely extended,

789
00:36:06,540 --> 00:36:09,900
the ways in which the processes that I use

790
00:36:09,900 --> 00:36:14,580
to form beliefs are belief-forming processes.

791
00:36:14,580 --> 00:36:16,940
That is, the processes that I use

792
00:36:16,940 --> 00:36:20,140
to form an opinion about a piece of information,

793
00:36:20,140 --> 00:36:25,140
to process information, might themselves be extended,

794
00:36:25,340 --> 00:36:30,340
and I think are extended to our digital devices.

795
00:36:34,100 --> 00:36:36,660
And I think extended to each, actually, other people.

796
00:36:36,660 --> 00:36:38,620
I think that when I ask somebody

797
00:36:38,620 --> 00:36:40,340
and the philosopher Sandy Goldberg

798
00:36:40,340 --> 00:36:42,580
has said this sort of thing, he says,

799
00:36:42,580 --> 00:36:45,220
look, even in testimony, if I stop somebody on the street

800
00:36:45,220 --> 00:36:47,780
and ask them how to get to the restaurant,

801
00:36:48,660 --> 00:36:51,100
then the process by which I'm forming that belief

802
00:36:51,100 --> 00:36:54,100
is extended, both what's going on in my head,

803
00:36:54,100 --> 00:36:55,780
as I'm listening to them,

804
00:36:55,780 --> 00:36:58,540
there's also what they're doing and their expertise.

805
00:36:58,540 --> 00:37:02,460
So if I end up knowing where the restaurant is

806
00:37:02,460 --> 00:37:05,460
on the basis of that testimony that they gave me,

807
00:37:05,460 --> 00:37:07,420
it's because their testimony was accurate.

808
00:37:07,420 --> 00:37:09,020
But their testimony was accurate

809
00:37:09,020 --> 00:37:11,580
based on what's happened to them, right?

810
00:37:11,580 --> 00:37:13,340
That depends on them,

811
00:37:13,340 --> 00:37:14,740
whether they've been to the restaurant before

812
00:37:14,740 --> 00:37:15,580
and they know where they are

813
00:37:15,580 --> 00:37:18,580
and they know how to get there, stuff like that.

814
00:37:18,580 --> 00:37:20,460
So I think that's persuasive,

815
00:37:20,460 --> 00:37:24,740
and I think that it also means that much of what we know

816
00:37:24,740 --> 00:37:27,580
today, Google knowing, is extended in just this way.

817
00:37:27,580 --> 00:37:30,900
I think that's what makes Google knowing really interesting,

818
00:37:30,900 --> 00:37:32,620
is that when we Google know,

819
00:37:32,620 --> 00:37:37,500
we're radically dependent on what's going on elsewhere

820
00:37:37,500 --> 00:37:42,180
in space, that is, what's going on with these devices,

821
00:37:42,180 --> 00:37:45,900
whether the, what's going on with the software,

822
00:37:45,900 --> 00:37:47,940
the hardware, and the people that of course

823
00:37:47,940 --> 00:37:49,500
program those devices.

824
00:37:49,500 --> 00:37:52,220
All those facts are part of the process

825
00:37:52,220 --> 00:37:53,780
of our information processing.

826
00:37:53,780 --> 00:37:57,380
So that in a sense, when we know what we know,

827
00:37:57,380 --> 00:37:58,380
when we Google know,

828
00:37:59,900 --> 00:38:02,500
if you asked where is the knowledge,

829
00:38:02,500 --> 00:38:03,900
the knowledge is distributed.

830
00:38:05,540 --> 00:38:09,260
And in that sense, knowledge is networked,

831
00:38:09,260 --> 00:38:11,260
which is something people a lot of times say,

832
00:38:11,260 --> 00:38:14,700
but they often don't, I think, really know what they mean.

833
00:38:14,700 --> 00:38:17,980
But that's what I mean when I say,

834
00:38:17,980 --> 00:38:19,380
in the truest sense of the word,

835
00:38:19,380 --> 00:38:21,100
knowledge is networked now,

836
00:38:21,100 --> 00:38:24,180
because it's actually to know in the Google sense

837
00:38:24,180 --> 00:38:26,540
requires a network, and in fact,

838
00:38:26,540 --> 00:38:29,740
that network is part of your information processing system.

839
00:38:29,740 --> 00:38:31,820
So that's the thesis of extended knowledge.

840
00:38:31,820 --> 00:38:35,260
Speaking of distributed knowledge and learning,

841
00:38:36,340 --> 00:38:38,140
as we create more information,

842
00:38:38,140 --> 00:38:40,900
it's becoming harder to analyze that information,

843
00:38:40,900 --> 00:38:42,700
and that's where machine learning

844
00:38:42,700 --> 00:38:44,500
and artificial intelligence becoming

845
00:38:44,500 --> 00:38:47,220
more and more important, I think,

846
00:38:47,220 --> 00:38:48,980
to process that information.

847
00:38:48,980 --> 00:38:52,060
One good example in, I think, about a month ago,

848
00:38:52,060 --> 00:38:56,060
that Microsoft artificial intelligence was put on Twitter,

849
00:38:56,060 --> 00:38:58,980
and almost overnight, it turned into

850
00:38:58,980 --> 00:39:01,180
a Hitler-loving sex addict.

851
00:39:01,180 --> 00:39:02,020
Right.

852
00:39:02,020 --> 00:39:03,340
So what do you make of that,

853
00:39:03,340 --> 00:39:07,860
and how do you see the progress of artificial intelligence?

854
00:39:10,180 --> 00:39:12,820
And then I wanna speak to you as a philosopher

855
00:39:12,820 --> 00:39:16,780
about the concept of ethics in artificial intelligence.

856
00:39:16,780 --> 00:39:17,620
Sure.

857
00:39:18,660 --> 00:39:23,660
So, yeah, so that was something that actually

858
00:39:23,660 --> 00:39:25,220
I've been sort of noodling about,

859
00:39:25,220 --> 00:39:28,700
that example of the bot,

860
00:39:28,700 --> 00:39:33,700
that overnight sort of became this racist, sexist monster

861
00:39:33,940 --> 00:39:35,540
and had to be shut down.

862
00:39:35,540 --> 00:39:37,460
Now, there was a reason for that, of course,

863
00:39:37,460 --> 00:39:41,780
which is that the way it was constructed

864
00:39:41,780 --> 00:39:45,100
and how it was trained to train itself

865
00:39:45,100 --> 00:39:49,900
was to listen, in a sense, to what other people said to it

866
00:39:49,900 --> 00:39:52,500
and the sorts of questions that they asked it,

867
00:39:52,500 --> 00:39:55,300
and it would abstract from those questions

868
00:39:55,300 --> 00:39:57,700
and make certain inferences from those questions,

869
00:39:57,700 --> 00:40:01,700
and of course, I mean, I think there was,

870
00:40:01,700 --> 00:40:04,140
in this particular case, there were some people

871
00:40:04,140 --> 00:40:09,140
that cottoned on to the fact that this was going on, right,

872
00:40:10,180 --> 00:40:13,660
and sort of deliberately did this.

873
00:40:13,660 --> 00:40:15,780
I'm not sure that everybody was sort of aware

874
00:40:15,780 --> 00:40:19,500
of what was going on, and that's not surprising

875
00:40:19,500 --> 00:40:24,500
because as you may know, social bots,

876
00:40:24,860 --> 00:40:26,660
bots in general all over the web,

877
00:40:26,660 --> 00:40:30,380
I mean, Twitter tried to ban them for a while,

878
00:40:30,380 --> 00:40:33,180
I mean, they're not hard,

879
00:40:33,180 --> 00:40:35,180
you make a bazillion of them there,

880
00:40:36,100 --> 00:40:40,820
and if you're, we all probably have our own techniques

881
00:40:40,820 --> 00:40:43,900
for trying to spot when one pops up

882
00:40:43,900 --> 00:40:46,700
and asks for a Facebook to be our Facebook friend,

883
00:40:46,700 --> 00:40:53,700
but what does that tell us, this fact that this bot became so

884
00:40:57,980 --> 00:41:00,740
biased so quickly, so hateful?

885
00:41:00,740 --> 00:41:02,540
Well, it says, it tells us exactly

886
00:41:02,540 --> 00:41:04,860
what we started out talking about.

887
00:41:04,860 --> 00:41:06,700
The internet is the greatest fact checker

888
00:41:06,700 --> 00:41:09,180
and also the greatest bias-confirmer

889
00:41:09,180 --> 00:41:12,420
in the history of history.

890
00:41:12,420 --> 00:41:17,420
What I remember when I said earlier about social media,

891
00:41:20,060 --> 00:41:22,140
you've got to concentrate on the social,

892
00:41:22,140 --> 00:41:26,420
the biases that in fact are offline life,

893
00:41:26,420 --> 00:41:29,500
in fact are online life, and actually our online life

894
00:41:29,500 --> 00:41:33,340
makes those biases, tendency to make those biases

895
00:41:35,700 --> 00:41:39,340
more entrenched and more visible in certain ways.

896
00:41:39,340 --> 00:41:43,220
Some of that has to do with the fact that much of web life

897
00:41:43,220 --> 00:41:46,740
is or can be more anonymous than offline life,

898
00:41:46,740 --> 00:41:48,300
but that's not the only reason.

899
00:41:50,140 --> 00:41:52,220
But I think that that is really what's going on,

900
00:41:52,220 --> 00:41:55,100
that's what that illustrates, that's a perfect illustration

901
00:41:55,100 --> 00:41:56,860
of the way in which social media,

902
00:41:56,860 --> 00:41:59,020
far from connecting us in certain ways,

903
00:41:59,020 --> 00:42:04,020
has made us exemplify those traits that don't connect us,

904
00:42:06,500 --> 00:42:07,340
and that's worrying,

905
00:42:07,340 --> 00:42:09,420
and we need to figure out ways to fix that.

906
00:42:10,380 --> 00:42:12,500
Because social media is not going away.

907
00:42:12,500 --> 00:42:13,700
I'm not going to stop using it.

908
00:42:13,700 --> 00:42:15,140
It's expanding.

909
00:42:15,140 --> 00:42:16,100
It's expanding.

910
00:42:16,100 --> 00:42:18,340
So we better start paying attention to this right now.

911
00:42:18,340 --> 00:42:22,260
And if that example of the social bot isn't going to,

912
00:42:22,260 --> 00:42:25,940
if that, it just crystallizes the point.

913
00:42:25,940 --> 00:42:26,780
Right.

914
00:42:26,780 --> 00:42:27,620
So, yeah.

915
00:42:27,620 --> 00:42:30,940
I just wanted to talk about artificial intelligence.

916
00:42:30,940 --> 00:42:35,300
There is a great discussion around the potential dangers

917
00:42:35,300 --> 00:42:38,020
of artificial intelligence and how unpredictable

918
00:42:38,020 --> 00:42:39,220
it's going to be.

919
00:42:39,220 --> 00:42:41,140
And one of the ways that it's been suggested-

920
00:42:41,140 --> 00:42:42,220
When we reach the singularity.

921
00:42:42,220 --> 00:42:43,300
Yes, yeah.

922
00:42:44,540 --> 00:42:46,780
One of the ways that maybe we can control

923
00:42:46,780 --> 00:42:49,300
or try to understand artificial intelligence

924
00:42:49,300 --> 00:42:51,860
is to put our own ethics into the machine

925
00:42:51,860 --> 00:42:56,340
so the machine will operate based on those ethics.

926
00:42:56,340 --> 00:42:57,180
Yeah.

927
00:42:57,180 --> 00:43:02,180
Do you think ethics is objective to begin with

928
00:43:02,180 --> 00:43:06,460
and how it will translate from humanity into machine?

929
00:43:08,580 --> 00:43:10,700
Those are two really great,

930
00:43:10,700 --> 00:43:13,540
another great couple of questions.

931
00:43:15,020 --> 00:43:18,540
So the first question is very complicated to answer,

932
00:43:18,540 --> 00:43:20,020
but I have a lot of opinions about,

933
00:43:20,020 --> 00:43:21,540
but I'll try to crystallize them.

934
00:43:21,540 --> 00:43:24,100
Yeah, short order is I think it's objective.

935
00:43:24,100 --> 00:43:26,780
I think, so a lot of my work in the past

936
00:43:26,780 --> 00:43:28,860
has been on the nature of truth and objectivity.

937
00:43:28,860 --> 00:43:32,660
That's what I sort of, as a philosopher, have worked on.

938
00:43:32,660 --> 00:43:36,500
And so I have views about that.

939
00:43:37,460 --> 00:43:40,220
To summarize them, I think that ethics is objective.

940
00:43:40,220 --> 00:43:42,340
Its objectivity, though, isn't in the same,

941
00:43:42,340 --> 00:43:46,820
doesn't consist in the same set of features,

942
00:43:46,820 --> 00:43:50,220
or it's not the same in its character

943
00:43:50,220 --> 00:43:53,500
as the objectivity of, let's say, physics.

944
00:43:55,300 --> 00:43:56,940
And in other words, what I mean by that

945
00:43:56,940 --> 00:44:00,620
is when I say something like grass is green

946
00:44:00,620 --> 00:44:03,060
or that grass is green pointing to some grass,

947
00:44:05,020 --> 00:44:08,740
that truth sort of depends on whether,

948
00:44:08,740 --> 00:44:10,900
if that is true, it depends on whether there's grass

949
00:44:10,900 --> 00:44:13,060
that actually is green that I'm pointing at.

950
00:44:14,860 --> 00:44:17,020
So the truth sort of depends on whether it corresponds

951
00:44:17,020 --> 00:44:19,260
to some objective facts in the world.

952
00:44:19,260 --> 00:44:21,780
Well, ethics, it's much harder to point to anything.

953
00:44:22,820 --> 00:44:24,420
You can't point to a human right.

954
00:44:24,420 --> 00:44:26,660
You can point to what you think is an example

955
00:44:26,660 --> 00:44:29,180
of a human right or violations of it,

956
00:44:29,180 --> 00:44:32,380
but a human right is an abstract notion, right?

957
00:44:33,780 --> 00:44:36,220
So, but of course there are other abstract things

958
00:44:36,220 --> 00:44:39,140
like numbers that we think that are objective truths about.

959
00:44:39,140 --> 00:44:40,460
I don't think you can point to a number.

960
00:44:40,460 --> 00:44:42,220
You can point to a numeral.

961
00:44:42,220 --> 00:44:44,900
I can write one down on the whiteboard behind me.

962
00:44:44,900 --> 00:44:47,740
But of course, if I erase that number off a whiteboard,

963
00:44:47,740 --> 00:44:49,500
it's not like I killed the number seven.

964
00:44:49,500 --> 00:44:51,500
Oh my God, the number seven's gone away!

965
00:44:51,500 --> 00:44:55,100
Oh my God, the number seven's gone away, right?

966
00:44:55,100 --> 00:44:59,060
I just killed, I just erased a sign for the number seven.

967
00:44:59,060 --> 00:45:04,060
And so there are abstract entities that we can talk about

968
00:45:04,460 --> 00:45:08,540
that are objective or our thoughts about them

969
00:45:08,540 --> 00:45:09,700
can be objective.

970
00:45:09,700 --> 00:45:14,700
And so I think that the nature of objectivity changes

971
00:45:15,500 --> 00:45:18,100
in those domains, but there's still objectivity.

972
00:45:18,100 --> 00:45:20,940
So yeah, the short answer is that I think there are truths

973
00:45:20,940 --> 00:45:23,740
and objective ones about what's about right or wrong.

974
00:45:23,740 --> 00:45:25,380
But the hard question is not that one.

975
00:45:25,380 --> 00:45:29,100
The hard question is how do we know which ones those are?

976
00:45:29,100 --> 00:45:32,460
In mathematics, it's easier because we have a series,

977
00:45:32,460 --> 00:45:35,300
we have techniques, proofs, and the like,

978
00:45:35,300 --> 00:45:39,220
that we can, we generally speaking,

979
00:45:39,220 --> 00:45:41,620
unless we're talking about really advanced mathematics,

980
00:45:41,620 --> 00:45:44,900
we're generally going to agree on whether

981
00:45:44,900 --> 00:45:51,180
certain mathematical propositions are true or not.

982
00:45:51,180 --> 00:45:53,220
Not so much with morality.

983
00:45:53,220 --> 00:45:55,100
And that of course convinces a lot of people to think

984
00:45:55,100 --> 00:45:57,100
that there is no objectivity at all.

985
00:45:57,100 --> 00:45:58,300
I think that's a mistake.

986
00:46:01,900 --> 00:46:03,860
I think in morality, what we have to do is realize

987
00:46:03,860 --> 00:46:06,060
that we've got to be tolerant of a certain reasonable amount

988
00:46:06,060 --> 00:46:08,700
of pluralism, that there might be more than one true story

989
00:46:08,700 --> 00:46:10,820
of the world when it comes to morality,

990
00:46:10,820 --> 00:46:14,900
even if it's the case that not every story is true.

991
00:46:14,900 --> 00:46:17,060
There could be ties for first place,

992
00:46:17,060 --> 00:46:18,900
morally speaking, for moral systems.

993
00:46:18,900 --> 00:46:23,780
But some, like the Nazis, are right out.

994
00:46:23,780 --> 00:46:25,220
And the hard question philosophically

995
00:46:25,220 --> 00:46:27,540
is to try to explain how that could be the case.

996
00:46:27,540 --> 00:46:29,900
I think most of us probably think that's the case.

997
00:46:29,900 --> 00:46:32,780
And the hard philosophical work is, how can that be?

998
00:46:32,780 --> 00:46:40,100
But that all suggests to me that the hard question is going to be,

999
00:46:40,100 --> 00:46:44,820
well, which ethical principles do we put in the robot?

1000
00:46:44,820 --> 00:46:45,820
What do we teach it?

1001
00:46:49,100 --> 00:46:50,900
One problem that we saw with the social bot

1002
00:46:50,900 --> 00:46:55,020
is that if we allow it to teach itself, it becomes a disaster.

1003
00:46:55,020 --> 00:46:56,180
It could be a disaster.

1004
00:46:56,180 --> 00:46:58,980
It'll end up like the rest of us bastards, right?

1005
00:46:58,980 --> 00:47:03,020
Right?

1006
00:47:03,020 --> 00:47:06,060
It turns out that some humans might turn out to be ethical,

1007
00:47:06,060 --> 00:47:07,100
and a lot, not so much.

1008
00:47:07,100 --> 00:47:07,600
Right.

1009
00:47:10,900 --> 00:47:14,580
So if we adopt the Asimov method, that we,

1010
00:47:14,580 --> 00:47:17,060
from the foundation novels and the robot novels,

1011
00:47:17,060 --> 00:47:20,420
if we think that we need to provide

1012
00:47:20,420 --> 00:47:25,620
our artificial intelligence with some firm moral guidance,

1013
00:47:25,620 --> 00:47:27,500
then the real question will be, well,

1014
00:47:27,500 --> 00:47:29,180
then the real question will be, what

1015
00:47:29,180 --> 00:47:31,060
is that moral guidance going to consist in?

1016
00:47:31,060 --> 00:47:35,820
And there, of course, people will have different views,

1017
00:47:35,820 --> 00:47:38,260
especially when we get past the abstract principles

1018
00:47:38,260 --> 00:47:39,940
to how we apply them.

1019
00:47:39,940 --> 00:47:42,380
I mean, a lot of us can agree on abstract principles

1020
00:47:42,380 --> 00:47:45,500
like don't kill innocents for fun.

1021
00:47:45,500 --> 00:47:46,780
That's not too hard.

1022
00:47:46,780 --> 00:47:49,100
Most moral systems will agree on that.

1023
00:47:49,100 --> 00:47:52,060
Question is, who are the innocents?

1024
00:47:52,060 --> 00:47:53,100
That's a very good point.

1025
00:47:53,100 --> 00:47:54,020
That's the hard part.

1026
00:47:54,020 --> 00:47:54,820
Yeah.

1027
00:47:54,820 --> 00:48:00,940
And I don't know how you're going to program anything

1028
00:48:00,940 --> 00:48:03,820
to deal with the messiness of human experience,

1029
00:48:03,820 --> 00:48:08,740
or any experience, human or otherwise.

1030
00:48:08,740 --> 00:48:12,260
So I think that at some point, it's going to be unavoidable.

1031
00:48:12,260 --> 00:48:15,300
I think the hope to somehow imbue

1032
00:48:15,300 --> 00:48:18,660
our artificial intelligence with good moral character

1033
00:48:18,660 --> 00:48:22,060
is going to be as successful as our attempts to do

1034
00:48:22,060 --> 00:48:24,420
that with our own children.

1035
00:48:24,420 --> 00:48:27,660
Sometimes, that works out.

1036
00:48:27,660 --> 00:48:31,260
Sometimes, not so much.

1037
00:48:31,260 --> 00:48:31,860
Right?

1038
00:48:31,860 --> 00:48:33,460
Yeah, you do your best by you.

1039
00:48:33,460 --> 00:48:35,740
Morality is not an easy thing to do.

1040
00:48:35,740 --> 00:48:38,180
You do your best, but you have no absolute control

1041
00:48:38,180 --> 00:48:39,380
over the outcome.

1042
00:48:39,380 --> 00:48:39,880
Right.

1043
00:48:39,880 --> 00:48:40,820
And why is that?

1044
00:48:40,820 --> 00:48:44,140
Because they're going to find a different path through life.

1045
00:48:44,140 --> 00:48:45,460
Different things are going to happen to them.

1046
00:48:45,460 --> 00:48:46,420
Right.

1047
00:48:46,420 --> 00:48:49,180
And they're going to run across questions

1048
00:48:49,180 --> 00:48:50,940
that you might not have run across, like,

1049
00:48:50,940 --> 00:48:56,180
is this person innocent or not?

1050
00:48:56,180 --> 00:49:01,660
So that which seems easy in the armchair of the philosopher

1051
00:49:01,660 --> 00:49:06,900
is rarely easy when it meets the cold, hard facts of daily life.

1052
00:49:06,900 --> 00:49:09,440
And that's going to be the case for artificial intelligence

1053
00:49:09,440 --> 00:49:10,980
researchers as well.

1054
00:49:10,980 --> 00:49:12,140
Excellent.

1055
00:49:12,140 --> 00:49:15,780
We've been speaking to Professor Michael Lynch,

1056
00:49:15,780 --> 00:49:17,780
his latest book, The Internet of Us,

1057
00:49:17,780 --> 00:49:20,900
Knowing More and Understanding Less in the Age of Big Data,

1058
00:49:20,900 --> 00:49:24,140
available now in the US and will be available in Canada

1059
00:49:24,140 --> 00:49:26,860
this week or next week.

1060
00:49:26,860 --> 00:49:28,900
It's been a pleasure speaking with you.

1061
00:49:28,900 --> 00:49:31,300
I'm just going to ask you the last question that I'm

1062
00:49:31,300 --> 00:49:33,980
asking all my guests, that if you come across

1063
00:49:33,980 --> 00:49:37,580
an intelligent alien from a different civilization, what

1064
00:49:37,580 --> 00:49:42,020
would you say is humanity's worst thing, the worst thing

1065
00:49:42,020 --> 00:49:43,980
that humanity has done, and what would you say

1066
00:49:43,980 --> 00:49:53,780
as humanity's greatest achievement?

1067
00:49:53,780 --> 00:49:59,620
The worst thing that humanity has done

1068
00:49:59,620 --> 00:50:06,220
is allow itself to act on its deepest hates of each other.

1069
00:50:06,220 --> 00:50:10,820
The worst thing is the aspect of humanity itself,

1070
00:50:10,820 --> 00:50:16,780
acting on hate.

1071
00:50:16,780 --> 00:50:19,540
And of course, there's lots of examples of that.

1072
00:50:19,540 --> 00:50:22,900
But the mere fact of acting on hate

1073
00:50:22,900 --> 00:50:27,820
is probably the worst thing that we can do.

1074
00:50:27,820 --> 00:50:32,740
Having hate is bad enough, but acting on it is even worse.

1075
00:50:32,740 --> 00:50:35,100
What's the best thing?

1076
00:50:35,100 --> 00:50:42,540
The best thing is learning how to act out of love for others.

1077
00:50:42,540 --> 00:50:45,460
That's the best thing humanity has learned how to do.

1078
00:50:45,460 --> 00:50:51,420
It's unfortunate that we still need to keep learning,

1079
00:50:51,420 --> 00:50:55,420
but that's humanity.

1080
00:50:55,420 --> 00:50:57,060
That's what I would say to the aliens.

1081
00:50:57,060 --> 00:51:05,100
I learned how to enjoy our life.

1082
00:51:05,100 --> 00:51:18,540
I learned how to enjoy the life that Earth lives.

1083
00:51:18,540 --> 00:51:23,380
I learned about the life that humans live.

1084
00:51:23,380 --> 00:51:25,440
you

