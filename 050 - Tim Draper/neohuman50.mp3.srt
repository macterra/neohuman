1
00:00:00,000 --> 00:00:05,840
You know, I'm no longer a Republican or a Democrat. I was both. I was once a Republican,

2
00:00:05,840 --> 00:00:12,960
and then I was a Democrat. I'm a declined estate because both parties are building the

3
00:00:13,760 --> 00:00:21,360
duopoly that we suffer under. We need a major change in government now.

4
00:00:21,360 --> 00:00:34,320
Hello, and welcome to the 50th episode of Neo Human Podcasts. I'm Agabahari,

5
00:00:34,320 --> 00:00:40,000
an agologist on Twitter and Instagram, and you can follow the show on liveinlimbo.com,

6
00:00:40,000 --> 00:00:46,240
iTunes, and YouTube. And with me today, I have Tim Draper. Welcome to Neo Human Podcast, Tim.

7
00:00:46,240 --> 00:00:48,720
Terrific. My pleasure. Good to be here.

8
00:00:48,720 --> 00:00:52,080
It's great to have you. Let's start with your background, the work you've done,

9
00:00:52,080 --> 00:00:54,560
and what are you mainly focused on now these days?

10
00:00:54,560 --> 00:00:59,120
So my background, how far back do you want to go? I'm six.

11
00:01:00,400 --> 00:01:03,360
As far as you feel comfortable going back to?

12
00:01:03,360 --> 00:01:09,760
Well, okay. I was a Stanford electrical engineer and then Harvard MBA, and then I

13
00:01:09,760 --> 00:01:19,360
started Draper Associates, a venture fund, and I spun out an SBIC, a small business investment

14
00:01:19,360 --> 00:01:28,560
company that my father had started, and I borrowed $6 million against that from the government,

15
00:01:28,560 --> 00:01:33,200
and that was the $6 million I was able to start the venture business with.

16
00:01:33,200 --> 00:01:39,040
And so then it looked after three years like all the loans were being called, and it was going to

17
00:01:39,040 --> 00:01:49,200
be very scary and dangerous, and I flew back to the SBA and convinced them that things were

18
00:01:49,200 --> 00:01:56,640
just about around the corner. It was going to be okay, and they somehow went along with it.

19
00:01:56,640 --> 00:02:07,840
And so after that, in 1991, we had a whole bunch of IPOs. The window opened up, and one of those

20
00:02:07,840 --> 00:02:17,280
companies was Parametric Technology, and it made me and my investors about 175 times on the money,

21
00:02:17,920 --> 00:02:24,640
and we were able to pay back all the loans to whatever. I suddenly went to the SBA,

22
00:02:24,640 --> 00:02:30,160
you know, whatever. I suddenly went from being on the SBA's dirt list to their

23
00:02:33,600 --> 00:02:37,760
on their wall as the venture capitalist of the year.

24
00:02:37,760 --> 00:02:40,800
Nice. And how long did that take from zero to that point?

25
00:02:40,800 --> 00:02:45,200
From zero to that point was five or six years, and then I brought on

26
00:02:46,800 --> 00:02:52,240
John Fisher, and then later on, another three years later, I brought in Steve Jurvetson,

27
00:02:52,240 --> 00:03:05,040
and we built a very successful venture firm that had moments where it was the top venture

28
00:03:05,600 --> 00:03:12,240
firm in the world. A lot of that had to do with the fact that we were willing to venture

29
00:03:12,240 --> 00:03:19,600
outside of the Silicon Valley, and we were able to, we backed Skype and Baidu outside the Silicon

30
00:03:19,600 --> 00:03:32,560
Valley. And then about six years ago, I started to go solo and started to invest my own money

31
00:03:33,280 --> 00:03:47,360
alongside investors' money, and when I did that, I set up a whole new ecosystem for entrepreneurship.

32
00:03:47,360 --> 00:03:52,080
I built Draper University, which is attracting entrepreneurs from all over to learn

33
00:03:53,360 --> 00:04:00,080
how to become heroes, and I started Hero City, which is an incubator that allows

34
00:04:03,120 --> 00:04:09,440
entrepreneurs of all types to come in and just hang out for a while, and so we kind of created

35
00:04:09,440 --> 00:04:17,840
this great system where deals come from all over the world, and they come into our office, and we

36
00:04:17,840 --> 00:04:23,520
take a look at them, figure out if there's something we want to find. It's been very exciting, and

37
00:04:25,360 --> 00:04:35,680
since some of our best success came from international companies, we have a, I built

38
00:04:35,680 --> 00:04:42,560
the Draper Venture Network, and there are a series of, I think we've got something like 30 venture

39
00:04:42,560 --> 00:04:50,640
funds from around the world that are part of the network, and that also gives us a great flavor

40
00:04:50,640 --> 00:04:57,680
for what's going on in other parts of the world, and it's great. And then I got really early

41
00:04:57,680 --> 00:05:07,760
involved in Bitcoin and lost them all to Mt. Gox, and then realized that Mt. Gox didn't make Bitcoin

42
00:05:07,760 --> 00:05:17,040
go away, so I figured people really needed it, and so that's why I bid high and won the auction

43
00:05:18,000 --> 00:05:25,600
for Bitcoin when the U.S. Marshal's Office auctioned off the Silk Road Bitcoin. And since then,

44
00:05:25,600 --> 00:05:32,640
we've been sort of the center of all that activity in crypto, so that anybody who's starting an

45
00:05:32,640 --> 00:05:41,440
ICO or a cryptocurrency or a blockchain company, they know they can come to us as sort of a good

46
00:05:41,440 --> 00:05:49,680
first stop. So this has been a very exciting career, and I'm loving it.

47
00:05:49,680 --> 00:05:55,520
Loving it. So you've been, everything that you've done had something to do with technology. How has

48
00:05:55,520 --> 00:06:01,840
the growth of technology helped you with the evolution of your perspective towards the world

49
00:06:01,840 --> 00:06:08,240
as an investor and an individual? Yeah, so it's really interesting to see when I'm a

50
00:06:09,360 --> 00:06:16,640
as a venture capital investor, I'm always looking for where there's an industry that is providing

51
00:06:16,640 --> 00:06:22,960
bad service at a high cost, because those are the industries that are usually monopolies or

52
00:06:22,960 --> 00:06:31,520
oligopolies, and when there's a new technology that comes along, it usually gives an entrepreneur

53
00:06:31,520 --> 00:06:39,920
an opportunity to wedge into that industry. So the internet came along, and it transformed

54
00:06:39,920 --> 00:06:49,600
music and entertainment and communications and information and taxis. All those industries got

55
00:06:49,600 --> 00:06:55,840
completely transformed because the internet was there, and it was this new technology that allowed

56
00:06:57,040 --> 00:07:07,120
in effect, allowed products to spread and information to travel. Well, now, with the

57
00:07:07,120 --> 00:07:15,360
advent of Bitcoin, now we have another one of those, and a new technology, and that technology

58
00:07:15,920 --> 00:07:23,840
is allowing money to travel much more frictionlessly. And it also, when combined with

59
00:07:23,840 --> 00:07:31,280
with all this new data technology and artificial intelligence, has the ability to transform a bunch

60
00:07:31,280 --> 00:07:38,160
of other industries that weren't really that much affected by the internet coming. Banking industry,

61
00:07:38,160 --> 00:07:45,520
real estate, insurance, health care. Health care is going to change in a big way. It's going to be

62
00:07:45,520 --> 00:07:52,720
very data driven. And government itself, I think, needs some transformation. That's the monopoly

63
00:07:52,720 --> 00:07:59,200
that provides the worst service at the highest cost. And it really is the worst service at the

64
00:07:59,200 --> 00:08:05,360
highest cost. It really is the worst kind of monopoly, right? Yeah, it's the kind of monopoly

65
00:08:06,240 --> 00:08:13,120
you think you're stuck with. But it turns out now, government has two layers. They have the

66
00:08:13,120 --> 00:08:21,280
physical layer that is like, you've got to go to the government to see what kind of a building

67
00:08:21,280 --> 00:08:27,680
you're going to build or what services you're going to provide in that building. But then,

68
00:08:27,680 --> 00:08:31,920
there are all these things that can be done virtually, like your pension or

69
00:08:32,640 --> 00:08:39,440
some sort of a welfare state or health care insurance or social security. All of these

70
00:08:39,440 --> 00:08:46,160
can be done at the virtual level. So the geographic borders are no longer important here,

71
00:08:46,720 --> 00:08:53,040
because there's no reason I couldn't get my social security from Chile and my medical

72
00:08:53,040 --> 00:09:01,280
insurance from Canada and maybe a pension from a country that hasn't even been invented yet

73
00:09:01,280 --> 00:09:11,840
and doesn't even have a landmass. So there could be a whole new series of ways people manage

74
00:09:11,840 --> 00:09:19,520
themselves and govern themselves that's coming that isn't tied to the fiefdom that we've

75
00:09:19,520 --> 00:09:29,120
operated under for all these many centuries. So suddenly, you've got finance, enormous industries,

76
00:09:29,120 --> 00:09:36,160
trillion-dollar industries being transformed, financing. Venture capital is being transformed.

77
00:09:36,160 --> 00:09:44,000
In Q3 of last year, there was more money raised in ICOs than there was in venture capital. So our

78
00:09:44,000 --> 00:09:50,720
own industry is getting a transformation. And I'm embracing it. A lot of people kind of ran

79
00:09:50,720 --> 00:09:55,920
and hid in their shells, but I'm embracing it pretty excited about it.

80
00:09:55,920 --> 00:10:01,360
Well, you're embracing it both in business and politics, right? Because I want to ask you about

81
00:10:01,360 --> 00:10:08,400
the CAL 3 proposal. And as far as I understand, blockchain governance was one of the ideas,

82
00:10:08,400 --> 00:10:13,040
if not the principal ideas, that you had in mind for the three states to run themselves

83
00:10:13,040 --> 00:10:23,520
independently, right? Yeah. It was a part of it. Government in California has been pretty much

84
00:10:23,520 --> 00:10:32,960
stagnant for 50 years. Meanwhile, communications and information and all sorts of software

85
00:10:32,960 --> 00:10:39,760
entertainment, they're improving according to Moore's law. They're improving like $1,000

86
00:10:39,760 --> 00:10:46,800
worth of compute power doubles every 18 months. So they're doubling in value every 18 months,

87
00:10:46,800 --> 00:10:54,640
but our government services have been stagnant, even decreased in value over that same period

88
00:10:54,640 --> 00:11:00,400
of time. So this is sort of an interesting opportunity. There's a new technology that can

89
00:11:00,400 --> 00:11:07,040
actually improve government services. And I thought three Californias would be an opportunity for

90
00:11:07,040 --> 00:11:15,600
the worst state, the worst government. It's a great state to live in, but although they say

91
00:11:15,600 --> 00:11:20,320
quality of life, we're the worst quality of life. So we're the worst government. It provides the

92
00:11:20,320 --> 00:11:28,000
worst service at the highest cost. Education's gone from first to 47th. We are now 50th as a

93
00:11:28,000 --> 00:11:37,360
place to do business. We're 50th in quality of life because people can't afford the housing and

94
00:11:37,360 --> 00:11:43,600
the schools are bad and all that. And so I was looking and I was saying, well, this is an

95
00:11:43,600 --> 00:11:52,960
opportunity to create three new states that were fresh and kind of reboot the governance of this

96
00:11:52,960 --> 00:12:03,040
landmass. And what a great opportunity with the blockchain and Bitcoin available. All of these

97
00:12:03,040 --> 00:12:10,240
smart contracts and artificial intelligence and big data, all of them could easily replace

98
00:12:11,040 --> 00:12:19,840
the bureaucrats we have in place in California. And those bureaucrats can go find better things

99
00:12:19,840 --> 00:12:31,520
to do and we can be better managed or better governed through the cloud. And that is something

100
00:12:31,520 --> 00:12:39,200
that the Bunions, the bureaucratic unions, don't like because they were going to lose their people.

101
00:12:39,200 --> 00:12:45,280
And so they did everything they could to thwart me. First with six Californias, they made it so

102
00:12:45,280 --> 00:12:54,160
that the signature counters discounted the number of signatures that I had enough so that it wouldn't

103
00:12:54,160 --> 00:13:03,760
qualify. And then this time, they did it by, we qualified because we got twice as many signatures

104
00:13:03,760 --> 00:13:07,760
as we needed. So there was no way they could say they were no good.

105
00:13:07,760 --> 00:13:10,960
How many signatures did they get? Like 800,000, something like that?

106
00:13:10,960 --> 00:13:21,600
Yeah, we got 650,000 the second. And we needed 300 and something thousand. And so we got all these

107
00:13:21,600 --> 00:13:28,480
signatures and the Secretary of State's office said, okay, and so we were proposition nine.

108
00:13:29,280 --> 00:13:37,840
And so the Bunions set up a shill non-profit corporation to sue us and make it so that the

109
00:13:37,840 --> 00:13:52,080
California Supreme Court judges would have to vote on it just before they were about to print

110
00:13:52,080 --> 00:14:01,280
the ballots. And so that they wouldn't have to print them twice. They just yanked it off the

111
00:14:01,280 --> 00:14:07,840
ballot arbitrarily. That's crazy. And so you don't get to vote on it. I don't get to vote on it.

112
00:14:07,840 --> 00:14:14,800
The only people that got to vote on it were the six Supreme Court justices. So they basically yanked

113
00:14:16,160 --> 00:14:26,160
democracy away from us. And so it is going to be a challenge because these people,

114
00:14:26,160 --> 00:14:31,520
don't want to lose their jobs. They're very concerned about losing their jobs. But their

115
00:14:31,520 --> 00:14:39,680
jobs and their lives are going to be so much better if we have, you know, when you can use

116
00:14:39,680 --> 00:14:45,840
artificial intelligence, you should use artificial intelligence. And it makes you kind of move beyond

117
00:14:45,840 --> 00:14:54,320
it. I mean, if I can have, you know, a car drive me, and I can drive a car, and I can drive a car,

118
00:14:54,320 --> 00:15:01,040
you know, a car drive me without a driver in it, terrific. That's even better than having the

119
00:15:01,040 --> 00:15:05,840
driver have to drive around. Driver can drive around on their own if they like. But they've

120
00:15:05,840 --> 00:15:11,200
got the freedom. And they've been freed up into the economy. And they can do other things. They can

121
00:15:11,200 --> 00:15:25,760
go help us get off the planet or go to help us. They can grow the economy, they can spend money,

122
00:15:25,760 --> 00:15:33,280
they can earn money, they can do lots of different things. So this is, I think people

123
00:15:33,280 --> 00:15:42,160
didn't understand it at first. It was interesting. Our polling went from 8% to 12% to 17% to 27%,

124
00:15:43,120 --> 00:15:53,120
with 35% undecided. And I think that's what the Bunyan saw. They saw that this was a very real

125
00:15:53,120 --> 00:16:02,320
possibility. And so that made them very nervous. And that's why they sued and got the poll.

126
00:16:02,320 --> 00:16:08,400
They got the Supreme Court to yank it off the ballot. It was unprecedented. No one has ever,

127
00:16:10,160 --> 00:16:15,360
the Supreme Court has never done that before the vote. It's like un-American.

128
00:16:15,920 --> 00:16:20,240
Yeah, and definitely not democratic, right? So that's out of window right there.

129
00:16:20,240 --> 00:16:28,160
No, the whole point of it, democracy, is that the people help run, the government works for the

130
00:16:28,160 --> 00:16:33,600
people, not the other way around. And somehow it's, you know, here the people said,

131
00:16:33,600 --> 00:16:37,600
we want to vote. And the government said, we're not going to let you.

132
00:16:38,720 --> 00:16:44,480
Now, Elon Musk was on Joe Rogan's podcast. I don't know if you listened to it. Very interesting.

133
00:16:45,120 --> 00:16:50,320
And he was talking about how he became a fatalist with respect to artificial intelligence,

134
00:16:50,320 --> 00:16:56,640
because he tried to warn people of the dangers of not adapting as fast as we should, you know,

135
00:16:56,640 --> 00:17:00,240
including people in the Congress. And nobody really cared about what he was saying. That's

136
00:17:00,240 --> 00:17:06,000
basically what he was saying. So I'm a fatalist now, just to basically keep my own sanity.

137
00:17:06,000 --> 00:17:11,360
Do you think the government of the United States qualified to address any of these issues? You know,

138
00:17:11,360 --> 00:17:18,480
we've seen some of the tech CEOs testifying in Senate and Congress hearings. And how do you

139
00:17:18,480 --> 00:17:23,200
think government is doing compared to all the advancement and developments that have been

140
00:17:23,200 --> 00:17:31,360
happening? It's interesting, because a lot of these governments are run by very elderly people

141
00:17:31,360 --> 00:17:38,800
who have had very full lives, so that their judgment should generally be better. But they

142
00:17:38,800 --> 00:17:47,600
are not early adopters of technology. And so here, here's sort of an example. Let's say there's a

143
00:17:47,600 --> 00:17:57,280
23 year old, and he's just coming out of college, and he's $200,000 in debt. And, and this is

144
00:17:57,280 --> 00:18:03,440
typical, they've gone to college, and then they've had long big loans, and they're deeply in debt.

145
00:18:03,440 --> 00:18:08,720
And there's no real way for them to get out of debt, other than to just sort of pay it off over

146
00:18:08,720 --> 00:18:14,240
a very, very long time. And then all of a sudden, there's this new currency that comes along,

147
00:18:14,240 --> 00:18:25,920
Bitcoin, and, and this new currency is, is suddenly available, and, and spreading and growing and

148
00:18:26,320 --> 00:18:35,440
value and exciting and frictionless and decentralized, not tied to some grandpa's fiefdom.

149
00:18:36,400 --> 00:18:42,000
And they were saying, you know, well, grandpa set this thing up so that I was 200,000 bucks in debt,

150
00:18:42,000 --> 00:18:50,000
and I was educated in a way that really wasn't very valuable to me. And all of a sudden, now,

151
00:18:50,720 --> 00:18:58,480
there's this interesting opportunity for me. And I think that's what's going on. And now you've got

152
00:18:58,480 --> 00:19:06,800
the grandfathers trying to figure out how to regulate the grandchildren who are coming out

153
00:19:06,800 --> 00:19:12,720
with all these great ideas for new ways of tokenizing things and building things. To the

154
00:19:12,720 --> 00:19:20,480
grandfathers, this is, they might as well speaking, been speaking in Martian. They're, they don't,

155
00:19:20,480 --> 00:19:26,160
they don't understand what's going on. They don't like the idea of a change because, hey, let's face

156
00:19:26,160 --> 00:19:31,520
it, they've built up a whole stack of dollars. And they they're thinking, wait, wait, my dollars

157
00:19:31,520 --> 00:19:36,560
may not be worth as much because there's this new currency that's better frictionless. And

158
00:19:36,560 --> 00:19:46,640
less decentralized, not tied to any, you know, frivolous political source. This is a big change.

159
00:19:46,640 --> 00:19:55,680
This is a sea change. And, and it's generational. You know, if I go to, if I go to somebody,

160
00:19:55,680 --> 00:20:01,520
typical person about my age, who says, and I say, Hey,

161
00:20:01,520 --> 00:20:10,080
can I pay in Bitcoin? They say, they said, absolutely not. I need dollars. And if you go

162
00:20:10,080 --> 00:20:15,440
to somebody who's about 30 years old, you say, Hey, can I pay in Bitcoin? They said, yeah,

163
00:20:15,440 --> 00:20:23,200
I'll do it for you. Even if you do it in line at Starbucks, you can say to the barista, hey,

164
00:20:23,200 --> 00:20:27,600
can I pay in Bitcoin? And if that guy says, sorry, God, they don't have it here yet. The

165
00:20:27,600 --> 00:20:36,320
guy behind you in line says, I'll take your Bitcoin and I'll pay in fiat. And so, so I think the,

166
00:20:36,320 --> 00:20:41,840
the younger group, the up and comers, the ones who are going to kind of make the future

167
00:20:42,880 --> 00:20:49,040
are loving this new technology, they're loving the new currency, they're loving all of that great,

168
00:20:49,760 --> 00:20:56,000
all the great things that could potentially happen with it. And the old people, they're

169
00:20:56,000 --> 00:21:05,520
loving it. And the older generation who are kind of running the old monopolistic system, are

170
00:21:06,880 --> 00:21:12,880
either threatened by it or curious to it. It's interesting, you know, in China,

171
00:21:14,880 --> 00:21:24,480
that guy, the guy who's declared himself king is, is afraid of all of this, right? He lives in,

172
00:21:24,480 --> 00:21:32,960
in fear and, and any loathes, fear and loathing, fears and loathes, the idea of some other currency

173
00:21:32,960 --> 00:21:42,400
that he doesn't control. The SEC in the US is somewhere in between where they're kind of going,

174
00:21:42,400 --> 00:21:49,840
well, we know we need to protect the widows, but we don't want to give up all this great technology

175
00:21:49,840 --> 00:21:56,240
that's going to come out of this. And so they, they're trying to balance it. And in, in Tokyo,

176
00:21:56,240 --> 00:22:03,440
in Japan, Japanese said, Bitcoin's a national currency, here's how you do an ICO, come to our

177
00:22:03,440 --> 00:22:11,280
country. And, and you're seeing that in a lot of smaller countries, Malta, Gibraltar, Singapore,

178
00:22:11,280 --> 00:22:17,760
Estonia, where they're opening up and saying, hey, come to our country. We are open to this new

179
00:22:17,760 --> 00:22:25,760
currency. We're open to this new way of life. And we're not, we're not obsessed with our own

180
00:22:25,760 --> 00:22:32,320
currency. And we do understand that our own currency is subject to political whims. And

181
00:22:32,320 --> 00:22:40,560
that this new currency is more liquid and more transparent and more open. And I can take my

182
00:22:40,560 --> 00:22:48,000
Bitcoin anywhere. I mean, I can go anywhere. I can pull down my Bitcoin. And all of these other

183
00:22:48,000 --> 00:22:55,280
currencies around the world are kind of limited to some geographical area. Are you in favor of a

184
00:22:55,280 --> 00:23:00,000
total free market model for cryptocurrencies or wouldn't or wouldn't mind some regulations and

185
00:23:00,000 --> 00:23:04,480
oversight overall, because that's the problem that government has with them, right? That they can be

186
00:23:04,480 --> 00:23:09,840
used for terrorism and all the other negative things that they claim. Well, actually, the,

187
00:23:09,840 --> 00:23:18,800
the most dangerous currency is, is the paper dollar. That's the one that can't be tracked.

188
00:23:18,800 --> 00:23:25,280
The Bitcoin is completely trackable. In fact, the US Marshals Office has been catching

189
00:23:26,080 --> 00:23:31,760
all of these criminals who were trying to use Bitcoin to do nefarious things, because the

190
00:23:31,760 --> 00:23:42,480
blockchain is, is secure. And if somebody, now if somebody gets Bitcoin, they're not caught then.

191
00:23:43,120 --> 00:23:47,760
But, but what happens then is the Marshall's Office says, Oh, well, that's, that's where the

192
00:23:47,760 --> 00:23:55,760
bitcoins being held. Soon as it moves, we move in. And, and so as soon as people who are nefarious,

193
00:23:55,760 --> 00:24:03,360
try to spend their Bitcoin, they're caught. That's not true with, with paper money.

194
00:24:05,280 --> 00:24:15,760
So actually, you are much more secure as a population if you are using Bitcoin. And in fact,

195
00:24:16,880 --> 00:24:23,520
the banks are getting hacked at every day. All the banks are easily hacked now by plenty of

196
00:24:23,520 --> 00:24:30,800
software developers who come after them in lots of different ways. But the Bitcoin blockchain has

197
00:24:30,800 --> 00:24:36,480
never been hacked. There's been hacks around it in various currencies. But the Bitcoin blockchain

198
00:24:36,480 --> 00:24:43,280
has never been hacked. It is the most secure currency we have on the earth. And, and more and

199
00:24:43,280 --> 00:24:49,360
more miners are proving that to be the case. Every time a new miner is added, the currency

200
00:24:49,360 --> 00:24:56,080
becomes more secure. Do you think the argument for anonymity of a coin like Monero, for example,

201
00:24:56,080 --> 00:25:02,720
in contrast to traceability of Bitcoin has any kind of ground long term wise? And if so,

202
00:25:03,520 --> 00:25:09,760
what would stop again, an alternative community that use anonymous coins on blockchain to

203
00:25:10,560 --> 00:25:17,760
conduct their own business? I warn people all the time. Monero is also on a blockchain,

204
00:25:17,760 --> 00:25:25,840
all of those around a blockchain, you are not, you are not incognito, you act absolutely can

205
00:25:25,840 --> 00:25:33,520
be found out. And so I don't know why people just don't say, hey, let's open up, we're going to be

206
00:25:33,520 --> 00:25:41,680
transparent. Here's my Bitcoin. This is where it is. No problem. And why they're trying to use these

207
00:25:41,680 --> 00:25:48,400
dark places, because they're just gonna, I mean, they're gonna be caught. There's, there's no

208
00:25:48,400 --> 00:25:52,400
question about it. They are on a blockchain, they're going to be caught if they're doing

209
00:25:52,400 --> 00:26:01,440
something nefarious. So yeah, I don't buy the fact that some of them say, oh, yeah, we're the dark

210
00:26:01,440 --> 00:26:06,960
coin or the Monero or whatever, I don't buy it. Because no matter what, they're all going to

211
00:26:06,960 --> 00:26:13,920
eventually open up. So there is no true anonymous coin as of yet, or? Well, I think the criminals

212
00:26:13,920 --> 00:26:19,280
are trying to stay ahead of the law. Right. But I don't think they can. I mean, eventually,

213
00:26:19,280 --> 00:26:25,120
they all get caught. What are the most exciting developments and areas in tech for you today?

214
00:26:25,120 --> 00:26:29,760
We know blockchain is one of them. Well, I'll give you one kind of fun example. And that is,

215
00:26:29,760 --> 00:26:38,080
in healthcare, let's say, you put you control your data, okay, flip it around, right now,

216
00:26:38,080 --> 00:26:46,720
the hospital has all the data, you control your data, it's on the blockchain, you're using a,

217
00:26:47,440 --> 00:26:54,240
a using Bitcoin as a, as an insurance through the insurance program, you're paying in Bitcoin and

218
00:26:54,240 --> 00:27:03,760
you're, and the all your medical visits are paid in Bitcoin. And you're putting all of your

219
00:27:03,760 --> 00:27:11,200
information, your medical records, your, your blood test results, your genetic history, that all goes

220
00:27:11,200 --> 00:27:19,680
into the cloud. But not just that, your Fitbit results, what you had for breakfast, where you

221
00:27:19,680 --> 00:27:30,160
traveled recently, what your kind of work you've been doing, how much travel you've done, what

222
00:27:30,160 --> 00:27:38,960
stress you've been under all those things are put into your data. And then that data goes up into

223
00:27:38,960 --> 00:27:46,320
the cloud. And then you say, I've got a headache. Okay, if you say you've got a headache, it might

224
00:27:46,320 --> 00:27:51,920
be like, Oh, take two aspirin and call me in the morning, I say I've got a headache. They say, Oh,

225
00:27:51,920 --> 00:28:02,320
boy, you've been on Fire Island, you might have Lyme tick disease. And we need antibiotics. And so no

226
00:28:02,320 --> 00:28:11,920
doctor in the world in 12 years can memorize all of that data. And all of the, all of the little

227
00:28:11,920 --> 00:28:20,000
changes that would happen to people's lives, as well as people taking their information, putting

228
00:28:20,000 --> 00:28:24,800
it into the cloud and having the artificial intelligence of the cloud, the deep learning of

229
00:28:24,800 --> 00:28:33,200
the cloud, big data of the cloud, all combine and get you your diagnosis. This is going to be a

230
00:28:33,200 --> 00:28:41,120
magic time, people are going to be get much better, better care and your doctor, I suspect will be

231
00:28:41,120 --> 00:28:53,600
become more of a consultant than a than a direct dictator of what you should be doing. And, and that

232
00:28:53,600 --> 00:29:02,240
will be because they will have this, this advisor with them, which is the cloud. And then your data,

233
00:29:02,240 --> 00:29:08,800
you'll get money for selling your data into the drug companies and the hospitals and whoever else

234
00:29:08,800 --> 00:29:15,360
you want to sell your data into. Now, a lot of people say that any technological progress works

235
00:29:15,360 --> 00:29:20,560
as best as a society that adapts it. So how are we doing as a society in your opinion? Because I

236
00:29:20,560 --> 00:29:24,400
remember a couple of years ago, people were attacking other people who were wearing Google

237
00:29:24,400 --> 00:29:29,520
glasses, I think it was in San Francisco, or we're getting really pissed off at tech companies

238
00:29:29,520 --> 00:29:34,640
because of rise in the housing price. And they were they were blaming tech, so they should blame

239
00:29:34,640 --> 00:29:42,880
government there. California allows people to create startups, it allows those startups to become

240
00:29:42,880 --> 00:29:50,640
worth something. But then as soon as those startups want to employ people, California makes it

241
00:29:51,120 --> 00:30:00,480
untenable to operate here. I mean, sure, there are a few, you know, Facebook still has their headquarters

242
00:30:00,480 --> 00:30:09,600
here, Salesforce. But most of them, when they get big, Tesla's battery factory is a perfect example,

243
00:30:09,600 --> 00:30:14,320
they're going to set up a battery factory is going to employ 10s of 1000s of people. Well, they,

244
00:30:14,960 --> 00:30:21,440
they wanted to put it here in California, but California made it untenable. And they had to

245
00:30:21,440 --> 00:30:28,800
push it out so that they looked around, they went to Nevada instead. Now, what happens then?

246
00:30:28,800 --> 00:30:36,800
All the jobs leave California. And so who's left? The people who made a bunch of money on that,

247
00:30:37,280 --> 00:30:44,800
on that startup, and these people who can't get jobs because all the jobs just left. And California

248
00:30:44,800 --> 00:30:51,520
says, Oh, it must be the business people, and it must be the tech people, it must be the something

249
00:30:51,520 --> 00:30:59,200
people, it must be the something people. It's not it's the government shoving big businesses out of

250
00:30:59,200 --> 00:31:04,640
the state, we want the big businesses here, so that they can employ the middle class so that they

251
00:31:04,640 --> 00:31:11,600
can employ those people who are right now out on the street and homeless, because I am jobless,

252
00:31:11,600 --> 00:31:20,080
because we don't have the jobs for them. But we would have if we had made it so that

253
00:31:20,080 --> 00:31:26,000
put his battery factory here, or we would have if, if they hadn't pushed Sony and all of the

254
00:31:27,600 --> 00:31:33,360
studios out of the state, so that they go and they do their their filming somewhere else.

255
00:31:36,080 --> 00:31:39,680
Oh, Amazon was going to put their second headquarters somewhere else.

256
00:31:41,200 --> 00:31:46,640
California didn't even make the second cut. They were thinking, Yeah, well, we'll check out

257
00:31:46,640 --> 00:31:50,560
Sacramento when they were there for like 10 seconds, and they realized, Oh, my God,

258
00:31:51,280 --> 00:31:57,120
these guys are going to regulate us to death. There's no way we want to employ people here.

259
00:31:57,120 --> 00:32:02,160
So would you be in favor of a Republican governor for California if that person runs on the policies

260
00:32:02,160 --> 00:32:07,760
that are closer to what you were saying as a business ideal? You know, I'm not I'm no longer

261
00:32:07,760 --> 00:32:13,120
a Republican or a Democrat. I was both I was once a Republican, and then I was a Democrat. I'm a

262
00:32:13,120 --> 00:32:21,440
declined estate, because both parties are building the duopoly that we suffer under.

263
00:32:22,640 --> 00:32:31,520
We need we need a major change in government. Now, somehow, the bunions have grown to be where

264
00:32:31,520 --> 00:32:36,000
they're, they're so big and so powerful, we don't really have a representative government anymore,

265
00:32:36,000 --> 00:32:44,320
right, as proven by Cal three. So, so I don't know whether a Republican or a Democrat will make any

266
00:32:44,320 --> 00:32:53,760
difference. I think it has to do with a major overhaul to the system. And, and that's not going

267
00:32:53,760 --> 00:33:00,960
to happen unless somebody, unless the Supreme Court lets Cal three go through or, or something

268
00:33:00,960 --> 00:33:08,480
else. I mean, and of course, the you know, the press will beat it down for a long time before

269
00:33:08,480 --> 00:33:17,440
they embrace it and say, Oh, yeah, it was our idea all along. You ever watch the watch a sports game

270
00:33:17,440 --> 00:33:22,560
where it's sort of an underdog somehow comes and wins the game in the last minute, right?

271
00:33:23,440 --> 00:33:28,800
You listen to the sportscasters and they go, Oh, yeah, I knew this was gonna happen. So and so,

272
00:33:28,800 --> 00:33:34,480
you know, this team, the favorites gonna win and that's the way it's gonna be and blah, blah, blah,

273
00:33:34,480 --> 00:33:40,240
how great they all are. And then all of a sudden, the underdog comes and overthrows them and wins

274
00:33:40,240 --> 00:33:47,440
the game. And, and then they go, Oh, boy, that guy so and so is so great. We knew about it all along.

275
00:33:47,440 --> 00:33:54,320
He came from nowhere, blah, blah, blah. And, and there's no comment on like, we got it so wrong

276
00:33:54,320 --> 00:34:01,840
before. But right now, the press is just sort of, you know, the problem that supports what the

277
00:34:01,840 --> 00:34:07,520
government's doing. And, oh, we just leave it the way it is. And we'll slowly all walk off a cliff,

278
00:34:08,400 --> 00:34:16,080
as opposed to, hey, there are some interesting things happening here. We have a real problem.

279
00:34:16,080 --> 00:34:26,480
Let's wake up. But I think that's the free press is a wonderful thing here, because it is, it is

280
00:34:26,480 --> 00:34:33,760
so valuable to the United States. It, I mean, at least people can take shots at their president,

281
00:34:33,760 --> 00:34:43,600
that at least people can say what they think. I guess unless the Supreme Court of California

282
00:34:43,600 --> 00:34:52,560
rules against them. Yeah, that's very unfortunate. It's worse than that. It's like 250 years of a

283
00:34:52,560 --> 00:34:58,080
democracy that was working, and all of a sudden, they're clamping it down. But are you going to

284
00:34:58,080 --> 00:35:05,520
push for another, not a form of Cal 3 like model, maybe in California or even somewhere else?

285
00:35:05,520 --> 00:35:13,360
Well, maybe you can do that. I'm exhausted. And I got blindsided twice. Yeah, so you've had enough.

286
00:35:13,840 --> 00:35:19,760
Maybe one of our listeners is going to listen and say, Hey, how do we, how do we make this better?

287
00:35:20,400 --> 00:35:25,600
Honestly, I see the government, as I mentioned in these hearings and stuff, and you know, just the

288
00:35:25,600 --> 00:35:30,480
questions and the fight over, you know, it's just ridiculous. And you're like, nobody's talking

289
00:35:30,480 --> 00:35:33,840
about artificial intelligence. Nobody's talking about big data. Nobody's talking about big data.

290
00:35:33,840 --> 00:35:40,240
Nobody's talking about algorithms. They started talking about algorithms with respect to social

291
00:35:40,240 --> 00:35:46,400
media, when you know, but they don't really understand what's going on. I know, I wish they

292
00:35:46,400 --> 00:35:56,640
would. But, you know, we're, we, we bolt them in. And then we keep them in forever. I mean,

293
00:35:56,640 --> 00:36:02,720
there are term limits on some of them, I guess, we keep them in forever. But what's worse than,

294
00:36:02,720 --> 00:36:07,760
than the politicians, because the politicians actually have to compete for their jobs,

295
00:36:09,120 --> 00:36:16,560
or the bureaucrats, the bunions, they don't have to do anything. They can stay there forever.

296
00:36:17,600 --> 00:36:23,360
They look at politics like a new president, new president should be like, hey, this is awesome.

297
00:36:23,360 --> 00:36:27,520
We got a new president. But they look at the new president, they say, you'll be here for four years,

298
00:36:27,520 --> 00:36:34,400
maybe eight, we're here forever. Any policies you put in, we can sit on.

299
00:36:35,040 --> 00:36:37,120
Is that a deep state that Trump's talking about?

300
00:36:37,840 --> 00:36:46,080
I'm not sure what Trump's talking about. But when we had, there were some brilliant people that put

301
00:36:46,080 --> 00:36:54,480
together this jobs act. And it went through and it passed through law, the Congress passed it,

302
00:36:54,480 --> 00:37:02,080
the president signed it. And, and, and it was going to allow crowdfunding, it was going to allow

303
00:37:03,840 --> 00:37:09,040
small investors to invest in private companies, it was going to allow a whole bunch of things

304
00:37:09,040 --> 00:37:16,240
that were going to create a lot of jobs. The bureaucrats didn't like it. And they just sat on

305
00:37:16,240 --> 00:37:23,600
it forever. And then they just kept adding more and more terms. And they took all of the wings

306
00:37:23,600 --> 00:37:32,160
off of the law. And so now, you know, you see any crowdfundings? I mean, we did a little bit with

307
00:37:32,160 --> 00:37:37,760
Meet the Drapers, but, but I'm not seeing a lot of crowdfundings. Because they've just they've

308
00:37:37,760 --> 00:37:45,440
added so many hurdles and so much bureaucracy and so many forms to fill out. They've basically taken

309
00:37:45,440 --> 00:37:53,920
the wings off of that. What's the solution to go beyond them? Well, I think you've got to be able to,

310
00:37:53,920 --> 00:38:02,240
you know, unions were were started so people could take care of themselves and gang up if,

311
00:38:03,120 --> 00:38:13,040
if a boss was giving them bad, a bad shake, where it's like the the mines were bad for their health

312
00:38:13,040 --> 00:38:19,360
or the, you know, the factory was making them work too many hours or they couldn't get sleep,

313
00:38:20,000 --> 00:38:29,520
or is unsafe. And those are all good uses of a union. But right now, the Bunions have negotiated

314
00:38:29,520 --> 00:38:38,080
for that the public unions have negotiated for where, where we go out and we pay the bills

315
00:38:38,080 --> 00:38:44,480
and, and pay for our government. Well, the average government employee makes more money

316
00:38:45,520 --> 00:38:53,120
than the average private citizen. And they take longer vacations. And they have a shorter work

317
00:38:53,120 --> 00:39:02,400
week. And they have a bigger pension. And so somehow, these people who are who we have

318
00:39:02,400 --> 00:39:11,280
hired to go in there, are not leaving ever. And, and are not even allowing the new laws to go

319
00:39:11,280 --> 00:39:19,920
through. So if you want something to happen, it's got to start with a bunions. Do you see any country

320
00:39:19,920 --> 00:39:25,760
that is doing a good job with adapting to new form of technology in in terms of running their

321
00:39:25,760 --> 00:39:33,600
country? Yeah, Japan, Japan, right, right. Awesome. And Singapore, of course, for years has been

322
00:39:33,600 --> 00:39:40,000
awesome. And China before they got this new dictator, China before they got the new dictator,

323
00:39:40,000 --> 00:39:46,720
they were open, they had a free market, they were encouraging innovation, they were driving

324
00:39:46,720 --> 00:39:51,520
business, things were going well, people were getting employed, the quality of life had gone

325
00:39:51,520 --> 00:40:01,760
way up. Poverty was going way down. And, and then now they got a dictator who, who is controlling

326
00:40:01,760 --> 00:40:10,640
everything and, and, and the free markets are getting pulled back on. Well, and now all of

327
00:40:10,640 --> 00:40:24,480
these little countries, Malta and Gibraltar and Cayman and Bermuda and Singapore, Estonia,

328
00:40:24,480 --> 00:40:31,120
Latvia, Lithuania, all these smaller countries are looking at this as a big opportunity. And

329
00:40:31,120 --> 00:40:34,960
they are embracing new technologies or seeing what they can do to make it better.

330
00:40:34,960 --> 00:40:39,680
Yeah, something that is very interesting about Japan is that how society has adapted already to

331
00:40:39,680 --> 00:40:45,040
different forms of robotics and technology, you know, from anime and cartoons to those crazy

332
00:40:45,040 --> 00:40:51,200
vending machines and their robot guides all around shopping mall. There is no this kind of a social

333
00:40:51,200 --> 00:40:57,520
acceptance of robotics in the United States. Would you agree? Well, in the US, they like to

334
00:40:57,520 --> 00:41:08,240
sell fear in the, in the news. And so they want, they want to, there are probably equal, an equal

335
00:41:08,240 --> 00:41:18,800
number of stories saying how unions, I mean, how robots are going to take over the world or, or

336
00:41:18,800 --> 00:41:23,200
artificial intelligence is going to take all our jobs. There are probably more stories about that

337
00:41:23,200 --> 00:41:27,920
than all the really cool things that are going to happen because of robotics and artificial

338
00:41:27,920 --> 00:41:33,920
intelligence. I mean, we're working, we've got a company that makes a, makes a pizza with a robot.

339
00:41:33,920 --> 00:41:42,880
So cool. And a perfect pizza every time. And artificial intelligence is, is making it so that

340
00:41:43,680 --> 00:41:49,840
we are, by the way, we're all operating with artificial intelligence today. We just don't,

341
00:41:49,840 --> 00:41:55,920
we just don't recognize it as such. The spell checker, that's artificial intelligence. The,

342
00:41:57,120 --> 00:42:04,560
the recommendation engines from the social media, that's artificial intelligence. Those things are

343
00:42:04,560 --> 00:42:13,280
good. They're great. We get better recommendations. We spell better. We, all those things are just like

344
00:42:13,280 --> 00:42:22,880
an intelligence item that's moved, moved forward. And hey, I mean, if, if I'm able to raise, the

345
00:42:22,880 --> 00:42:29,760
moment I'm able to raise a fund all in Bitcoin, fund companies all in Bitcoin and have those

346
00:42:29,760 --> 00:42:36,640
companies pay all their employees and suppliers all in Bitcoin, that's the moment I will no longer

347
00:42:36,640 --> 00:42:43,760
need an accountant. So think of all those really smart people who are accountants and what all the

348
00:42:43,760 --> 00:42:54,960
cool things they can do are. And so I think it's just better. I think the biggest challenge so far

349
00:42:54,960 --> 00:43:01,920
is that what, what is going to happen to the user's data and how they can be prevented from being

350
00:43:01,920 --> 00:43:06,720
manipulated or, you know, I've always found the business model of Facebook being a platform that

351
00:43:06,720 --> 00:43:12,640
depends on users data, but then the users are not getting paid by generating data and sharing data.

352
00:43:12,640 --> 00:43:18,640
So that's where our, that's where our company data wall, it really has it right. They give you

353
00:43:18,640 --> 00:43:25,680
control of your own data. So you can figure out whether you want to use it for this ad, use it

354
00:43:25,680 --> 00:43:33,600
for this company, and you get paid for that. So I think that's going to be the future of data and

355
00:43:33,600 --> 00:43:39,920
it has to be flipped from whether the way it works now. But I think that's going to be happening.

356
00:43:40,560 --> 00:43:45,680
Allow me to ask you the last question that I ask all my guests. If you come across an intelligent

357
00:43:45,680 --> 00:43:50,800
alien from a different civilization, what would you say is the worst thing humanity has done and

358
00:43:50,800 --> 00:44:02,240
what would you say is our greatest achievement? The worst thing is clearly wars. I think the

359
00:44:02,240 --> 00:44:21,680
greatest achievement is yet to come.

