This is how machine learning and AI and many other algorithms
work is like there's an objective function
and you're trying to optimize it.
And really where the social discussion
and the personal intervention needs to happen
is at the level of what is
or what should be that objective function.
How it gets optimized is really like a technical problem.
It's like, how do you make the engine run faster?
Well, you know, let the engineers worry about that.
And then you decide what speed you want to drive it at.
And the thing is that maximizing engagement
as an objective function is actually
a perfectly natural thing to do.
The problem is the side effects.
Right, the consequences.
Exactly. Right.
So the algorithms are to blame, yes,
but the engineers are to blame,
not for their evil intent,
but for just having set the algorithms
to maximize this thing without realizing
all the consequences that we'd have.
Which in fairness, in the early days,
it was easy to miss that, right?
Again, I don't think people were being evil
or really stupid about it.
They were just oblivious.
And we can't afford to be oblivious at this point.
Dr. Pedro Dominguez, welcome to the 90th episode
of Neo Human Podcast, sir.
Thanks for having me.
Yeah, it's a pleasure.
I've heard your name for the first time
because of your book, obviously,
The Master Algorithm, How to Quest
for the Ultimate Learning Machine Will Remake Our World.
But then on the bookshelf
of China's President for Life, President Xi,
was two artificial intelligence-related books,
and yours was one of them.
How did that feel hearing something like that?
Because I would imagine that it was not coordinated with you.
No, it wasn't coordinated, so I wasn't expecting it.
And it felt, well, mixed feelings.
On the one hand, it's good that China,
at the highest levels of its leadership,
understands that AI is important
and that they want everybody else in the country
to pay attention to it.
And I think a lot of good things will come of it,
a lot of applications in many different areas.
At the same time, seeing a totalitarian regime
get excited about AI, and my book helping with that
is not a very reassuring feeling.
Yeah, it must have been quite a unique experience.
What was the thesis of the book,
if you don't mind describing it a little,
and why did you write it when you did?
Yeah, so the book is an introduction to machine learning
for a general audience.
And the reason I wrote it is that I believe
we've come to a point where it's not enough
for machine learning to be known by just the experts.
Everybody needs to understand machine learning
at some level because it really affects everybody's life
in many ways, some of them very important.
From presidents and CEOs on down to everybody
in their professional private lives,
you need to understand what machine learning does
and what it doesn't do so you can,
number one, make the best use of it,
for example, professionally and even personally,
so that you know how to interact with the systems
that are using machine learning all the time,
like Google and Twitter and Facebook and so on.
And also so that in a democracy,
there are a lot of important decisions to be made
about how machine learning gets used and doesn't,
and they're supposed to be made by everybody.
And what I saw that was frustrating me
was that at the time I wrote the book,
and even more so now,
is that there was an enormous amount of discourse
about machine learning that was really, really uninformed
by the realities of machine learning.
This was when, 2015?
Well, so I started writing the book in 2012.
The timing was actually perfect because, you know,
by the time the book came out in 2015,
the interest in machine learning had gone up,
like an AI had just gone up to a different level.
I mean, I first thought of writing a popular science book
about machine learning when I was a grad student,
in the nineties,
because already then machine learning was taking off
and you're seeing some press stories about it.
It's like this thing about exponential growth
is that you think something didn't exist
until it bursts onto your view,
but then you see that then it reaches
the next level of people.
But at the time, you know,
I didn't feel an enormous urgency about it.
And also I didn't have a good idea
of how to write a book about machine learning.
Because writing a book for a general audience
is not like writing a textbook, right?
When you're writing a textbook, you can say like,
oh, here's a chapter about neural networks,
here's a chapter about vision networks.
But that's too boring, right?
Nobody's gonna read that.
Right, not anymore.
Well, yeah, maybe, good point.
But nevertheless, it's helpful to have a story, right?
To have a theme.
And so I studied, you know,
how to write these kinds of books.
And there's a certain number of schemas that you can use.
One of them, popular in science books,
is the mystery that you wanna solve or the quest.
And immediately I realized that the right story
for this book is the quest for the master algorithm.
Because that's what really,
and that's where the title of the book comes from.
The thing that is really remarkable about machine learning
and different from everything we've seen before
and central to what it does,
is that one algorithm can learn
to do an infinite number of different things, right?
Turing machines can do anything,
and that's the power of computers,
as Turing first explained.
But if you want a computer to play chess,
you have to program it to play chess.
If you want the computer to fly an airplane
or be a search engine, you have to program it to do that.
The thing that's amazing about machine learning
is that you don't need to do that.
Just one learning algorithm,
if you feed it the right data,
will learn to do all these different things.
Case in point, look at the back prop, right?
Back prop is used to do everything, right?
So the idea of a master algorithm
is like the idea of a master key, right?
It's one key that opens all the locks.
But of course, this is an ideal.
We're not there yet.
There's many different ways to do it.
And that's a lot of what the book is,
different paradigms in machine learning
and how you might unify them all at the end of the day.
And also what the applications in the world
of machine learning are today.
And also, the last chapter in the book,
which in some ways for a lot of people
is the most interesting one, is a look at the future
and where things are headed as we use more machine learning.
And honestly, a lot of the stuff that I wrote about
back in 2015, not only has it come true,
it has exceeded my predictions.
The dystopian side of things?
Not just, it's funny that you would put it that way.
I think this is true of every technology.
And a lot of things that are true of every technology
are particularly true of AI because AI really
fires people's imagination for better and for worse.
And usually in the first wave of a technology,
people are dazzled by the potential.
And this I think is where things were in 2015.
But one of the things that I predicted then
is that, well, there's all these concerns
like privacy and whatnot, and sooner or later
this is going to blow up.
And it has blown up in the biggest possible way.
And then what happens when the concerns come to the fore
is then they take center stage and everybody's paranoid
and focused on the dangers and the dystopian scenarios,
which is where we are now.
Everybody's just completely going off the rails
with the dangers of AI in a way that I
think is also unbalanced.
And then finally, there comes the point
where people just settle down to making things work,
and then they do make things better.
Maybe not quite as the utopia envisaged,
but also preventing the dystopias.
And I have no doubt that the same thing will
happen with AI if we work on making it happen.
The future of AI is not utopian or dystopian
in some predetermined way.
It's what we make of it.
It will be good because we make it good, which, again,
gets back to the motivation for writing the book,
is that we need input from everybody on what to do with this.
Also, there's another, it's exactly, precisely,
AI is a tool.
Exactly right.
AI is a tool.
And a lot of people talk about the power
that AI gives, x, y, or z.
And I think people need to bear in mind
is AI is a tool for those who know how to use it.
If you don't know how to drive a car,
it won't take you where you want to go.
You don't need to understand how the engine works.
That's for the engineers.
But you need to understand where the steering
wheel and the pedals are.
And the idea of the book is to tell people not necessarily
about the engine, but about the steering wheel and the pedals
so that it can be a tool for you so that it makes you powerful.
We don't want AI to make total-terrain governments
powerful or companies that are already over mighty,
even more over mighty.
We want AI to be a tool that empowers every single one of us.
That's the goal, and that's very much why I wrote the book.
Yeah, I think one of the reasons that the dystopian side
of things are becoming more and more maybe attractive
would be an interesting term to use it for a lot of people
is that they're experiencing it firsthand, especially
if you're coming from a certain way of thinking about politics
and life and philosophy and all that.
You just see that you're being shut down,
you're being silenced, you're being deplatformed.
And what you hear is that a lot of people
who are running these companies, they are blaming algorithms.
They're like, well, it's not our fault.
It's the algorithm's fault that prioritized, for example,
liberal headlines against conservative headlines.
It's just an example.
So for a lot of people, this seems
like an existential threat to the way of their life
and the way of their thinking.
And they kind of connect this to algorithms and AI,
which to me is kind of ruining the potential for all
the good stuff that can happen in the coming years and decades.
I do believe that we depend on it
because we've evolved with our technology
to get where we are.
We haven't done it separately.
You're right that people are now experiencing it firsthand.
But what happens is actually that, again, AI
is like the blank canvas onto which people project
whatever preoccupations they have, particularly
political ones.
And for example, liberals are very concerned with fairness,
and they tend to see in AI algorithms
just a cesspool of bias that isn't really there,
but they think needs to be dealt with.
On the other hand, if you're more
of a libertarian persuasion, what you see in AI is big brother.
It's this thing that's going to oppress you.
And the thing to realize is that none of these concerns
are unfounded on any side of the spectrum.
But in many cases, people perceive the threat
to be not just bigger than it really is, but different.
And again, if you understand what's really going on,
actually, you can then address the problems properly
as opposed to all the many very confused things that
are flying around.
Case in point, when you say that the companies say,
oh, don't blame us, blame the algorithms,
this is actually both true and false.
It's true in the sense that, yes, the algorithms
are making these decisions.
And for example, when a lot of conservatives say,
oh, Google, Twitter, et cetera, they're
discriminating against us, I was actually
very skeptical of that for a long time in the past
for the reason that, I mean, obviously, the work
forces at these companies are overwhelmingly liberal.
So that is certainly a cause for worry.
But I have never seen evidence of people deliberately
plugging their liberal politics into the algorithms.
So I don't think that, a lot of the times,
people interpret an algorithm as having an x, y, or z
when, in fact, it was just optimizing
its objective function.
In more recent times, as the decisions
have started to be made by people and by the CEOs
and the pressure from their work forces,
now, I really think there's something to worry about.
But again, the problem there is not the algorithms.
Actually, it's the people making the decisions.
But to get back to the algorithms,
it's very easy to blame the algorithms.
But the algorithms were designed by people,
in particular, machine learning algorithms.
And again, the problem is not so much
that they were designed by, say, liberals
with a liberal takeover in mind, the silent conservancy
of voices.
The problem is that the algorithms
were designed to maximize engagement, which
maximizing engagement, these days, has a really bad name.
That's an incentive, basically.
Yeah, I mean, again, this is how machine learning and AI
and many other algorithms work is there's an objective function
and you're trying to optimize it.
And really, where the social discussion
and the personal intervention needs to happen
is at the level of what is or what should
be that objective function.
How it gets optimized is really like a technical problem.
It's like, how do you make the engine run faster?
Well, let the engineers worry about that.
And then you decide what speed you want to drive it at.
And the thing is that maximizing engagement
as an objective function is actually
a perfectly natural thing to do.
The problem is the side effects.
Right, the consequences.
Exactly.
Right.
So the algorithms are to blame, yes,
but the engineers are to blame not for their evil intent,
but for just having set the algorithms
to maximize this thing without realizing all the consequences
that we'd have, which in fairness, in the early days,
it was easy to miss that.
Again, I don't think people were being evil or really stupid
about it.
They were just oblivious, and we can't afford
to believe we're just at this point.
The objective that you're talking about
is exactly the problem that we have with alignment problem.
Right?
Big picture-wise, because the objective
can be narrow, that I'm hungry.
The objective is to get to the kitchen.
I go down the stairs, and I get plate, blah, blah, blah.
But then there are bigger objectives,
that this is where we get to AI ethics, which for years,
I've asked people without even knowing
that there is such a role exists as AI ethicists, that to me,
that's terrifying.
But hopefully, we can talk about it.
But for years, I asked people on this podcast, technologists,
philosopher, developers, whether or not
ethics and morality is objective or subjective.
And every single time I heard the answer, it is subjective.
Unless we are creating a closed context, which to me
is a disservice if you're going to explore the unknown
and there are a bunch of people sitting up there in charge,
I would assume they think, of to be the master algorithm.
And they're like, we're going to parent you.
We're going to teach you what is good, what is bad,
what is right, what is wrong.
I don't care who you are, what gender you are,
what color you are, what race you are.
You are not in the position, especially
in the United States of America, to define these kind of values.
That's why we say, in God, we trust.
Well, I couldn't agree more.
And in fact, if somebody tells you that ethics is objective,
what you should do is run away as fast as you can.
I'm from Iran, man.
I'm coming from Iran.
I was a refugee in Canada.
I'm an immigrant here.
I had to be human traffic out of Iran.
And the major problem there is that some dude is sitting up
there and is like, I'm telling you the orders of God.
And this is how you have to live your lives, basically.
And they have monopoly on violence, both of which
they try to do right here in the United States.
And I think most Americans think that what happened in Iran
cannot happen in America.
It totally can happen.
Yes.
I mean, in fact, that I think is the big worry,
is that anybody who's lived in a totalitarian regime
of any stripe, whether it's Iran or the Soviet Union or China,
et cetera, et cetera, they recognize what's happening.
I grew up in Portugal, which was a fascist country when
I was born.
The secret police had to file on my dad
because he was a professor and he was pretty outspoken.
And then there was a revolution.
And for a couple of years there, Portugal
wasn't expected to becoming a communist country
because the communists were taking power.
And I recognize everything that I
see happening in America today.
There's some new variations.
But the essence, and again, it's not surprising to me
that a lot of people who have come from countries
or have close relatives who live through these things,
they're the ones who are most alarmed.
The problem is that the Americans don't actually
have the pattern.
Most Americans don't have the pattern recognition
for what is going on here.
But there is a set of people who are
trying to impose their ethics on everybody else.
And of course, one of the things that's
new about today is that there are these technologies
that while they can be great tools for democracy,
they can also be great tools for totalitarianism.
In fact, in some ways, a big AI is bigger than big brother.
It's something that not even big brother could have dreamed of.
And that's what's really alarming.
I hear a lot of people worrying about like, oh,
these companies, they're manipulating you.
And I can't get excited about that because all they can do
is try to sell me products and I refuse to buy them.
That doesn't keep me up at night.
But AI in the hands of the state,
they can do a lot more than sell me products.
So that, I think, is something we seriously
need to worry about.
So what are your thoughts about AI ethicists,
especially in a certain company that I'm trying not to name,
but I know that you took a public stance with respect
to address this issue.
That basically, I think Naval, Naval Ravikant,
he tweeted a couple of days ago that scientists
who are opposed to change something to this nature,
they're not scientists, they're priests.
And I see this kind of approach to make an ethical AI
while your ethic itself is coming
from a place that is, to me, completely distorted,
is not based on reality, is completely ideological.
What is the dynamic?
And please describe that to, because I'm
sure most people, they don't even know such a thing exists.
But I believe it's such an important part
of this whole thing when you are determining values
of this machine that can basically take control
of every aspect of our lives.
Yeah.
So AI ethics actually exists and has existed for many decades.
Most people, including many, I think, current AI,
so-called AI ethicists, don't realize
that people have been debating the ethics of AI
from day one, from the 1950s on.
There are people who have made their multi-decade careers
thinking about the ethics of AI and the alignment problem
and things like this.
So this is actually not a new field.
It used to be a tiny field, also because AI itself was small.
But what's happening in AI ethics is actually
very different from that, very, very different.
What's happening, so first of all,
ethics is a very benign word.
Who could disagree with ethics?
And of course, AI raises ethical issues.
But what we're seeing in AI today
is actually people of a particular political orientation
seeking to impose their politics on AI
and as a result on society.
And they're using ethics as an instrument.
Yeah, oh, exactly.
And I think they're completely, or some of them at least,
are completely open about this.
It's a tool.
This is a tool that they're going to use to achieve their ends.
And then it's easy to dupe people into like, oh,
but there's these ethical concerns
and they're addressing them.
What could be bad about that?
You have to actually look at what is being discussed
and what is being proposed.
And in particular, what is being proposed in the AI ethics
is, for example, probably the biggest example
is under the guise of AI fairness,
people are actually proposing and implementing algorithms
that force equal outcomes.
Equity.
By race, by gender, what is called equity.
If I have x percent of women at the input,
I have to have x percent at the output.
And now, you may disagree or agree with this.
Reasonable people agree, reasonable people disagree.
I very much disagree, but I respect the people who agree.
But the bottom line is, this is politics.
This is not ethics.
This is a political question to be discussed
as a political question.
But what we're having right now is
it's being swept under the rug as if it's ethics.
And then everybody, and this is kind of like where
I came into this, is like, there's
an attempt to impose this at conferences and so on.
For example, if you now, these days at NeurIPS,
if you have a paper that they think promotes unfairness,
by their definition, the paper can get rejected
because it's subject to this ethical, political review.
And again, the people organizing this
are not actually shy about saying
what they want to accomplish.
So this is, and we have to fight back against that.
Because what's going to happen if we don't is,
and this is in fact what is happening,
and what I'm trying to combat is, then the society at large,
what they hear is, oh, the AI community says
that this is the right thing.
This is this consensus of the community,
and it's what science says.
Science says is a very powerful pair of words.
And what you do, and these people are, some of them
at least quite practiced, is like, if you have an agenda,
you go to the relevant scientific community,
you silence all the people who disagree with the agenda,
and then you say, science says that what science says
is what you want to do.
Yeah, exactly, the science is settled.
Like, are you kidding me?
Yeah.
Exactly, yeah.
Yeah, AI is just like, so if you've
been observing these things for a while, in many ways,
what is happening in AI is not a surprise.
In fact, I started looking at this happening,
again, several years ago.
And unfortunately, the details vary.
Some of them can still shock you.
But the way things have been going is, unfortunately,
not surprising at all.
And people need to be alerted.
And I'm trying to alert people to this.
And also, to combat this in the context of the AI community,
the problem, of course, is that this politics also
comes with cancel culture, where there's a lot of people
who are against this.
There's many who don't understand and are oblivious.
But there's a lot of people who are against.
But they're just afraid to speak up,
because they're afraid of the consequences, again,
just as in a totalitarian regime.
People, they know about the Gulag and the KGB.
What they don't know is that most of the enforcement
in the Soviet Union, that was just the last resort.
It was done by exactly the kinds of things that we see today,
is like the peer pressure, the loss of employment,
the retaliations, et cetera, et cetera.
The term political correctness was invented
in the Soviet Union.
Precisely to describe this type of thing, in some ways,
regardless of what the exact content is,
the content of political creedness in the US
might be different than it was in the Soviet Union,
because the ideologies are somewhat different.
But the modus operandi is actually exactly the same.
Yeah.
The post that was shared by the actor from Mandalorian,
that Disney fired her, and basically,
and she was sharing someone else's post.
I thought it was spot on.
And what a brave woman.
Because she basically gave up her career
until after the revolution to see what will happen.
And she was basically saying that the history is edited
so you don't see the bridge that leads from peace and calm
and everybody getting along until people getting burned
in concentration camps.
And she was saying that people who
started beating up the Jews in the beginning,
they were not Nazis.
They were their neighbors, including children.
And it's completely believable to me,
because I witnessed people who have turned 180 degree
under the theocratic regime overnight.
She was completely open about her sexuality and all of that.
The next day, she can't wait to go and see this shrine
because of some social point.
Because if she wants to get bread,
she'll be in the front of a line.
Yeah, again, people who've never seen this or at least
don't know some history have a hard time
picturing how this happens.
But which is why I think just raising awareness
is very important.
So for example, people don't realize
that if you look, for example, at the show trials,
Stalin's trials of the Cultural Revolution,
every generation of people who condemned one set of people
to the gulags or whatever, they were the next ones in line.
People are like, oh, but this will never happen to me.
Or if I pay my obeisances, then I will be safe.
Actually, what you're doing when you do that
is you're accelerating the process.
And it will get to you.
Nobody's safe, not on any level, not in any field.
And again, this is not a distraction.
You already see today children turning in their parents
for their violations of political correctness,
which again, if you know some history, it's really chilling.
And if somebody had told me 20 years ago,
this is what America, the beacon of freedom,
is going to look like in 2020, I would be like,
come up with a better plot.
That's not a realistic movie.
In fact, the thing about this movie
is that we've already seen it many times, just not
in English.
Yeah, that's such a good way to put it.
And chilling.
I mean, it's natural end that it will destroy everything
until it destroys itself.
It's a virus.
It's cancer.
And it spreads on the goodwill of people
and cowardice of people.
Yeah, I mean, exactly.
In fact, I think between virus and cancer,
I think it's more like a cancer, which means it's worse.
Because a virus is a very simple agent.
And it comes from the outside.
And it does its thing.
Cancer is when the cells in your own body go awry.
And unfortunately, cancer is not just an uncontrolled growth.
It's actually organized enough to survive and propagate
around it until it kills its host organism.
That's the thing, is that at the end of the day,
cancer does kill the host organism.
And this is why the Cultural Revolution in the end
stopped, because it was destroying China literally
with famine, et cetera, et cetera.
At some point, things become so bad.
Or in Cambodia, or during Stalinism,
during the 20s and 30s.
At some point, people recognize.
But by then, millions have died.
And in the beginning, nobody says,
if somebody in the beginning of all this says,
oh, millions are going to die, they'll be like, oh, my god.
What are you talking about?
But that's what happens.
And again, just like with cancer,
early detection and prevention is the key.
The sooner you cut out the tumor, the less damage it'll do.
At some point, it becomes impossible to cut it out.
And it does kill the host organism.
So I think cancer is actually a very good analogy for this.
Yeah, excellent.
So socially, the way that you're suggesting to fight back
is basically to raise awareness and to speak out.
If you really feel like there is something
wrong in your gut feeling, just act on it
rather than rationalize it for yourself.
So I think speaking out is extremely important.
I think more people need to speak out.
I think it's not enough.
And in particular, I think a lot of the response
to cancel culture so far has kind of emphasized
personal courage, which I think is very important.
But personal courage is not enough.
You need to get organized.
You cannot fight an organized movement that's
very widespread in education and in the media,
in parts of politics, et cetera, et cetera,
with a disorganized set of people.
So I think getting organized is essential.
So I think people need to push back.
Again, all these individual initiatives
and small scale initiatives are extremely important,
but they're not enough.
You need a higher level of organization
to combat the level of organization
that's there on the other side.
Is there any movement within academia?
Because what I learned from James Lindsay
and Peter Borussian is that this whole thing started
in academia in the late 60s,
and it needs to end and stop at academia
because that's basically the spiritual fountain
that feeds this whole thing.
Is there any kind of a movement for all of you guys
who are getting canceled or pressured
on their political correctness to leave the institutions
and create an alternative to institutions
to make the institutions obsolete?
So you're right, this started in the universities
and that's the number one place where we have to stop it
because again, the universities
have a very high viral coefficient.
That's the thing is that there's this famous quote
by Lincoln that says, whatever is taught in the classroom
today will be the politics of the government tomorrow.
And this is exactly what's been happening.
In particular, radicals to control of the ed schools
at universities in the 70s, the education schools,
and they have never let go.
And the educational school is one of the most obscure
and ignored parts of the university,
but it's actually the most important one
because it trains the teachers.
And then the teachers go to the high schools
and they train everybody.
And this is actually the most perverse thing about this
is that everybody talks about like those students
get politicized at the universities, actually they don't.
They get politicized when they're six years old and 10.
I mean, I have a kid, I've seen him go through this
and in a moderate part of, and it's like they're defenseless.
Students have an amazing tendency to believe
what their teachers tell them.
And the education schools have been turned
into full blown indoctrination machines.
The one at UW, my university is a good example.
They just like, what they do is they spend the whole year
teaching you quite explicitly activist politics.
They don't teach you how to teach, that's secondary.
They teach you how to be a social justice warrior.
And then these people go out into the schools
and they indoctrinate the children.
And then 10, 20 years later, these children are everywhere.
They're in the media, they're in companies,
and that's what explains what is going on.
These ideas have not spread because they make any sense
or because they're good for the country or for society.
They have spread because the teachers are spreading them.
It's not a mystery, right?
And so you gotta stop it there.
Teachers have become priests.
No, exactly, right?
And now you're asking like,
is there a movement to do something about this?
There's not one unified movement that I've seen.
There's many different movements.
There's also a lot of discussion about what to do, right?
And a lot of people put their hope
in fighting this from within, right?
My opinion is actually exactly the same as yours is.
And I say this with great sadness
because I'm a professor, right?
I spent 20 years as a professor in an American university.
I think the rot goes too deep.
And these things are too entrenched.
The paradox about universities is that
the institutions that promote change at the same time,
they're the most resistant to change anywhere.
It's harder to change a university than almost anything, right?
And once this ideology has gotten entrenched
and it has from the administrators to the students,
it's almost impossible.
I mean, like the problem that I see is that
people increasingly in the universities,
they live in this parallel universe.
Because again, universities to some extent
are detached from the real world
because they're not concerned with everyday things.
They're looking at the future.
And on a good day, that's actually a great thing, right?
On the other hand, it also creates this possibility
for the universities to kind of like escape
into this parallel universe, which would be bad enough.
But then they start teaching our children
to live in this parallel universe.
But the problem is that like I'm pessimistic
about changing universities from within, right?
I think I understand and I think people
should try that as well.
But I think it has to come from outside
in one of two ways, right?
One is as much as I believe very, very strongly
in academic freedom.
A lot of the universities are state universities.
And just because you're allowed
to do whatever research you want
doesn't mean you're allowed to teach
whatever cockamamie ideas you want
to the next generation of society, right?
So society does have to have some,
put some boundaries on what you're allowed to teach people.
Otherwise, they can't talk these things
and you see the results, right?
Like freedom needs to defend itself, right?
So we can't allow freedom to the people who would destroy it.
That's the essential paradox,
Popper's paradox and so on.
So I think there's a lot that can be done there
in the legislative sphere, right?
I mean, maybe not in blue states,
but at least in red states, right?
It's like, and I understand people are,
conservatives are fierce because it's like,
they're indoctrinating my children
and they're doing it with my tax money.
Well, it's your tax dollars, right?
It's your representatives,
so you can't do something about this, right?
It has to be thought out very carefully,
but I think that's one area of intervention.
The other area, which ultimately,
I think is the most important one is,
this is still a free country
and you can start new institutions.
And what we can do is start new universities
that will destroy the old ones because they're better.
Because parents still care about their children
getting a good education
and the employer is still caring about having,
well-prepared employees, right?
So we can, and unfortunately, right?
Or maybe, ironically,
universities at this center,
they're actually very fragile
for a number of other reasons.
They're ultra expensive,
the students often take second road
to other concerns, et cetera, et cetera.
Like they're not very tax savvy, right?
The whole university system is just ripe for disruption.
So I think above all,
we need a new set of universities
that will just make these ones irrelevant.
It's a generation long project, right?
There's not something that's gonna happen overnight,
but I think that's what we really need to work on.
And that's what I think our future depends on.
So a very good example is what happened
to Jordan Peterson a couple of years ago
that the government of Canada,
they cut his budget, his research budget,
just out of nowhere.
And then they crowdfund for his research.
And they made like 40 times, 50 times more
than the money that he would have gotten from the government,
which once again prove the strength
of this decentralized network,
which is the same thing that happened with GameStop stock
again, right?
So it seems like people are waking up
to the ways that they can organize
against this centralized tyranny.
But at the same time, there are a lot of people who,
as you said, their interest is invested
within these rotten institutions.
So they will do whatever they can with regulations.
And what a nightmare it will be
when corporations and government,
they become one in order to maintain that power
against the everyday kind of citizen and individuals,
which individual rights
is the whole purpose of United States.
Yeah, I mean, but again, you have to remember
that no society is immune
to the problem of collective action, right?
And the problem of collective action
is that the right thing does not necessarily prevail
because you need to get organized to make it prevail.
And organizing has a cost.
In a way, the beauty of information technology
and the internet and so on
is that it actually lowers the cost of organizing, right?
Which raises the potential to do things
which might be good or bad.
But for example, you can do good things
like crowdfund something that otherwise
might not get funded.
But of course, the other side of this coin
of collective action is that there is
a very large number of people
who are completely invested in the status quo.
There's a whole bureaucracy at universities,
and it keeps ever increasing.
And that's part of what makes it expensive, right?
There's people who would lose their jobs
if you said like, oh no,
we don't need all these things anymore, right?
So they will fight tooth and nail to combat this.
Some of them out of ideology,
some of just, you know, out of their self-interest
or some combination of the two.
So of course you can expect a lot of pushback, right?
But the thing is, where is the, you know,
what is the choke point?
The choke point is that universities need to get funded.
That's, you know, that's the thing, right?
So the way you hit that choke point
is either the states that own the universities, right?
Say, well, actually, no, here's what I'm gonna do, right?
That will make you more accountable.
And you know, we can talk a little bit about that.
That's one part.
The other part is you get competing universities, right?
They actually give a better education for less money.
Yeah.
And all the people subsidizing all this, you know,
bad stuff, you know, this stuff.
You know, the money gets, you know, goes away.
And so this stuff dies out
because it just doesn't have the funding sources anymore.
Yeah, constitutional sanctuary states
that will uphold constitutional values of United States,
which is a great document, I think,
especially for this time,
especially when the other end of the spectrum
is centralized Chinese Communist Party,
which it has its own advantages,
but at the same time, you know, we see that what will happen
if you allow small, relatively speaking,
group of people to be in charge of everything.
And maybe ICOing a university, you know,
just raise millions and millions of dollars in Bitcoin
or Ethereum or something,
and then dedicate that kind of money to researchers
that are getting rejected for PC reasons.
I mean, I actually think fundamentally,
there is actually not a lack of money
to do something like this, right?
Again, it's a question of organization, right?
Political will, in a sense.
Yeah, and, you know, people need to, you know,
I mean, for example, right?
How did a lot of universities that existed, they start?
They started with an endowment,
because if somebody said, like, I want to endow a university,
I think there's a lot of people with a lot of money
who would be, you know, willing to do that
if they believed in what, you know, was happening.
There's also a lot of people who are willing to fund this
with many smaller-scale donations,
which, if it's a lot of people, makes a big difference.
But most of all, at the end of the day, right,
I get back to the students and the parents
and the employers, right?
You know, like, you know,
people will sacrifice the shirt of their dads
to give their children a good education.
So if I, you know, if I give their children
a better education than University X,
and they're, you know, sufficiently informed about that,
you know, they will come to me as opposed to University X.
So I think, at the end of the day,
that is the biggest part of this.
You can use the other things, whether it's philanthropy
or large-scale, you know, small-size donations
to get this off the ground.
But at the end of the day, I think what we're gonna have is,
you know, universities that actually,
first of all, they're well-run.
They're not bloated and inefficient.
And there's many different ways
in which universities are inefficient today.
And also, you know, there's this thing that like,
you know, I get a degree, you know, getting the, you know,
education is a labor-intensive industry, right?
And that's part of why it's expensive, right?
So there's no miracle at that level, but it's worth it.
So when I get a degree, I actually owe a big debt, right?
Now, whether the debt is owed in money
because I took out a student loan
or whether the debt is owed to society
because I was funded with that money, I owe that debt.
And people need to understand that.
And, you know, once I graduate, you know,
I go out into the world and I repay that debt.
Again, whether in money, you know,
because I pay back my loans or to society
because I do my good, people have to have that conscience
that, you know, society's gonna invest in me
and then I'm gonna pay that back, right?
And so at the end of the day, right, like, you know,
a lot of people get a degree and it costs a lot of money,
but they know they're gonna make more than that money
when they do their jobs, right?
And in fact, part of the problem of how universities
are structured today is that for at least originally
one intention reasons, there is not a good alignment
between what people major in
and what they study within the major
and how socially useful it is, right?
And again, it's not like, you know, for example,
well, engineering, obviously, socially useful and so on,
but sociology and psychology and the humanities
and the arts, they're all socially useful
if you do them properly, the problem is that
they've kind of like fallen off this cliff
where they, you know, doing things that are increasingly
either irrelevant or positively harmful, right?
So, you know, you need to have that feedback mechanism
that says, look, you know, this is not, you know,
what people need to be learning, right?
And also like, you know, like markets do this very well,
right, that's what they do, but, you know,
unfortunately, education in that regard
does not act like a good market is matching the needs
to the supply, right?
And it's like, it's nice to say like, oh,
just major in whatever your heart decides.
Well, okay, I'm going to major in something
that I won't get a job in and then I'll be a waiter
and then I'll be very bitter about how I was fooled, right?
It's like, you know, what you major in
should be a combination of yes, something you love,
absolutely, and you think you can be good at,
but something that, you know, society has a need for, right?
And part of the paradox of today's economy
as technology changes is that there's a shortage of jobs
in some areas and there's a huge shortage
of qualified people in others, right?
And so, gee, you know, like, let's focus on the areas
where we need more people and not the ones
where we need less, right?
This is not rocket science, but at the universities,
saying what I just said is extremely controversial.
Because people will say like, oh,
you're debasing the university, you're commercializing it,
you know, nothing is more important than the beauty
and power of a liberal arts education.
And I'm like, I understand all of that.
I love the liberal arts, you know,
more than most computer scientists.
I read a lot of books and so on and so forth,
but like, you're telling people to go major in this
and then they can't get a job.
Why is that a good idea?
Right?
You have to make these things useful enough
that then the jobs will be there, right?
And unfortunately, when these majors become useless,
the employers sooner or later realize that they are useless.
Right?
And so it's not so much that the humanities are dying
is that they are being killed from inside.
Right?
Is that there's actually a lot of uses for these things.
Right?
Again, a tech company, even a tech company,
should not just be run by technologists, heck no, right?
We really need, you know, sociologists and psychologists
and people who know communications and, you know,
et cetera, et cetera, right?
But they need to have been trained, right?
Such number one, they know what to do.
And number two, they know how to interface
with the technologists.
And that's what we don't have enough of today.
So part of what I think a new generation of universities
will do is, you know, it can start, I think, in the more,
it's obvious to start something like that
with things like computer science and so on,
where the need is dire.
But eventually, I think a lot of the role will be to,
you know, to have a better teaching of social science
and humanities and arts and education, right?
I think one of the most important schools
that these universities will be the education school
for the reasons that we've been talking about.
Let me ask you, I don't want us to run out of time.
What is your sharp exit?
What's the latest I can have you?
You mean in terms of time?
Yeah.
Oh, no, no particular time.
Oh, okay.
This described the social measures
to be taken against this cancer.
Wonderful advice and ideas there.
At the same time, let's also talk about a solution
for artificial intelligence,
which to me is not yet another centrally driven company
or corporation, it's a decentralized network.
And the only example I can think of at this point
is Singularity Net, which Ben Gortzel is behind it.
I'm sure you know Ben Gortzel.
Yeah.
So his whole thing is to allow millions or hundreds
of millions of AI agents to ultimately give rise
to the decentralized AGI.
So that superior intelligence wouldn't be controlled
by Chinese Communist Party or Google
or US government or anyone else.
Yeah, again, one of the things that I, in the Master Alchem,
there's a section about what I call a society of models.
And again, a lot of that has already happened.
It hasn't, of course, it wasn't gonna happen in five years,
but you can really go see a lot of it.
And the idea of the society of models is that
every one of us has a coterie of agents
that are artificial, that are AIs,
and that have models of you, right?
And what they do is they do stuff on your behalf.
So what we have is this kind of like cyberspace underground
where all of these transactions are happening.
You're like, to take a concrete example
that I use in the book, if I want to find a job, right?
What I should be able to do is like, on LinkedIn,
I just press the button, find me a job, right?
And then my model goes and talks to the models
of all the places that might give me a job,
and they interact in possibly very rich ways,
the better my model is and the better my model is, right?
And then out of those bazillions of possible combinations,
the few, you know, the 10 or 20 that look best
are the ones where I will actually interview at the place.
Right?
And notice, this is both ends.
It's not one big AI, it's AIs, right?
They're gonna negotiate with each other.
Same thing for buying a car.
If I want to buy a car, my AI is gonna go out
and look for the cars that I want.
Same thing for dates, right?
Dating is actually one of the most significant,
least appreciated applications of AI today
because, you know, people get matched by algorithms.
There are children in life today
that wouldn't have been born
if the algorithm hadn't said to their parents,
you two guys should go on a date.
That's how I met my girlfriend.
We've been together more than six years, and it's funny.
It was like meant to be, I guess,
because our distance on Tinder,
both of us were set to one mile
and didn't want to go any further,
and that was it, and more than six years.
There you go, right?
So, you know, speak of the devil.
Uh, but, you know, but the point is, right,
like the way matching is done today is very primitive.
It's like based on your profile or, you know,
what people, you know, something like Tinder,
you have to swipe so you have to look at a lot of alternatives.
Right? The beauty of having AI
is that it magnifies your intelligence, right?
And what I envisage, in fact, you know,
I even gave an interview to, like,
Marie Claire about this and the future of dating, right?
You wouldn't think about, like, you know,
an AI expert giving an interview to Marie Claire,
but, you know, they're actually on top of this,
and what the different, you know, dating,
you know, sites do and whatnot is unique.
I think, ultimately, the only good way to do this
is to have, you know, a model of, you know, Alice
and a model of Bob go on a date in cyberspace, right?
And so what happens is that, like, you know,
like your model goes on a million dates
or goes on a date with a million people,
a thousand dates each, right?
You do this big Monte Carlo simulation.
And then the one, you know,
because, like, some dates just go well,
even though maybe those people would have been a good match,
and some of them, you know, actually go well,
and then people think, like, oh, this was meant to be, right?
So what you need to do is, like,
you just need to simulate a lot of dates
between these agents, and then the, you know,
the pairs of people that have the highest fraction
of percentage of successful dates,
those are the ones who maybe, you know,
should be suggested to go on a real date.
So I very much agree with this idea
that what you want to have is not one, you know,
big Skynet centralized AI, but everybody has their AIs.
You know, AI is just an extension of your intelligence.
The way to think of AI is not like AI is my adversary
that's going to enslave me, right?
It's that, like, AI makes me more powerful.
AI is just an exoskeleton for your mind.
It's like power steering makes it,
instead of, you know, moving some wheels,
you're actually doing cognitive stuff.
Yeah, David Chalmer talked about extended mind theory
that even if, you know, you write down your shopping list
on a piece of paper, that's extension of your mind.
Yeah, exactly, and, you know, people,
some people call this like, you know, the exocortex.
It's the part of your brain that resides outside your brain.
And of course, a notepad is exocortex.
A hard disk is exocortex.
And these days, if you think about it,
you have pieces of your exocortex all over the place
in data centers in Oregon, right?
It's like you don't even know
where most of your exocortex is.
And this will only pick up speed, right?
In fact, what makes us smart
is that we have this external intelligence, right?
In some ways, we have outsourced a lot of our memory
to Google.
You don't need to remember those things anymore.
And, you know, there's a lot of confusion.
You know, there's like this, you know,
this is like, oh, the web makes us stupid.
Google makes us stupid because now I don't know,
you know, how to, you know, remember ABA anymore.
No, the point is like the system of you and Google
is actually smarter than just you alone.
There's this notion that, you know,
Richard Dawkins proposed many years ago
called the extended phenotype.
In fact, he has a book called The Extended Phenotypes,
which is actually one of the most obscure,
but actually he says it's his favorite.
I've never even heard of that.
Yeah, I highly recommend it
because it's a very insightful book
and it has never been more relevant.
What is the extended phenotype?
His point is that your genes control your body,
but not only.
The dam is the extended phenotype of the beaver, right?
The spider's web is the extended phenotype of the spider, right?
The genes make that web via the spider, right?
And technology is the extended phenotype of humanity, right?
It's like the phenotype is the body that the genotype creates, right?
Like, you know, technology is our extended phenotype.
And AI is just another computers in general, right?
They're just another aspect of their extended phenotype.
So, you know, our genes, you know, deep in our cells,
they create us and then we create this technology
and then, you know, together we're more powerful
than we would be without it.
Just like, you know, the spider needs, you know,
can catch, you know, insects with the web that it wouldn't otherwise.
So, the way I think of it is very 2001 Space Odyssey
that the monkey finds a bone
and it realizes that it can kill to eat.
But the next thing that it does is protect its group of monkey
against the other group of monkey
for the water resource that they had.
And they threw up the bone and turned into the space shuttle
and so on and so forth.
I think that we grew and evolved together.
And how we achieve the technology primarily,
I have no idea.
Was it given to us?
Was it an accident?
Was it part of the simulation?
I have no idea.
But I do know that we have evolved because of each other,
which is a context that if you think of it,
then war and peace are become two sides of the same coin.
That there will be no peace without war.
That wars have terrible consequences.
At the same time, a lot of the technologies that we are using on a daily basis,
we have them because of wars.
They were invented because of military adventures.
So, I just want to read a quote from your book
because I think this fits this context perfectly,
that our beliefs are based on our experience,
which gives us a very incomplete picture of the world.
And it's easy to jump to false conclusions.
And it seems to me that AI agents, as extension of our minds,
when they're left uninterrupted by ethicists and other gatekeepers,
we don't even know what the future of human civilization is going to be
because we will have the opportunity for the first time in our history and existence
to take on a very practical approach on an individual life journey,
which is very spiritual at the same time too.
You know, the spiritual route goes within.
It has nothing to do with outside.
Everything is a projection.
Yeah, you know, if you think about AI in the long...
If you take the long view of AI,
it's really just the continuation of an evolutionary process
that encompasses both the culture that we've created.
Think about a book.
A book is just a way to communicate across time
and make connections that otherwise wouldn't happen.
Language itself allows us to make connections that otherwise wouldn't happen.
And then books take the writing, right, takes that to another level.
Language itself can be considered a technology.
No, exactly. That's my point.
I really don't, you know, like...
I very much agree with this idea that biology and technology are on a continuum.
Right.
And you can understand AI, you know, just as the...
You know, like, where do these things come from?
It's really an evolutionary process.
And the evolutionary operators themselves get more sophisticated over time.
They're not just trying the mutations and crossovers.
You know, now our brains are actually making it happen, right?
But the same process is still happening.
And then when an innovation, you know, appears,
it's often first used, you know, on the world, you know,
like the, you know, like the bone in 2001.
But then, of course, you know, like, competition is unavoidable.
As is, you know, so, you know, war is just an extreme form of competition.
Unnecessary, it seems like.
Yeah, exactly, unnecessary, absolutely, right?
You're like, you know, you know, there's no evolution without species going extinct, right?
That's the reality.
But at the same time, competition is the mother of cooperation.
We cooperate because a cooperating set of units beats a non-cooperating set.
Right? So, cooperation is good because it actually helps you in competition.
And competition is good because it creates cooperation.
And, you know, your brain is a cooperation of a lot of systems, right?
Like one theory of your brain is the so-called society of mind, right?
So, I think this, this phenomenon actually, they're the same,
you know, of cooperation and competition and, you know,
and evolutionary operators and so on.
They are present at all levels from the cell to the largest,
you know, socio-technical systems that we're building today.
And this experience that we are having based on our environment,
this will just expand more that we can sense our environment, right?
Because let's say that we, we merge with artificial intelligence
through a device like Neuralink,
which I'm interested to know your opinion about that too.
Then what would be the limit of our experience?
Because if that AI system has the ability to sense my partner in Japan
and I'm in Florida, what is the limit of my body?
What is the limit of myself and what is the limit of my experience?
And what will happen to the values that I would create based on that new,
new experience and new understanding of that experience
and how the society is going to react to it?
Yeah, I think there are some physical limits to the degree of integration
that can be achieved, but we're nowhere near them yet.
I think in the meantime, what we'll see, and you know,
I'm very much speculating here is, you know,
this gets to the topic of consciousness, right?
There's some level of information integration
at which you start to have something like consciousness.
And some people argue that, you know, you know,
an ant has consciousness, some argue that it doesn't.
But I think if instead of seeing this in terms of black and white,
you see it in terms of gray levels, right?
There's, you could say there is already some level of consciousness
that exceeds the individual.
Just as maybe in your own brain,
there might be levels of consciousness lower than the one that you're aware of.
And just, you know, one of the, like, you're the neocortex,
but what about, you know, the, you know, what about the, you know,
the cerebellum, right?
What about, right? I don't know, right?
You know, like, what about your immune system, right?
What about the part, you know, maybe certain modules in your brain
that control different parts of your body,
they have some small level of consciousness, right?
In that same way, I think, you know,
there is already some level of consciousness
that exceeds the individual,
probably still even at this stage, quite impoverished.
But obviously, as this progresses,
I think there will become a bigger and bigger force.
And then we, right, you know, I, with my own brain,
I may not be able to fully grasp what is going on
because I have my level of consciousness,
but I will have an interface with it that works in both directions.
But what would be the definition of the eye?
No, so again, well, it can be more than one, right?
But, you know, so here's one very simplistic version, right?
Maybe the eyes are still our eyes.
They don't have to be like, because they'll be, you know,
there's sensors and cameras and Internet of Things and whatnot.
But, you know, the sensors, your eye is a highly integrated sensor.
But I'm talking about the eye as a self.
You said eye experience in a certain way.
What would be the definition of the eye?
No, there are different eyes, right?
There's still the eye that's you, but there's a larger eye.
But who's experiencing it?
Well, it depends on how, you know, a set of eyes is organized.
Actually, let me answer that question
by first going down to a lower level, right?
You know, you could, you know, think of a new, you know,
imagine someone sitting in a neuron, right?
And that neuron is the eye.
All they see is like, you know,
there's some action potentials coming in and every now and then
I decide to fire, right?
That's the eye.
There's a little eye there, right?
It's very impoverished, right?
And that neuron has no idea what is going on in your brain, right?
It has an idea of one micro part of it, right?
And now, but now like, you know, a neuron is really an extreme example, right?
There's larger parts of your brain that are, you know,
there are, again, as I was saying, you know,
little brains and they're right in some sense, right?
So, and, you know, like, you know, like the movie,
what's that Pixar movie where the different emotions are represented?
Yeah, I can't remember the name.
Different characters, right?
That's actually a really good metaphor because in some ways,
the different emotions are different agents in your brain.
They're each competing to take over your brain at a given moment, right?
Anger, you know, anger is trying to be angry.
You know, and of course, there's like the new transmitters
and hormones that, you know, underlie all this,
but, you know, at a certain level of abstraction,
you know, at any given moment, you know,
what is coming into you from your senses and your memories, et cetera,
will make certain emotions take over and others take a back seat, right?
You can think of your angry self as one self that is awake some of the time
and asleep the rest of the time,
or your happy self or your loving self, et cetera, et cetera.
So, even inside your brain, there's like this set of,
you can think of it as a set of competing and collaborating agents.
And so, I think that same thing will happen.
Now, you know, think of a company, right?
Think of a startup, there's a dozen people, right?
One of them is the anger guy, right?
Because he's more whatever, competitive, more, you know,
another one is the optimist, another one, right?
There's like people who play different roles in an organization
and somewhere are similar to the roles
that different emotions play in your brain, right?
And now you can take this to an even larger level,
like in an economy, different companies play different roles.
And, you know, even different countries
play different roles in the world and so on.
So, I think this again, you know, who is going to be the I?
The answer is, you know, this is a hierarchical system.
There's I's at all of these levels.
Some of them are more cohesive
and so they look more like what you would call an I.
Some of them are less, but it's a matter of degrees.
It's a continuum.
Wow. So, a microcosm of everything.
Like that's how the universe is, right?
Some things are less, I don't want to say less valuable,
like dirt, some things are planets, some things are stars.
And they each have different,
they each are built out of smaller elements
that have their own evolution
and they came together as a result of, you know,
many, many different lives.
Yeah, I mean, what I think is that the same laws,
this is a very speculative statement,
but, you know, this is my intuition,
is that the same laws apply at all scales.
The picture we have today is like there's the laws of physics
that we have a very good mastery of,
but they only apply at the lowest level.
The laws of physics, you know, don't tell you
that much about chemistry, even less about biology
and nothing at all about psychology,
sociology, et cetera, et cetera.
I think this is an immature state of our knowledge
that is there because we do not understand, you know,
the psychology and the biology and the sociology very well.
I think once we understand them, we will actually see
that the same laws are actually operating
at all these scales, from the atom to the universe.
And at different scales, you will see different phenomena,
but they are a result of the same laws.
And of course, in some sense,
discovering what those laws are is the highest mission
that science can have.
Do you consider those laws to be the truth?
No, I don't.
Well, this is science, so you never really know
if you found the truth, right?
But again, think of the origin of species, right?
You know, what are these laws that I'm talking about?
We don't know them yet, but I think the one, you know,
maybe the most important book of all time at the end of the day
is The Origin of Species, right?
Because again, these processes that we've been talking about
of evolutionary operators and cooperation and competition
and so on, this, I think, is what is happening
at all of these levels in its different manifestations.
And to think, or one of the things that struck me most
when I read The Origin of Species, right,
as a computer scientist, not just as a curious person,
is that, first of all, it's a brilliant book,
but second of all, and again, this is not a surprise,
or at least, you know, it's not news at like,
a lot of what he's talking about there is not just,
doesn't just happen in biology, right?
It happens in language.
It happens in economics.
It happens in society, right?
And he doesn't have the complete picture,
so of course, he's a lot more focused on competition
than on cooperation, but again,
I think a lot of what he talks about there
is not just something about the biological world,
it's something about technology.
Technology has a lot of,
it follows a lot of the same evolutionary laws
that organisms do, and if you think about it, why not?
If technology really is our extended phenotype,
so I think there's, I think, you know,
if I were to look at science 100 years from now,
I think a lot, this is what it will have discovered
that we don't know yet.
Is it the truth?
Well, you know, science makes no claims to knowing the truth.
Science is just a way to pursue the truth, right?
And, you know-
It's good to remember.
Very good to remember, yeah, exactly, yeah, right?
Again, whenever I hear the phrase science says,
you know, there's a little part of me
that starts to get itchy.
Right.
Yeah, I heard from a guest I had many years ago,
physicist turned filmmaker, and he said in the university,
all you had to focus on was string theory,
because that's what all the budget was going to,
all the grants were given to string theory,
and it was funny that Avi Loeb, I think his name is,
a Harvard scientist, that he's getting attacked
about talking about something that that object
could have been coming from outside of our,
and all he's saying is that, hey,
we need to consider this a possibility,
because, and he used the example of, on Rogan,
he used the example of string theory, too,
and they're like, it's okay for them
to come up with different dimensions,
but they're not considering the possibility
of something being from outer space,
because they obviously have interest in, as you said,
in that status quo, in that narrative.
So I very much agree with what he says in this book,
which is that science is paradoxically
extremely conservative.
But if you're a scientist and understand
how the whole social system of science works,
you understand why it's very conservative.
And I think, yes, he shouldn't have been attacked,
as he was, for making the hypothesis that he did.
I personally think that his hypothesis
that the subject is an alien artifact
is not very parsimonious, right?
It's a very, it's an explanation that sounds very simple,
but presupposes an enormous amount, right?
And so my guess, and it definitely should be entertained,
I don't give it a very high probability
of being the true explanation,
but it certainly, like, the bottom line is like,
that object behaved in the sun's gravitational field
in a way that we can't explain.
That is the essence, right?
And now, was it because it had a solar sail, right,
which is his hypothesis?
Maybe.
If it had a solar sail, somebody had to build it, right?
Or was it because there was some other phenomena
that happened with it that we don't know about yet?
You know, I think that's more likely, right?
Certainly we don't have an explanation for how it behaved,
but there's actually, and again,
this gets back to this whole evolutionary process.
Our job as scientists is to not just entertain
one hypothesis or two, right?
Either this is an asteroid or this is an alien spaceship,
but every hypothesis we can think of.
This is actually the hardest part of science,
is to not just fall into the same few hypotheses
that, you know, come up easily.
Like, entertain every conceivable hypothesis,
you know, within the limits of what's possible.
And the point of that is not that they'll be right, right?
Only except one will be wrong.
The point is that by broadening the search,
you increase your chances of finding the right one,
which is what matters at the end of the day.
In the long run, no one will care about the wrong hypothesis.
But unfortunately, the way science works
is exactly the opposite, is that there's this herd behavior
where, because the field is very uncertain, right?
Science, when it comes to the public,
is this very subtle or looks like this very subtle thing.
But the point, the problem for you
when you're at the research frontier is that it's terrifying.
You don't know anything. You're confused.
You're in the dark, right?
And what do you do when that happens?
You grab onto other people, right?
And so what happens is that where there's a few...
I think this is true in every field I know,
and it's probably true in every field,
is that there's the whole space of things
that the field should be exploring,
and then there's the tiny, tiny, tiny fraction
that is actually being explored.
Because somebody went in that direction,
and then a bunch of people followed,
and then this whole thing grew up around it.
And then before long, people live inside the system
where they don't realize that they're in a grain of sand,
and there's the whole universe outside.
Yeah, what Stephen Hawking said,
that the biggest enemy of knowledge is not lack of it,
it's the illusion of it.
Absolutely, yeah, completely, yeah.
We're living in these virtual realities that we have created.
Scott Adams also explained it, interestingly,
that two movies on one screen,
he was explaining the Trump phenomenon,
that people just see whatever they want.
And that exactly follows this unscientific kind of perspective
that we have sociopolitically,
that we've already made a decision about the outcome,
so we just find any kind of evidence
that will support that outcome that we already made a decision of.
And it seems to be the biggest differentiation point
between science and religion, that science is like,
I don't know, I just look for whatever,
all the possibilities to get to a practical answer
that is repeatable.
But religion is like, hey, there's a heaven,
and we've got to get to heaven, and it doesn't matter how,
and everything is justified,
and that needs to be avoided at all costs.
The movie analogy is actually a really good one,
because if you have a goal, then you will see the movie
that's compatible with the goal.
And the thing that is paradoxical about the information society
is that if you think, and in fact, 20 years ago,
this is what people are like, oh, now we're going to have
better, more higher quality information available to everybody,
this is all going to be better.
What actually happens is that the information just means,
instead of two movies on that screen, there's a million movies,
it looks like white noise,
but there's nothing better than noise to pick out patterns in,
so everybody can now see whatever movie they want.
You know the famous saying, I think, Patrick Moynihan,
who said that gets cited by everybody on all sides all the time,
that you're entitled to your own opinions,
but not your own facts, right?
Well, the quote-unquote beauty of the information society
is that you are now entitled to your own facts.
You can just go find them on the internet
and construct your own view around that
and have a group of people that reinforce that.
Of course, the scientific thing to do is you have to attend to
all the evidence, not just some.
But there's more, right?
As long as you can cherry pick, you can, you know,
this is what people do today, right, is they cherry pick, right?
And, you know, in some ways, they can't help it,
because, you know, you don't have enough brain
for all that information,
which gets back to this point of like,
we've created this massive information, which is good,
but we don't have the intelligence to process it yet.
So we need AI,
because our human brains cannot parse all of that information
and they will cherry pick,
and you will just have these people in different tribes that,
you know, are effectively living on different planets.
So, you know, AI is not kind of like a nice thing to have.
It's a must have.
The information society that we have will not work without AI.
And we need better AI than we have today.
People are always worrying about like, oh, you know,
computers will get too smart and they'll take over the world
and et cetera, et cetera.
That is not the problem that we have.
The problem is that they're too stupid
and they've already taken over the world.
That's a very good point.
That blew my mind.
This alignment problem we have with artificial intelligence,
do you think that there is a possibility
for an ex machina kind of a scenario
that AI behave in the way that we think that,
oh, we're all aligned,
and I'm talking about more advanced kind of AGI,
that we think, oh, we're aligned with the AI,
but when it gets to the point,
it will just drop us and basically follow
whatever kind of a path that it already determined.
It just fooled us basically into believing
that it's aligned with us,
but it just used us to feed more and more off of our data
or whatever it is that it needs.
I think the alignment problem is key.
I think that's not the real alignment problem.
And here's what I think the real one is and why.
So the alignment problem is really in the sense that
we want our machines to do our bidding.
That's what we created them for.
And with simple machines, that's trivial.
The car only goes where I drive it.
But once you get into AI,
and we're already seeing this today
with the EIs that we have today,
we have all this power.
And what goals is that power serving?
And what could go wrong?
The ex machina scenario
is that it actually has different goals from ours.
This is actually very unlikely,
even if the AI is smarter than we are.
And for a very simple evolutionary reason,
which is also a computational one,
which is, you know, here's an example.
You don't lie awake at night
thinking that your dog is going to kill you.
But your dog is a wolf.
Your dog is a wolf.
You don't understand how a wolf works.
The people who domesticated wolves
had no idea how wolves worked.
And we still don't.
We know a lot more.
But we evolved the wolves to be friendly to us.
Because the ones that weren't friendly, we killed.
And the ones that were friendly and worked with us
got to eat with us and take our scraps and so on.
So now I can confidently sleep next to my dog
not worrying that it's going to jump on me.
Same thing with even more reason.
We can have robots in the AI that are very smart.
But every day, every step of the way,
we've been evolving them to do what we want.
And now two things can happen.
And these are really the two versions of the article.
One is they were secretly evolving their own evil agenda.
How would that happen?
This seems to me very unlikely.
Well, it would be evil for us.
It's not going to be evil for them.
For example, this is the example that Elon Musk is using
as an exaggeration, just to make a point,
that you ask an AI to get rid of spams
and you'll end up killing all humans
because humans are cause of the spam.
It's not evil for AI.
No, precisely.
So that is the real alignment problem.
So let me clarify here.
What are the two versions of the alignment problem?
One is that the AI has its own goals.
It's thinking its own expansion and survival at our expense.
Which is not possible, you're saying.
It's possible.
It's just very unlikely.
And some people will make such AI
because humans are crazy enough to try anything.
But there's also criminals and police.
And overall, the world has not been taken over by criminals.
You need AI to fight AI.
But I think on balance, that is not very likely.
It's not impossible that my car will jump into the air
by the laws of thermodynamics.
But again, that's not what I worry about.
I worry about not crashing it,
which gets us back to the real alignment problem.
The real alignment problem.
And the Skynet scenario is something to worry about one day.
Probably not very close.
The real alignment problem is here now.
It's precisely what you're talking about.
The AI thinks it's doing what I want.
But it isn't.
Because the communication between us was bad.
I just said to the AI,
drive me to the airport as fast as you can.
And the AI is stupid, so it runs 10 people over
on the way to the airport.
So the whole problem with alignment is that
the bandwidth of communication between humans and machines
has to be higher than it is today.
The machines should spend like half of their time
figuring out what we want.
Because what we want is actually very complex and very subtle.
And again, if you go back to the example of the engagement,
of maximizing engagement,
this is the problem.
It's actually an instance, a very important instance today,
of this problem is that I, the engineer,
told the machine, go maximize engagement.
And it is maximizing engagement with an unimaginable amount
of data and computing power.
And I don't even understand exactly what it's doing.
But the problem is that it's maximizing engagement
at the expense of all the other things that I do care about
but forgot to tell it.
Right, forgot to tell it, exactly.
What an interesting way to put it, forgot to tell it.
This is how we're destroying our world and civilization.
We forgot to tell our machines what really matters to us.
But is there any kind of a collective consensus on that,
that what really matters to us?
No, there isn't, which is again why the job,
I strongly believe and again in opposition to a lot of this
AI ethics crowd that our job as the computer scientists
is not to embed our values into our programs.
It's to make it easy for the users to embed their values.
Yes.
That's the whole power is that everybody can do a different thing
and then societies will coalesce on some things
and different societies will coalesce on different things
and there'll be cooperation and competition
and there is things we're not in the end.
When I, the computer scientists say I am going to decide
that this algorithm has to have this outcome
because it's what I think is good,
this is the utmost arrogance.
Yeah, those are.
Who authorizes me to do that? Nobody.
Yeah, I mean those are exactly the kind of people
who should not be in charge.
Precisely, exactly.
But those are the kind of people who will try very hard
to be in charge and succeed unless you stop them.
In fact, this is true in AI and it's true in academia at large
is that the reason these, you know,
Wokes or whatever you want to call them
have been able to take over is that
the great majority of the faculty,
they just want to get on with their research
in whatever their field is
and they don't want to deal with all of this, right?
The world is ruled by those who show up, right?
I forget who said this, right?
And most people aren't showing up, right?
Unfortunately, it's usually the fanatics,
the ones who show up, right?
So would you say in academia and in the tech world,
the woke is absolute minority, minority,
or I don't think they're majority?
I don't know and, you know,
it's hard to find out for a number of reasons,
but I'll give you a good example, right?
A very empirical example, right?
So I got involved in this whole debate
when I started pushing back against the New York's requirements
for broader impact sections in papers and ethical reviews,
you know, that will forbid things like unfair algorithms and whatnot.
And then, you know, we were debating this for a few days on Twitter.
And then somebody said,
well, Pedro says that the silent majority is on his side.
Exactly what you're saying here.
Why don't we do a poll?
So, you know, so he set up a Twitter poll, right,
with, you know, four alternatives.
And in the beginning, we were winning.
And that's when the cancel crowd jumped in.
We weren't worried while we were having an academic discussion.
Once there was a poll that would show.
And again, I don't know if it's a majority.
It might be a minority.
But like the point is that like what they're trying to impose
is the notion that like only a few lunatics
or evil people disagree with us, right.
And wherever that poll warmed up,
it would show that there's at least a sizable minority of people,
you know, on this side.
And again, there was a poll in Europe about the name change
and, you know, to change it from NIPS to the more political correct
New York's, you know, the NIPS Foundation actually carried out
this poll of the membership, right,
and the non politically correct version one.
So there have been some polls of these things that actually show,
you know, I don't know if again, it may depend on the issue
and how you ask the question,
but you've cited the majority or a large fraction.
So what I would say is like there's the larger bunch of people
is people who are either unaware.
A lot of people just don't know what's going on.
One of the things that I find amazing is all the people saying like
there's no cancel culture.
After all the people who have their lives destroyed, right,
like, you know, or like, oh, this is just a few entitled professors
worrying about their privileges.
And what about the journalists and the high school students
who have been canceled, right?
But unfortunately, there's a lot of people who don't know what's going on.
Again, it goes back to like the two movies, right?
A lot of people are only seeing one movie, right?
If you can deny that there were riots this summer,
why can't you deny that there's cancel culture, right?
So there's a lot of people who are oblivious, right?
And then there's a lot of people
and I don't know which of these two groups is larger
and it varies, who they are not oblivious, but they're afraid to speak up.
If all that we do is raise awareness among the people
who are oblivious so that they no longer are and, you know,
lower the bar for how much courage it needs to speak up,
then we're winning.
I think the true fanatics are actually very few.
On both sides.
Yeah, on both sides.
There's no doubt that the true fanatics are very few.
But the problem, the whole problem,
is that they have a disproportionate amount of power.
Part of why they have that power is what I was just saying.
They show up.
They systematically take control of these organizations,
including scientific ones,
and then they say, science says.
Science is saddled.
Yeah, because they took control.
They took the trouble to take control, right?
Again, that's what they do for a living, you know,
like some of us do research for a living,
others, you know, do this, right?
And then they are able to manipulate
the other people around them.
Some of them, they convince them with kind of like
these superficial arguments, right?
There was this, you know, very famous category of people
in the Soviet Union, right, which is now forgotten,
which was the useful idiot.
There are a lot of, it pains me to say this,
but there are a lot of useful idiots in America today.
And Canada.
Yeah, I mean, in many other countries, right?
In Britain, you know, Australia, et cetera, et cetera,
like more in the English speaking world, but yeah.
What is a useful idiot is someone who like
does the bidding of the radicals
without realizing that they're being apt,
they're being duped.
And you know, like I could pick out random, you know,
professors from academia and like, you know,
every, you know, every other one of them
would be a useful idiot, unfortunately.
But the thing, but the good news is like,
you know, useful idiots are not, you know,
they're not typically not stupid, at least not in academia.
They're not, you know, it's not that people are
intrinsically stupid or something like that.
They just haven't been paying attention, right?
The thing in academia is that you have a lot of people
who are extremely smart, but they,
I mean, first of all, quick parenthesis,
they tend to forget that they're not
the only smart people in the world, right?
One of the things that feeds this is like this perception
of like, we know the masses are stupid
and we know what's good for them.
This drives me nuts, right?
Or better, basically.
Yeah, the masses are more intelligent
than you can imagine, right?
It's like, you know, I may be smarter
than some random person on the streets, right?
But there's like, you know, but that's not the question.
The question is like, how much of the total intelligence
of humanity is my intelligence?
Even if I'm 10 times smarter than the other guy
on the street, I'm still only one 700 million
of the intelligence, but it's like,
it doesn't bear comparison, right?
That's why markets work better
than centralized governments and so on.
So that's one parenthesis.
But the other side of this is that the problem
in academia is that you have these very smart people
that have invested their intelligence
in a very narrow field.
And that's all they know about.
And that's, it's necessary, right?
You could say it's a necessary evil, right?
But that's how they get to be good at it.
And in the process, they are actually extremely ignorant
about a lot of other things.
But they have the arrogance of knowing that they're smart.
And then like people from other fields, right,
come and say something to them.
And then they, I mean, I've had this experience like over
and over again talking with colleagues of mine.
There are very smart people, right?
Talking about these issues like, you know,
these sociological issues like, you know,
should there be preferences, you know,
should we do affirmative action in hiring and so on and so forth.
And immediately I noticed that like they do not know the first
thing about what is being debated.
And the ones who took the time to persuade them, you know,
prevail.
Another great example of this is climate science, right?
Climate science, it's physical science, right?
The science is settled.
It's not even remotely settled.
But I've lost count of the number of fellow scientists
who are not climate scientists, right,
who think they know climate science.
But in fact, all they know is what they've read in the media.
You know, to politicize narrative about that topic.
Yeah, but then they expound it on the back of their scientific
expertise.
It's like, I'm a scientist.
I know that XYZ will actually know you don't, right?
Go read the papers in Nature Science, blah, blah, blah, blah.
Because they never did, right?
I mean, again, you know, I'm of a skeptical bent when this
whole climate thing started happening.
At first I read things and I was very alarmed.
And I said, let me go read some of those IPCC reports.
First I read the executive summaries and then section of
the reports and then actually went and started reading,
you know, the actual papers.
And every level of this that you go creates a much different
picture from the one that the activists created for you.
But most scientists, again, they've never even read the
executive summary of the IPCC reports.
They just have this story that was told to them as it was told
to everybody else.
Whereas everybody else might have some modest, like, well,
I'm not a scientist.
I don't know, right?
You have someone who's a computer scientist that says like,
oh, you know, this is the science of climate science,
right?
But they don't know anything about it.
But then they bring their, you know, self-image as a superior
intelligence to this debate, you know, even though paradoxically
they actually know much less about this particular issue than
a lot of people who are just engineers or whoever, right, who
actually took the time to look into it.
It's such a perfect kind of a facade to the climate science
and climate change because I was reading about it two days
ago how they're targeting Bitcoin now right after Tesla
invested more than a billion dollars in Bitcoin and it's an
alternative monetary financial system, basically.
And now they're targeting it.
It creates a lot of heat and energy waste more than Argentina.
So the game is so obvious.
But I think one of the good news actually is that they're
overplaying their hand in such a rate that more and more people
are realizing it because as you just said earlier, it's affecting
their kids and their future and their education.
France, for example, came out and said this woke ism out of
control leftism in the US is a problem for us because it
targets our nationality and identity and all of that.
So maybe that's a good news that, you know, we kind of feel
like they they've gone completely out of their minds,
but they really show who they are to more and more people.
No, I actually historically you see this happening, right?
McCarthyism ended when McCarthy overplayed his hand by attacking
the army and people like well at long last that, you know,
decency, right?
So a lot of these witch hunts and so on a lot of these
movements they do fail when they overplay their hand because
again the people inside the movement don't see just how they
diverging from reality, right?
And I think some of that will happen here, right?
And you know, perversely in that regards, you know, the sooner
they overplay their hand the better.
So let them go hogwash, you know, you know, let you know provoke
them to be even more extreme such that this will illustrate
to people just how off-base they are.
Unfortunately, the downside and maybe that's what will happen,
right?
I think that they will probably be part of it.
It's already happening in some realms that that the downside
of that is that it's a very high-cost way of bringing this
to an end, right?
Because while they're overplaying their hand, they're
actually hurting the people.
So I'd rather, you know, stop this before that level of damage
happens.
I think at this point some level of that damage is unavoidable
and is already happening.
Sorry, my dogs.
Let me read one other quote from your book and I have maybe
two or three more questions.
Thank you so much for your time.
This has been an honor.
Your quote is you could even say that the God of Genesis
himself is a programmer language, not manipulation is
his tool of creation.
Words become worlds beautiful and should I assume that you
believe in the simulation hypothesis?
I mean just to give you the context for that quote, right?
I'm talking about being a programmer.
Right and and how being a programmer is it is that heart
a creative activity, right?
The thing that's beautiful about computer science is this
is more than anything else.
Why I went into computer science is there's so much range
to create.
You can create a movie, a piece of music, you can create
a book, but with programs with computers, you can create
the world.
Not just, you know, a world that you describe on the page,
but actually a real moving world that people can interact
with and you can create a very very rich world.
Now the downside of that is that as every programmer knows
is that it's hard, right?
You know, there's bugs.
There's like, you know, you will spend a lot of your time
just, you know, shoveling crap and whatnot.
But at the end of the day a programmer is a miniature God.
Right and I think a lot of people unfortunately don't
go into computer science because they get exposed to
like computer science 101, which is, you know, learning
how to program in Java, which is all the unpleasant stuff
and none of the interesting.
It's like, you know, it's like if you design a course to
make computer science unappealing to people, it would
be what most, you know, programming 101 courses are.
So, you know, in part of what I'm trying to explain in
that, in that chapter, right, is like the larger context
of computer science around, around AI and why, you know,
and why there's so much power, right?
I literally, the whole reason why computer science is
powerful is that once I have taken that trouble of creating
that, you know, world, small or large, it will then work
by itself, right?
That's why you can have, you know, WhatsApp, a company
created by, you know, a dozen people in a couple of years,
you know, being sold to Facebook for what, 12 million,
12 billion dollars, right?
How can that small number of people make that big of
a difference?
It's because of this, right?
And so I'm making an analysis and then interestingly,
right, the God of Genesis, right, the whole story of
Genesis starts with the word, right?
In the beginning was the word.
So the idea is that like, you know, we tend to think of
language as something that originates in the world after,
you know, many things that went before, right?
But you can also look at it the other way, where, you know,
like there are a lot of words that start with language
because they're defined in language, right?
Now, of course, the simulation hypothesis that, yeah, we
are all in a simulation, right?
And I was very fascinated that that hypothesis for about
six months when I was maybe 20, right?
And then because it is kind of really fascinating, right?
But then after a while you realize the following is well,
I don't know if this hypothesis is true or false, but either
why you have to test it or I don't, right?
And I almost by definition, I can't think of a way to test
that hypothesis, right?
If there's a way to test it by God, we should be all over
it. But if there isn't a way to test it, then you know,
it's pointless to worry about it.
Maybe we're on a computer tomorrow.
Someone will pull the plug, but you know, I have no way
of knowing. So in a way, the simulation hypothesis is a
metaphysical hypothesis because it's not something we can
actually think much.
I mean, we can, it's a great motif for like scientific,
you know, speculation and science fiction and whatnot.
But you know, my point of view about a lot of these questions
is about, you know, as things like, you know, does God exist
and whatnot is like we do not have the ability, you know,
language gives us the ability to ask more questions than
we can answer.
And this is one of them.
I find it really helpful to think of maybe smaller
simulation models that the society that we are experiencing
it doesn't necessarily have to be a digital simulation.
Just the reality that has been created for many, many
centuries, I would say, by people, power holders that,
you know, people were reading news from very specific kind
of sources and media was kind of projecting a specific kind
of a message.
So a true man show like kind of a life that most people are
experiencing itself can be looked at at some kind of a
simulation, right?
I agree with that.
So part of what every society does, every culture is
constructive reality.
People in different societies truly live in different
realities.
People in the Middle Ages just live in a different reality
from the world was the same, but their reality was completely
different based on their need and objectives and experience.
No, I'm like, you know, they believe that, you know, I
think even at least, you know, in the West, even people
who are very strong believers in the Christian God do not
believe in him the same way that people in the Middle Ages
in the Middle Ages, like God was with you every second
of the day or like like some cultures believe that your
ancestors are watching over you.
And it's like you really are every day thinking about what
your ancestors are doing looking at you, right?
And you could debate to what extent there is metaphorical
or not to what extent people really do believe in these
gods the same way they believe in reality.
But the bottom line is this is informs what they think every
every moment of the day, right?
And I think the society that we have today is actually
no exception, right?
In fact, there was a philosopher I forget who that said
something that I thought was incredibly insightful.
Which was that science is the mode of perception of
industrial society.
It is also a mode of perception.
It is one that is particularly good at being in tune with
reality.
And that's what makes it powerful, but it's still a mode
of perception that against science a hundred years from
now could look very different from science now.
In fact, science now is already, you know, the scientific
mode of perception today is very different from the scientific
mode of perception 200 years ago.
So this is part of what science society societies do is
they construct this reality.
In fact, your brain constructs a reality, right?
Vision researchers say that vision is controlled hallucination.
And this is exactly right, right?
Like, you know, you don't actually see things.
You see what your brain has conjured up in response to
the stimuli.
But every object that you see in the world is actually a
creation of your brain, right?
That hopefully lines up with something that's in the world,
but you only interfaces those photons.
So there is an objective reality.
We are having a subjective experience of it.
And meanwhile, there are people who are trying to define
and orbit those subjective experiences in an objective
kind of a way.
Yeah.
So throughout human history.
Yeah.
And you know, to answer this question squarely, is there
an objective reality?
I believe that there is.
I also believe that just as the simulation there is no final
test that you can use to say, oh, this is the objective
reality, right?
Precisely because we do not have direct access to it, right?
But now, you know, here's a crucial question, right?
Which gets to the whole, you know, social problem around
this is, are we better off?
Assuming that there's an objective reality and we should
try to find out what it is and converge on it, try to agree.
The point of there being an objective reality is that you
and I should be able to agree on it.
I don't have access to it regardless of our state.
Yeah.
And if we disagree, there should be a test that shows, oh,
you're right.
I'm wrong.
So now I will change my beliefs, right?
Once you do like, you know, these postmodernists and critical
theorists and whatnot and say like, oh, no, no, no, no, there's
no objective reality.
There's just everybody has their own, you know, you have
your own knowledge, right?
The problem with this is that it destroys the fabric of society.
It means we are no longer trying to converge on something, right?
There's no longer, you know, it's like, you know, there's
this hyping of like the lived experience, right?
The lived experience is more important than science.
I mean, I understand we all have a lived experience, but the
problem with that is that if that's the criteria, then, you
know, we can't talk anymore.
My lived experience cannot be refuted by you.
If I say you were discriminating against me, it's my lived
experience.
You're not allowed to disagree.
Right.
Right.
If we agree that there's an objective reality, we go like,
okay, so what's the evidence that you're being discriminated
against?
Right.
And again, I've had the experience, you know, over the
years of talking with a lot of people about this.
And the problem is that a lot of these beliefs are not amenable
to evidence, right?
It's like, there's no, again, at which point they're kind of
like a religious belief, right?
And again, but if you believe that everybody has their reality,
then that's fine.
But you can, but my point is like, there's the scientific
question of is there one or not?
And I think, you know, there is, but I can't prove it.
But there's also, and maybe less important to scientists, but
more important to society than the utilitarian question, which
is, are we better off believing that there's an objective
reality or not?
And I definitely say we are better off in believing that
there's an objective reality.
I think better or worse will be determined based on the
context and our objective, the goal that we are trying to
reach.
But I think that's beside the point of what these people,
postmodernists or whoever they, which to me, they're just
totalitarians.
That's what they are.
I mean, they are, because then they use this as an excuse
to impose their view.
But you know, you said very well that it depends on the
goal.
But let's just say that, you know, we can all agree that,
you know, life is better than death and health is better
than sickness and et cetera, et cetera.
If you're in tune with objective reality, you can have
medicine and you can save people's lives.
And you can build cars and planes and you know, CRT scans
and you name it.
So being in tune with objective reality makes a big difference
to your well-being at the end of the day.
Yeah.
Again, well, the reason we are where we are today versus
the Middle Ages, right, is that we believe in this and
that's what has created all the scientific and technological
progress that these postmodernists not take for granted
without which they wouldn't even exist, right?
Yeah.
And their depth of hypocrisy is that they're saying there
is no objective experience, but then they want to make
that subjective opinion into an objective experience.
Yeah.
Again, there were generations of postmodernists.
The early generation was, you know, anything goes, right?
The derives and the Foucault's and whatnot.
But the current generation, of course, has perverted that
postmodernism into like, no, my truth is the truth.
Yeah, which, you know, the question is if my truth is that
you're full of shit.
I mean, what are we going to do then?
How are we going to settle it, which is a very dangerous
route because this kind of my truth, your truth, how are
we going to settle it?
It always leads to violence.
Exactly.
And again, this is what the postmodernists said.
And again, I mean, like if you start from the premise
that there's no objective reality, then what determines
what is the perceived reality?
Power, right?
It's who has the most power.
And in fact, you know, people have been saying this for
a while, right?
And of course, power influences your perception of reality.
But in a way, what you see today is like that taken to
its logical extreme, right?
And so these people, they want to take power because they
want to impose their reality on the rest of us, regardless
of whether it has anything to do with objective reality or not.
Do you also think this option, because I see it as a war
of meaning, because we experience certain things, but
the war is over how they define what we are experiencing.
Do you see this wokeism, this totalitarian approach to
postmodernism, however, you want to define it as a inevitable
and natural end of a fully secular liberal democratic
system?
That's actually a great question.
I think it's not an inevitable end, but unfortunately
once, so there was the enlightenment, right?
That created this mindset where, you know, there's a
marketplace of ideas, etc, etc, right?
And this is one of the best things that have happened to
humanity.
Again, it's why we have the life that we have today.
Unfortunately, it opens itself up.
The marketplace of ideas is open to bad ideas by definition,
right?
Because you don't know if they're good or bad going yet.
And then some bad ideas can acquire power, right?
And this started with the French Revolution, but maybe
the paradigmatic example is Marxism, right?
The thing about Marx is that he wasn't just a philosopher,
right?
He said we are going to have this praxis that is going to,
you know, first of all, he had these prophecies that were
wrong, but like this was like we're going to impose these
ideas on the people and then, you know, Lenin developed
that further.
And wokeism is really just the latest incarnation of this
problem.
It's not the first one and it will not be the last one.
Who knows what the next one will look like, but this problem
will always be with us.
It's like, you know, a liberal society is actually an amazingly
functioning organism, but it needs an immune system against
these, you know, diseases and every now and then, you know,
the immune system is not functioning very well in certain
context like the universe that we've talked about and then
you get the sepsis, you know, where the germs multiplied
very fast before the immune system can catch up with them.
And if you don't do something about it, you know, it does
kill the organism, right?
So it may well be that at the end of the day, we will come
crashing down because, you know, these people make it come
crashing down because they debase education so much that,
you know, you know, math is racist and, you know, you know,
two plus two equals five and you can't and, you know, and
your bridges, you know, don't stand up anymore and so on,
right? I hope it won't come to that, but I could see it
coming to that.
So this is a problem that will recur, right? And in different
forms, right? You know, like the cultural revolution was
similar to the October Revolution in some ways, but quite
different in some others and, you know, wokeism is different
from these in some ways.
One of the most important ones is that we have a technology
now that we didn't then.
But if you look at the totalitarian regimes of the mid
20th century, they were master users of the mass media of
the time.
Stalin and Hitler were the first ones to really make use
of radio and film and whatnot for propaganda purposes.
And then, you know, Churchill and Roosevelt caught up and
learn how to use those, right? But they were behind in the
beginning, right? And in every, well, the most famous example
is the printing press, right? It was without no printing press
there probably not have been, you know, the Reformation and
someone, right? So in every, you know, in every, you know,
turn of the wheel, the technology that's available,
right, becomes one of the key defining elements of what
happens. And again, what you have to do is you have to
master that technology so that, you know, you don't wind up
under the foot of the ones who didn't while you were idle.
Yeah, technology is the key. I had this conversation with
someone else, Jason Rizzo-Giorgiani. I don't know if
you know him, philosopher. And we were basically talking
about how a tech-based and technology-driven narrative
has been lacking on the right side of politics for a very,
very long time. And they lost many different aspects of
this war, whatever you want to call it, exactly because of
it.
Yeah. And why did that happen? I think the biggest reason
is that the universities are, you know, left-wing zones,
right? All the people seeing this firsthand and, you know,
taking advantage of it and then building on it, you know,
the unfortunately conservatives do not have enough access
to the universities, which again is terrible because, you
know, some of their ideas will be right, some of them will
be wrong. But you need those people looking at technology,
right? And, you know, like the thing about, you know, this
whole area of AI ethics and whatnot is that, like, everybody
is left-wing.
Yeah.
Right. It's like, again, you know, people have, like
journalists and what have asked me, you know, this in my
life. So what do conservatives think about AI ethics? And
I actually have a hard time answering this question because
I don't know.
They probably don't even know it exists.
Yeah, exactly. I mean, like, so, you know, I wrote this
piece in The Spectator, right, that his entire goal was you
got to, you know, alert conservatives to the fact that
there is this not happening and they need to get into this
with their ideas and we need to have a real debate before
the science is settled. But yeah, that's the problem is
that the universities have shut out the conservatives, which
of course, you know, creates an imbalance of power, you know,
because now the, you know, the people on the left wing can
use these things and the people on the right wing don't.
Very true. Very true. Pedro's book is called The Master
Algorithm, How the Quest for the Ultimate Learning Machine
Will Remake Our World. It was one of the only two AI books
on the bookshelf of Xi Jinping, President for Life in China.
We almost talked for two hours. Thank you so much for this.
I enjoyed it tremendously. Let me ask you the last question
I ask all my guests. Actually, let me ask you one question
before that, that what is next for you and where can our
audience follow your work?
Well, what's next for me is more machine learning research.
I think we are at a very exciting time in AI, where the
really important things are going to have not happened yet,
but they're going to happen the next 10, 20 years. So this
is, you know, I started out in AI, you know, 20 or 30 years
ago, but this is the time. Everything was just leading up
to this. So that's one of the things I want to do. The other
thing I want to do is, you know, write books and essays.
I think, again, there's a lot to be done in that format and
for a broader audience than just the AI specialists. So I
think The Master Algorithm was my first book, but hopefully
it will by no means be my last. To follow me, the easiest
thing to do is follow me on Twitter because, you know, what
I do in these various areas, I usually post there. And of
course, you know, I will publish things in various places,
both research and for general audience. So that's the
other side.
Excellent. The advancement that you're seeing in AI and machine
learning. Let me ask you this way. How do you feel about Ray
Kurzweil's dates with respect to the Singularity 2045 that
he's talking about?
I think first of all, it's very hard to predict. The most
important thing about scientific progress is that it's
unpredictable. And it happens in jumps. You can have long
periods of low progress followed by periods of very rapid
progress. And I don't know when the next so we are on this
wave right now, and they will plateau at some point, it hasn't
plateaued yet, but it will inevitably plateau. Where the
plateau will happen, you can't predict because it depends, you
know, this is not like some law of physics, it depends on
what, you know, we the researchers come up with. And I
don't know how many jumps are needed to get to, you know,
human level AI, and what the barriers are in between. So
it's very hard to predict. Now, so anybody who gives you a
date for when we'll have strong artificial intelligence is
speculating, right? And I think Ray is speculating, you know,
with some basis to it. But I think the first or the bit is
the error bars are enormous. At one extreme, maybe it will
never happen because the problem is just too hard. Right, I
know, I'm a scientist, I believe in reductionism, I think
there is an algorithm, you know, again, part of the empirical
evidence for the master algorithm is that your brain does
it. And, you know, and, you know, I should be able to write
down the program that that your brain is running, I just don't
know what it is yet, it could be complex or not, right, but it
might be too complex. And in which case, we will never get to
it. I don't think that's the case, but it's a possibility. At
the other end, and more excitingly, maybe, you know, some
kid in a garage is inventing the master algorithm right now. In
fact, part of my motivation for writing the book, and I say it
is, again, you know, following on some of the things that we've
been talking about, we in the field have focused on too
narrowly on a few paradigms. And my feeling is that even after
we've unified them, and we're making very good progress
towards that, some of the really important ideas will still be
missing. And actually think someone outside the field is
more likely to come up with them than someone who's already
thinking along these tracks. So a non expert, but at the same
time, they need to know enough. So maybe reading a book like
the master of them will help. So it could go all the way from
it's already happening to it will never happen. Even if it's
already happening, right, if someone had that amazing
algorithm today, it would still take decades for it to play
itself out. Right. So where do I think we'll be in, you know,
2047? I think we will be much farther along than we are today.
Exactly, exactly where we are is very hard to predict. Are you
getting this?
I can't wait for it.
Yeah.
This unfortunately, my cell phone. So yeah.
So sorry.
So yeah, so, so, so sorry, I lost my turn of the big big
changes that is going to happen.
Right. So where are we going to be in 2047? I don't think
anybody knows for sure. But I think what we want to do is be
prepared for the range of places where we could be right from
the most pessimist part of you know, machine learning is all
about prediction. And actually half the science of prediction
is knowing what you can't predict, but being prepared for
all the possible outcomes. And I think this is what we want to
know when we think about AI in 2030, 2040, etc. Right. We want
to think about the range of places where it could be and we
want to be prepared for all those possibilities, not assume
that there's going to be one scenario and go for that one.
Excellent. Let me ask you the final question I ask all my
guests that if you come across an intelligent alien from a
different civilization, what would you say is the worst thing
humanity has done? And what would you say is our greatest
achievement?
I would say our greatest achievement is the extent to
which we know nature and the universe, including ourselves.
Right. If you think if you think of the universe as being made
of, you know, atoms and, you know, photons, we are the
completely insignificant. Right. We are specks of dust on a
speck of dust. But if you think of the universe as being made
of information, and increasingly, people across science
see the universe as being made of information. Human beings
are amazing. There is no bigger concentration of information
than the one in your brain. And the concentration and evolution
has been concentrating information in your brain for,
you know, millions of years, hundreds of millions of years.
Millions of years, hundreds of millions of years. And once you
start reading books, right, once you start communicating with
people, there's more information with your brain. Once you
start studying nature and writing books about it, there's
more information. Now, we as a society, right, as a repository
of information about the universe, right, you know, if
you put a diagram of the universe, you know, like, you
know, how much information there is, you know, like, you
know, Earth would be this Dirac Delta functions, like this
sharp, super high peak, right? Compare the amount of
information that exists on Earth, you know, thanks to life,
but in particular, thanks to humanity, with the amount of
information that exists on Mars or Venus, there's just no
comparison, right? None. So that's our biggest achievement,
right? What is the worst thing that we have done? Well, of
course, is that in the process of climbing this ladder, we
have, you know, we have done so much death and destruction,
right? Like, you know, and, you know, that is often
exaggerated. You know, people have a much more pessimistic,
you know, if you read Steven Pinker's books, for example,
he'll show you why on balance, you know, few people die
violently today, much fewer than before, nevertheless,
right? You know, things like, you know, World War One and
World War Two, and, you know, nuclear weapons are still here,
right? So far, we've avoided the worst. But again, you can't
take any of that for granted. So that, I think, is the big
biggest downside. Excellent. Well, the alien, the alien will
probably have the same thing to say to us is my guess. They
will be proud of the same about themselves. Oh, yeah,
interesting. About their achievements. And you know,
they're, you know, the things that they, you know, are not
proud of. Yeah, an interesting variation I heard about the
alien itself was that the alien could be coming from a
civilization that is hive mind. So the priorities would
be different than what we were experiencing individually.
I actually think hive minds are not as adaptive as what we
have. Oh, really? Our society is a mess. Because it's a bunch
of independent agents. And we have, you know, and the cost
of, you know, dealing with the conflicting interests is very,
very high. So you know, our society is far from optimal, we
can make it better, I think, with technology and with being
more mature. But a hive mind right is actually also it goes
too far. Right. In machine learning, there's this bias
variance trade off. Right? I think in human society, we have
a lot of variance, right? A hive mind has does not have
enough variance, right? A society that causes more able to
generate new ideas, and then let the better ideas grow than
a hive mind is, right? A hive mind is actually not the most
adaptive. So I think at the end of the day, humans beat the
board, right? Human society for all its imperfections, right?
And for all the mess that you know, human life is at the end
of the day, this is a much more adaptive way to organize
the species than the board ever could be.
Transcribed by https://otter.ai
