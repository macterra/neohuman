1
00:00:00,000 --> 00:00:02,500
This is how machine learning and AI and many other algorithms

2
00:00:02,500 --> 00:00:04,100
work is like there's an objective function

3
00:00:04,100 --> 00:00:05,800
and you're trying to optimize it.

4
00:00:05,800 --> 00:00:07,680
And really where the social discussion

5
00:00:07,680 --> 00:00:09,480
and the personal intervention needs to happen

6
00:00:09,480 --> 00:00:10,800
is at the level of what is

7
00:00:10,800 --> 00:00:13,120
or what should be that objective function.

8
00:00:13,120 --> 00:00:15,840
How it gets optimized is really like a technical problem.

9
00:00:15,840 --> 00:00:17,620
It's like, how do you make the engine run faster?

10
00:00:17,620 --> 00:00:20,200
Well, you know, let the engineers worry about that.

11
00:00:20,200 --> 00:00:23,040
And then you decide what speed you want to drive it at.

12
00:00:23,040 --> 00:00:25,680
And the thing is that maximizing engagement

13
00:00:25,680 --> 00:00:28,520
as an objective function is actually

14
00:00:28,520 --> 00:00:30,300
a perfectly natural thing to do.

15
00:00:31,640 --> 00:00:33,580
The problem is the side effects.

16
00:00:33,580 --> 00:00:35,120
Right, the consequences.

17
00:00:35,120 --> 00:00:36,120
Exactly. Right.

18
00:00:36,120 --> 00:00:39,080
So the algorithms are to blame, yes,

19
00:00:39,080 --> 00:00:41,640
but the engineers are to blame,

20
00:00:41,640 --> 00:00:43,240
not for their evil intent,

21
00:00:43,240 --> 00:00:45,220
but for just having set the algorithms

22
00:00:45,220 --> 00:00:48,000
to maximize this thing without realizing

23
00:00:48,000 --> 00:00:49,880
all the consequences that we'd have.

24
00:00:49,880 --> 00:00:52,040
Which in fairness, in the early days,

25
00:00:52,040 --> 00:00:53,640
it was easy to miss that, right?

26
00:00:53,640 --> 00:00:55,520
Again, I don't think people were being evil

27
00:00:55,520 --> 00:00:56,880
or really stupid about it.

28
00:00:56,880 --> 00:00:58,320
They were just oblivious.

29
00:00:58,320 --> 00:01:01,080
And we can't afford to be oblivious at this point.

30
00:01:01,080 --> 00:01:06,080
Dr. Pedro Dominguez, welcome to the 90th episode

31
00:01:12,520 --> 00:01:14,720
of Neo Human Podcast, sir.

32
00:01:14,720 --> 00:01:15,680
Thanks for having me.

33
00:01:15,680 --> 00:01:16,680
Yeah, it's a pleasure.

34
00:01:16,680 --> 00:01:20,200
I've heard your name for the first time

35
00:01:20,200 --> 00:01:21,840
because of your book, obviously,

36
00:01:21,840 --> 00:01:23,720
The Master Algorithm, How to Quest

37
00:01:23,720 --> 00:01:26,240
for the Ultimate Learning Machine Will Remake Our World.

38
00:01:26,240 --> 00:01:29,400
But then on the bookshelf

39
00:01:29,400 --> 00:01:32,440
of China's President for Life, President Xi,

40
00:01:32,440 --> 00:01:36,820
was two artificial intelligence-related books,

41
00:01:36,820 --> 00:01:38,040
and yours was one of them.

42
00:01:38,040 --> 00:01:41,640
How did that feel hearing something like that?

43
00:01:41,640 --> 00:01:45,360
Because I would imagine that it was not coordinated with you.

44
00:01:45,360 --> 00:01:48,380
No, it wasn't coordinated, so I wasn't expecting it.

45
00:01:48,380 --> 00:01:52,320
And it felt, well, mixed feelings.

46
00:01:52,320 --> 00:01:55,200
On the one hand, it's good that China,

47
00:01:55,200 --> 00:01:57,120
at the highest levels of its leadership,

48
00:01:57,120 --> 00:01:59,680
understands that AI is important

49
00:01:59,680 --> 00:02:01,720
and that they want everybody else in the country

50
00:02:01,720 --> 00:02:02,760
to pay attention to it.

51
00:02:02,760 --> 00:02:04,640
And I think a lot of good things will come of it,

52
00:02:04,640 --> 00:02:07,600
a lot of applications in many different areas.

53
00:02:07,600 --> 00:02:12,600
At the same time, seeing a totalitarian regime

54
00:02:12,600 --> 00:02:16,200
get excited about AI, and my book helping with that

55
00:02:16,200 --> 00:02:18,400
is not a very reassuring feeling.

56
00:02:18,400 --> 00:02:22,640
Yeah, it must have been quite a unique experience.

57
00:02:22,640 --> 00:02:24,200
What was the thesis of the book,

58
00:02:24,200 --> 00:02:26,240
if you don't mind describing it a little,

59
00:02:26,240 --> 00:02:29,000
and why did you write it when you did?

60
00:02:29,880 --> 00:02:32,600
Yeah, so the book is an introduction to machine learning

61
00:02:32,600 --> 00:02:34,600
for a general audience.

62
00:02:34,600 --> 00:02:37,400
And the reason I wrote it is that I believe

63
00:02:37,400 --> 00:02:40,720
we've come to a point where it's not enough

64
00:02:40,720 --> 00:02:44,920
for machine learning to be known by just the experts.

65
00:02:44,920 --> 00:02:46,920
Everybody needs to understand machine learning

66
00:02:46,920 --> 00:02:49,840
at some level because it really affects everybody's life

67
00:02:49,840 --> 00:02:53,320
in many ways, some of them very important.

68
00:02:53,320 --> 00:02:56,200
From presidents and CEOs on down to everybody

69
00:02:56,200 --> 00:02:58,320
in their professional private lives,

70
00:02:58,320 --> 00:03:00,640
you need to understand what machine learning does

71
00:03:00,640 --> 00:03:02,720
and what it doesn't do so you can,

72
00:03:02,720 --> 00:03:04,440
number one, make the best use of it,

73
00:03:04,440 --> 00:03:07,680
for example, professionally and even personally,

74
00:03:08,560 --> 00:03:10,600
so that you know how to interact with the systems

75
00:03:10,600 --> 00:03:12,200
that are using machine learning all the time,

76
00:03:12,200 --> 00:03:15,200
like Google and Twitter and Facebook and so on.

77
00:03:15,200 --> 00:03:18,440
And also so that in a democracy,

78
00:03:18,440 --> 00:03:21,240
there are a lot of important decisions to be made

79
00:03:21,240 --> 00:03:23,360
about how machine learning gets used and doesn't,

80
00:03:23,360 --> 00:03:25,800
and they're supposed to be made by everybody.

81
00:03:25,800 --> 00:03:27,600
And what I saw that was frustrating me

82
00:03:27,600 --> 00:03:29,480
was that at the time I wrote the book,

83
00:03:29,480 --> 00:03:31,120
and even more so now,

84
00:03:31,120 --> 00:03:34,520
is that there was an enormous amount of discourse

85
00:03:34,520 --> 00:03:37,640
about machine learning that was really, really uninformed

86
00:03:37,640 --> 00:03:39,520
by the realities of machine learning.

87
00:03:39,520 --> 00:03:41,600
This was when, 2015?

88
00:03:42,640 --> 00:03:46,480
Well, so I started writing the book in 2012.

89
00:03:46,480 --> 00:03:49,840
The timing was actually perfect because, you know,

90
00:03:49,840 --> 00:03:53,920
by the time the book came out in 2015,

91
00:03:53,920 --> 00:03:55,520
the interest in machine learning had gone up,

92
00:03:55,520 --> 00:03:58,920
like an AI had just gone up to a different level.

93
00:03:58,920 --> 00:04:01,880
I mean, I first thought of writing a popular science book

94
00:04:01,880 --> 00:04:04,400
about machine learning when I was a grad student,

95
00:04:04,400 --> 00:04:05,480
in the nineties,

96
00:04:05,480 --> 00:04:07,760
because already then machine learning was taking off

97
00:04:07,760 --> 00:04:09,640
and you're seeing some press stories about it.

98
00:04:09,640 --> 00:04:11,840
It's like this thing about exponential growth

99
00:04:11,840 --> 00:04:14,240
is that you think something didn't exist

100
00:04:14,240 --> 00:04:15,760
until it bursts onto your view,

101
00:04:15,760 --> 00:04:18,600
but then you see that then it reaches

102
00:04:18,600 --> 00:04:20,440
the next level of people.

103
00:04:20,440 --> 00:04:21,400
But at the time, you know,

104
00:04:21,400 --> 00:04:23,400
I didn't feel an enormous urgency about it.

105
00:04:23,400 --> 00:04:27,000
And also I didn't have a good idea

106
00:04:27,000 --> 00:04:30,560
of how to write a book about machine learning.

107
00:04:30,560 --> 00:04:32,880
Because writing a book for a general audience

108
00:04:32,880 --> 00:04:34,800
is not like writing a textbook, right?

109
00:04:34,800 --> 00:04:36,360
When you're writing a textbook, you can say like,

110
00:04:36,360 --> 00:04:38,080
oh, here's a chapter about neural networks,

111
00:04:38,080 --> 00:04:40,560
here's a chapter about vision networks.

112
00:04:40,560 --> 00:04:41,800
But that's too boring, right?

113
00:04:41,800 --> 00:04:42,840
Nobody's gonna read that.

114
00:04:42,840 --> 00:04:44,840
Right, not anymore.

115
00:04:44,840 --> 00:04:47,800
Well, yeah, maybe, good point.

116
00:04:47,800 --> 00:04:50,720
But nevertheless, it's helpful to have a story, right?

117
00:04:50,720 --> 00:04:51,800
To have a theme.

118
00:04:51,800 --> 00:04:53,280
And so I studied, you know,

119
00:04:53,280 --> 00:04:54,680
how to write these kinds of books.

120
00:04:54,680 --> 00:04:58,520
And there's a certain number of schemas that you can use.

121
00:04:58,520 --> 00:04:59,920
One of them, popular in science books,

122
00:04:59,920 --> 00:05:03,160
is the mystery that you wanna solve or the quest.

123
00:05:03,160 --> 00:05:05,920
And immediately I realized that the right story

124
00:05:05,920 --> 00:05:09,280
for this book is the quest for the master algorithm.

125
00:05:09,280 --> 00:05:10,400
Because that's what really,

126
00:05:10,400 --> 00:05:12,560
and that's where the title of the book comes from.

127
00:05:12,560 --> 00:05:15,560
The thing that is really remarkable about machine learning

128
00:05:15,560 --> 00:05:18,640
and different from everything we've seen before

129
00:05:18,640 --> 00:05:20,520
and central to what it does,

130
00:05:20,520 --> 00:05:24,080
is that one algorithm can learn

131
00:05:24,080 --> 00:05:27,400
to do an infinite number of different things, right?

132
00:05:27,400 --> 00:05:29,080
Turing machines can do anything,

133
00:05:29,080 --> 00:05:30,400
and that's the power of computers,

134
00:05:30,400 --> 00:05:33,560
as Turing first explained.

135
00:05:33,560 --> 00:05:35,560
But if you want a computer to play chess,

136
00:05:35,560 --> 00:05:37,720
you have to program it to play chess.

137
00:05:37,720 --> 00:05:39,600
If you want the computer to fly an airplane

138
00:05:39,600 --> 00:05:41,760
or be a search engine, you have to program it to do that.

139
00:05:41,760 --> 00:05:43,400
The thing that's amazing about machine learning

140
00:05:43,400 --> 00:05:45,000
is that you don't need to do that.

141
00:05:45,000 --> 00:05:46,760
Just one learning algorithm,

142
00:05:46,760 --> 00:05:48,720
if you feed it the right data,

143
00:05:48,720 --> 00:05:50,800
will learn to do all these different things.

144
00:05:50,800 --> 00:05:52,680
Case in point, look at the back prop, right?

145
00:05:52,680 --> 00:05:56,080
Back prop is used to do everything, right?

146
00:05:56,080 --> 00:05:57,440
So the idea of a master algorithm

147
00:05:57,440 --> 00:05:59,160
is like the idea of a master key, right?

148
00:05:59,160 --> 00:06:02,400
It's one key that opens all the locks.

149
00:06:02,400 --> 00:06:03,840
But of course, this is an ideal.

150
00:06:03,840 --> 00:06:04,960
We're not there yet.

151
00:06:04,960 --> 00:06:08,360
There's many different ways to do it.

152
00:06:08,360 --> 00:06:09,800
And that's a lot of what the book is,

153
00:06:09,800 --> 00:06:11,440
different paradigms in machine learning

154
00:06:11,440 --> 00:06:14,200
and how you might unify them all at the end of the day.

155
00:06:14,200 --> 00:06:17,400
And also what the applications in the world

156
00:06:17,400 --> 00:06:19,560
of machine learning are today.

157
00:06:19,560 --> 00:06:22,280
And also, the last chapter in the book,

158
00:06:22,280 --> 00:06:23,640
which in some ways for a lot of people

159
00:06:23,640 --> 00:06:26,720
is the most interesting one, is a look at the future

160
00:06:26,720 --> 00:06:30,080
and where things are headed as we use more machine learning.

161
00:06:30,080 --> 00:06:33,560
And honestly, a lot of the stuff that I wrote about

162
00:06:33,560 --> 00:06:37,160
back in 2015, not only has it come true,

163
00:06:37,160 --> 00:06:40,160
it has exceeded my predictions.

164
00:06:40,160 --> 00:06:44,600
The dystopian side of things?

165
00:06:44,600 --> 00:06:48,120
Not just, it's funny that you would put it that way.

166
00:06:48,120 --> 00:06:52,200
I think this is true of every technology.

167
00:06:52,200 --> 00:06:54,440
And a lot of things that are true of every technology

168
00:06:54,440 --> 00:06:57,640
are particularly true of AI because AI really

169
00:06:57,640 --> 00:07:00,760
fires people's imagination for better and for worse.

170
00:07:00,760 --> 00:07:03,920
And usually in the first wave of a technology,

171
00:07:03,920 --> 00:07:07,440
people are dazzled by the potential.

172
00:07:07,440 --> 00:07:10,160
And this I think is where things were in 2015.

173
00:07:10,160 --> 00:07:12,200
But one of the things that I predicted then

174
00:07:12,200 --> 00:07:14,440
is that, well, there's all these concerns

175
00:07:14,440 --> 00:07:16,760
like privacy and whatnot, and sooner or later

176
00:07:16,760 --> 00:07:19,520
this is going to blow up.

177
00:07:19,520 --> 00:07:23,000
And it has blown up in the biggest possible way.

178
00:07:23,000 --> 00:07:25,560
And then what happens when the concerns come to the fore

179
00:07:25,560 --> 00:07:29,960
is then they take center stage and everybody's paranoid

180
00:07:29,960 --> 00:07:32,320
and focused on the dangers and the dystopian scenarios,

181
00:07:32,320 --> 00:07:34,280
which is where we are now.

182
00:07:34,280 --> 00:07:36,880
Everybody's just completely going off the rails

183
00:07:36,880 --> 00:07:38,640
with the dangers of AI in a way that I

184
00:07:38,640 --> 00:07:41,320
think is also unbalanced.

185
00:07:41,320 --> 00:07:43,040
And then finally, there comes the point

186
00:07:43,040 --> 00:07:46,040
where people just settle down to making things work,

187
00:07:46,040 --> 00:07:47,640
and then they do make things better.

188
00:07:47,640 --> 00:07:49,800
Maybe not quite as the utopia envisaged,

189
00:07:49,800 --> 00:07:51,520
but also preventing the dystopias.

190
00:07:51,520 --> 00:07:53,440
And I have no doubt that the same thing will

191
00:07:53,440 --> 00:07:57,960
happen with AI if we work on making it happen.

192
00:07:57,960 --> 00:08:00,400
The future of AI is not utopian or dystopian

193
00:08:00,400 --> 00:08:01,480
in some predetermined way.

194
00:08:01,480 --> 00:08:02,880
It's what we make of it.

195
00:08:02,880 --> 00:08:05,320
It will be good because we make it good, which, again,

196
00:08:05,320 --> 00:08:07,360
gets back to the motivation for writing the book,

197
00:08:07,360 --> 00:08:11,080
is that we need input from everybody on what to do with this.

198
00:08:11,080 --> 00:08:14,200
Also, there's another, it's exactly, precisely,

199
00:08:14,200 --> 00:08:15,400
AI is a tool.

200
00:08:15,400 --> 00:08:16,520
Exactly right.

201
00:08:16,520 --> 00:08:18,040
AI is a tool.

202
00:08:18,040 --> 00:08:19,840
And a lot of people talk about the power

203
00:08:19,840 --> 00:08:22,200
that AI gives, x, y, or z.

204
00:08:22,200 --> 00:08:24,400
And I think people need to bear in mind

205
00:08:24,400 --> 00:08:28,480
is AI is a tool for those who know how to use it.

206
00:08:28,480 --> 00:08:30,760
If you don't know how to drive a car,

207
00:08:30,760 --> 00:08:33,960
it won't take you where you want to go.

208
00:08:33,960 --> 00:08:35,960
You don't need to understand how the engine works.

209
00:08:35,960 --> 00:08:37,000
That's for the engineers.

210
00:08:37,000 --> 00:08:38,440
But you need to understand where the steering

211
00:08:38,440 --> 00:08:39,440
wheel and the pedals are.

212
00:08:39,440 --> 00:08:41,840
And the idea of the book is to tell people not necessarily

213
00:08:41,840 --> 00:08:45,120
about the engine, but about the steering wheel and the pedals

214
00:08:45,120 --> 00:08:49,320
so that it can be a tool for you so that it makes you powerful.

215
00:08:49,320 --> 00:08:51,840
We don't want AI to make total-terrain governments

216
00:08:51,840 --> 00:08:54,600
powerful or companies that are already over mighty,

217
00:08:54,600 --> 00:08:55,840
even more over mighty.

218
00:08:55,840 --> 00:09:00,080
We want AI to be a tool that empowers every single one of us.

219
00:09:00,080 --> 00:09:03,400
That's the goal, and that's very much why I wrote the book.

220
00:09:03,400 --> 00:09:06,120
Yeah, I think one of the reasons that the dystopian side

221
00:09:06,120 --> 00:09:10,560
of things are becoming more and more maybe attractive

222
00:09:10,560 --> 00:09:14,480
would be an interesting term to use it for a lot of people

223
00:09:14,480 --> 00:09:17,800
is that they're experiencing it firsthand, especially

224
00:09:17,800 --> 00:09:22,600
if you're coming from a certain way of thinking about politics

225
00:09:22,600 --> 00:09:24,720
and life and philosophy and all that.

226
00:09:24,720 --> 00:09:26,920
You just see that you're being shut down,

227
00:09:26,920 --> 00:09:29,800
you're being silenced, you're being deplatformed.

228
00:09:29,800 --> 00:09:32,800
And what you hear is that a lot of people

229
00:09:32,800 --> 00:09:36,160
who are running these companies, they are blaming algorithms.

230
00:09:36,160 --> 00:09:37,680
They're like, well, it's not our fault.

231
00:09:37,680 --> 00:09:41,320
It's the algorithm's fault that prioritized, for example,

232
00:09:41,320 --> 00:09:45,720
liberal headlines against conservative headlines.

233
00:09:45,720 --> 00:09:47,840
It's just an example.

234
00:09:47,840 --> 00:09:49,800
So for a lot of people, this seems

235
00:09:49,800 --> 00:09:53,920
like an existential threat to the way of their life

236
00:09:53,920 --> 00:09:55,480
and the way of their thinking.

237
00:09:55,480 --> 00:09:59,080
And they kind of connect this to algorithms and AI,

238
00:09:59,080 --> 00:10:02,160
which to me is kind of ruining the potential for all

239
00:10:02,160 --> 00:10:04,800
the good stuff that can happen in the coming years and decades.

240
00:10:04,800 --> 00:10:06,720
I do believe that we depend on it

241
00:10:06,720 --> 00:10:08,920
because we've evolved with our technology

242
00:10:08,920 --> 00:10:10,920
to get where we are.

243
00:10:10,920 --> 00:10:13,480
We haven't done it separately.

244
00:10:13,480 --> 00:10:18,880
You're right that people are now experiencing it firsthand.

245
00:10:18,880 --> 00:10:22,360
But what happens is actually that, again, AI

246
00:10:22,360 --> 00:10:25,520
is like the blank canvas onto which people project

247
00:10:25,520 --> 00:10:28,520
whatever preoccupations they have, particularly

248
00:10:28,520 --> 00:10:30,400
political ones.

249
00:10:30,400 --> 00:10:34,960
And for example, liberals are very concerned with fairness,

250
00:10:34,960 --> 00:10:36,680
and they tend to see in AI algorithms

251
00:10:36,680 --> 00:10:41,200
just a cesspool of bias that isn't really there,

252
00:10:41,200 --> 00:10:42,800
but they think needs to be dealt with.

253
00:10:42,800 --> 00:10:44,200
On the other hand, if you're more

254
00:10:44,200 --> 00:10:47,520
of a libertarian persuasion, what you see in AI is big brother.

255
00:10:47,520 --> 00:10:49,760
It's this thing that's going to oppress you.

256
00:10:49,760 --> 00:10:52,800
And the thing to realize is that none of these concerns

257
00:10:52,800 --> 00:10:55,560
are unfounded on any side of the spectrum.

258
00:10:55,560 --> 00:10:59,760
But in many cases, people perceive the threat

259
00:10:59,760 --> 00:11:03,600
to be not just bigger than it really is, but different.

260
00:11:03,600 --> 00:11:06,000
And again, if you understand what's really going on,

261
00:11:06,000 --> 00:11:08,600
actually, you can then address the problems properly

262
00:11:08,600 --> 00:11:11,520
as opposed to all the many very confused things that

263
00:11:11,520 --> 00:11:12,520
are flying around.

264
00:11:12,520 --> 00:11:15,400
Case in point, when you say that the companies say,

265
00:11:15,400 --> 00:11:18,760
oh, don't blame us, blame the algorithms,

266
00:11:18,760 --> 00:11:22,440
this is actually both true and false.

267
00:11:22,440 --> 00:11:26,840
It's true in the sense that, yes, the algorithms

268
00:11:26,840 --> 00:11:28,200
are making these decisions.

269
00:11:28,200 --> 00:11:30,680
And for example, when a lot of conservatives say,

270
00:11:30,680 --> 00:11:32,920
oh, Google, Twitter, et cetera, they're

271
00:11:32,920 --> 00:11:36,360
discriminating against us, I was actually

272
00:11:36,360 --> 00:11:40,640
very skeptical of that for a long time in the past

273
00:11:40,640 --> 00:11:43,760
for the reason that, I mean, obviously, the work

274
00:11:43,760 --> 00:11:46,680
forces at these companies are overwhelmingly liberal.

275
00:11:46,680 --> 00:11:49,000
So that is certainly a cause for worry.

276
00:11:49,000 --> 00:11:53,120
But I have never seen evidence of people deliberately

277
00:11:53,120 --> 00:11:57,720
plugging their liberal politics into the algorithms.

278
00:11:57,720 --> 00:11:59,360
So I don't think that, a lot of the times,

279
00:11:59,360 --> 00:12:01,840
people interpret an algorithm as having an x, y, or z

280
00:12:01,840 --> 00:12:03,440
when, in fact, it was just optimizing

281
00:12:03,440 --> 00:12:05,200
its objective function.

282
00:12:05,200 --> 00:12:07,880
In more recent times, as the decisions

283
00:12:07,880 --> 00:12:11,840
have started to be made by people and by the CEOs

284
00:12:11,840 --> 00:12:13,600
and the pressure from their work forces,

285
00:12:13,600 --> 00:12:16,840
now, I really think there's something to worry about.

286
00:12:16,840 --> 00:12:19,200
But again, the problem there is not the algorithms.

287
00:12:19,200 --> 00:12:21,200
Actually, it's the people making the decisions.

288
00:12:21,200 --> 00:12:24,040
But to get back to the algorithms,

289
00:12:24,040 --> 00:12:26,880
it's very easy to blame the algorithms.

290
00:12:26,880 --> 00:12:29,800
But the algorithms were designed by people,

291
00:12:29,800 --> 00:12:31,760
in particular, machine learning algorithms.

292
00:12:31,760 --> 00:12:33,440
And again, the problem is not so much

293
00:12:33,440 --> 00:12:35,280
that they were designed by, say, liberals

294
00:12:35,280 --> 00:12:38,040
with a liberal takeover in mind, the silent conservancy

295
00:12:38,040 --> 00:12:38,720
of voices.

296
00:12:38,720 --> 00:12:40,080
The problem is that the algorithms

297
00:12:40,080 --> 00:12:45,400
were designed to maximize engagement, which

298
00:12:45,400 --> 00:12:48,120
maximizing engagement, these days, has a really bad name.

299
00:12:48,120 --> 00:12:50,160
That's an incentive, basically.

300
00:12:50,160 --> 00:12:54,960
Yeah, I mean, again, this is how machine learning and AI

301
00:12:54,960 --> 00:12:57,360
and many other algorithms work is there's an objective function

302
00:12:57,360 --> 00:12:58,960
and you're trying to optimize it.

303
00:12:58,960 --> 00:13:00,840
And really, where the social discussion

304
00:13:00,840 --> 00:13:02,760
and the personal intervention needs to happen

305
00:13:02,760 --> 00:13:04,680
is at the level of what is or what should

306
00:13:04,680 --> 00:13:06,320
be that objective function.

307
00:13:06,320 --> 00:13:09,080
How it gets optimized is really like a technical problem.

308
00:13:09,080 --> 00:13:11,080
It's like, how do you make the engine run faster?

309
00:13:11,080 --> 00:13:13,360
Well, let the engineers worry about that.

310
00:13:13,360 --> 00:13:16,200
And then you decide what speed you want to drive it at.

311
00:13:16,200 --> 00:13:18,960
And the thing is that maximizing engagement

312
00:13:18,960 --> 00:13:21,680
as an objective function is actually

313
00:13:21,680 --> 00:13:24,880
a perfectly natural thing to do.

314
00:13:24,880 --> 00:13:26,760
The problem is the side effects.

315
00:13:26,760 --> 00:13:28,280
Right, the consequences.

316
00:13:28,280 --> 00:13:29,000
Exactly.

317
00:13:29,000 --> 00:13:29,640
Right.

318
00:13:29,640 --> 00:13:33,280
So the algorithms are to blame, yes,

319
00:13:33,280 --> 00:13:36,480
but the engineers are to blame not for their evil intent,

320
00:13:36,480 --> 00:13:38,480
but for just having set the algorithms

321
00:13:38,480 --> 00:13:42,360
to maximize this thing without realizing all the consequences

322
00:13:42,360 --> 00:13:45,320
that we'd have, which in fairness, in the early days,

323
00:13:45,320 --> 00:13:46,880
it was easy to miss that.

324
00:13:46,880 --> 00:13:49,480
Again, I don't think people were being evil or really stupid

325
00:13:49,480 --> 00:13:50,080
about it.

326
00:13:50,080 --> 00:13:52,520
They were just oblivious, and we can't afford

327
00:13:52,520 --> 00:13:55,040
to believe we're just at this point.

328
00:13:55,040 --> 00:13:57,040
The objective that you're talking about

329
00:13:57,040 --> 00:14:00,480
is exactly the problem that we have with alignment problem.

330
00:14:00,480 --> 00:14:02,240
Right?

331
00:14:02,240 --> 00:14:04,280
Big picture-wise, because the objective

332
00:14:04,280 --> 00:14:06,040
can be narrow, that I'm hungry.

333
00:14:06,040 --> 00:14:07,680
The objective is to get to the kitchen.

334
00:14:07,680 --> 00:14:10,080
I go down the stairs, and I get plate, blah, blah, blah.

335
00:14:10,080 --> 00:14:11,800
But then there are bigger objectives,

336
00:14:11,800 --> 00:14:18,040
that this is where we get to AI ethics, which for years,

337
00:14:18,040 --> 00:14:19,960
I've asked people without even knowing

338
00:14:19,960 --> 00:14:23,440
that there is such a role exists as AI ethicists, that to me,

339
00:14:23,440 --> 00:14:24,360
that's terrifying.

340
00:14:24,360 --> 00:14:26,840
But hopefully, we can talk about it.

341
00:14:26,840 --> 00:14:30,400
But for years, I asked people on this podcast, technologists,

342
00:14:30,400 --> 00:14:33,480
philosopher, developers, whether or not

343
00:14:33,480 --> 00:14:37,840
ethics and morality is objective or subjective.

344
00:14:37,840 --> 00:14:41,720
And every single time I heard the answer, it is subjective.

345
00:14:41,720 --> 00:14:45,720
Unless we are creating a closed context, which to me

346
00:14:45,720 --> 00:14:49,920
is a disservice if you're going to explore the unknown

347
00:14:49,920 --> 00:14:53,960
and there are a bunch of people sitting up there in charge,

348
00:14:53,960 --> 00:14:58,520
I would assume they think, of to be the master algorithm.

349
00:14:58,520 --> 00:15:00,000
And they're like, we're going to parent you.

350
00:15:00,000 --> 00:15:02,080
We're going to teach you what is good, what is bad,

351
00:15:02,080 --> 00:15:03,360
what is right, what is wrong.

352
00:15:03,360 --> 00:15:05,600
I don't care who you are, what gender you are,

353
00:15:05,600 --> 00:15:08,320
what color you are, what race you are.

354
00:15:08,320 --> 00:15:11,080
You are not in the position, especially

355
00:15:11,080 --> 00:15:14,800
in the United States of America, to define these kind of values.

356
00:15:14,800 --> 00:15:18,120
That's why we say, in God, we trust.

357
00:15:18,120 --> 00:15:20,320
Well, I couldn't agree more.

358
00:15:20,320 --> 00:15:23,640
And in fact, if somebody tells you that ethics is objective,

359
00:15:23,640 --> 00:15:26,480
what you should do is run away as fast as you can.

360
00:15:26,480 --> 00:15:28,280
I'm from Iran, man.

361
00:15:28,280 --> 00:15:30,080
I'm coming from Iran.

362
00:15:30,080 --> 00:15:31,800
I was a refugee in Canada.

363
00:15:31,800 --> 00:15:33,160
I'm an immigrant here.

364
00:15:33,160 --> 00:15:35,840
I had to be human traffic out of Iran.

365
00:15:35,840 --> 00:15:40,600
And the major problem there is that some dude is sitting up

366
00:15:40,600 --> 00:15:44,600
there and is like, I'm telling you the orders of God.

367
00:15:44,600 --> 00:15:47,440
And this is how you have to live your lives, basically.

368
00:15:47,440 --> 00:15:49,480
And they have monopoly on violence, both of which

369
00:15:49,480 --> 00:15:51,680
they try to do right here in the United States.

370
00:15:51,680 --> 00:15:55,400
And I think most Americans think that what happened in Iran

371
00:15:55,400 --> 00:15:56,520
cannot happen in America.

372
00:15:56,520 --> 00:15:58,120
It totally can happen.

373
00:15:58,120 --> 00:15:58,600
Yes.

374
00:15:58,600 --> 00:16:01,760
I mean, in fact, that I think is the big worry,

375
00:16:01,760 --> 00:16:05,640
is that anybody who's lived in a totalitarian regime

376
00:16:05,640 --> 00:16:09,400
of any stripe, whether it's Iran or the Soviet Union or China,

377
00:16:09,400 --> 00:16:13,520
et cetera, et cetera, they recognize what's happening.

378
00:16:13,520 --> 00:16:15,960
I grew up in Portugal, which was a fascist country when

379
00:16:15,960 --> 00:16:17,720
I was born.

380
00:16:17,720 --> 00:16:19,680
The secret police had to file on my dad

381
00:16:19,680 --> 00:16:22,120
because he was a professor and he was pretty outspoken.

382
00:16:22,120 --> 00:16:23,680
And then there was a revolution.

383
00:16:23,680 --> 00:16:25,400
And for a couple of years there, Portugal

384
00:16:25,400 --> 00:16:27,200
wasn't expected to becoming a communist country

385
00:16:27,200 --> 00:16:29,240
because the communists were taking power.

386
00:16:29,240 --> 00:16:30,880
And I recognize everything that I

387
00:16:30,880 --> 00:16:32,240
see happening in America today.

388
00:16:32,240 --> 00:16:33,560
There's some new variations.

389
00:16:33,560 --> 00:16:37,360
But the essence, and again, it's not surprising to me

390
00:16:37,360 --> 00:16:39,680
that a lot of people who have come from countries

391
00:16:39,680 --> 00:16:42,560
or have close relatives who live through these things,

392
00:16:42,560 --> 00:16:45,280
they're the ones who are most alarmed.

393
00:16:45,280 --> 00:16:47,120
The problem is that the Americans don't actually

394
00:16:47,120 --> 00:16:48,160
have the pattern.

395
00:16:48,160 --> 00:16:50,880
Most Americans don't have the pattern recognition

396
00:16:50,880 --> 00:16:52,280
for what is going on here.

397
00:16:52,280 --> 00:16:54,640
But there is a set of people who are

398
00:16:54,640 --> 00:16:57,360
trying to impose their ethics on everybody else.

399
00:16:57,360 --> 00:16:59,000
And of course, one of the things that's

400
00:16:59,000 --> 00:17:02,760
new about today is that there are these technologies

401
00:17:02,760 --> 00:17:06,240
that while they can be great tools for democracy,

402
00:17:06,240 --> 00:17:09,640
they can also be great tools for totalitarianism.

403
00:17:09,640 --> 00:17:14,360
In fact, in some ways, a big AI is bigger than big brother.

404
00:17:14,360 --> 00:17:17,080
It's something that not even big brother could have dreamed of.

405
00:17:17,080 --> 00:17:19,160
And that's what's really alarming.

406
00:17:19,160 --> 00:17:21,200
I hear a lot of people worrying about like, oh,

407
00:17:21,200 --> 00:17:23,440
these companies, they're manipulating you.

408
00:17:23,440 --> 00:17:27,640
And I can't get excited about that because all they can do

409
00:17:27,640 --> 00:17:30,360
is try to sell me products and I refuse to buy them.

410
00:17:30,360 --> 00:17:32,760
That doesn't keep me up at night.

411
00:17:32,760 --> 00:17:36,240
But AI in the hands of the state,

412
00:17:36,240 --> 00:17:38,240
they can do a lot more than sell me products.

413
00:17:38,240 --> 00:17:40,160
So that, I think, is something we seriously

414
00:17:40,160 --> 00:17:41,440
need to worry about.

415
00:17:41,440 --> 00:17:43,760
So what are your thoughts about AI ethicists,

416
00:17:43,760 --> 00:17:47,600
especially in a certain company that I'm trying not to name,

417
00:17:47,600 --> 00:17:51,640
but I know that you took a public stance with respect

418
00:17:51,640 --> 00:17:53,200
to address this issue.

419
00:17:53,200 --> 00:17:57,920
That basically, I think Naval, Naval Ravikant,

420
00:17:57,920 --> 00:18:02,560
he tweeted a couple of days ago that scientists

421
00:18:02,560 --> 00:18:04,640
who are opposed to change something to this nature,

422
00:18:04,640 --> 00:18:07,480
they're not scientists, they're priests.

423
00:18:07,480 --> 00:18:13,520
And I see this kind of approach to make an ethical AI

424
00:18:13,520 --> 00:18:16,360
while your ethic itself is coming

425
00:18:16,360 --> 00:18:19,760
from a place that is, to me, completely distorted,

426
00:18:19,760 --> 00:18:22,640
is not based on reality, is completely ideological.

427
00:18:22,640 --> 00:18:23,960
What is the dynamic?

428
00:18:23,960 --> 00:18:26,920
And please describe that to, because I'm

429
00:18:26,920 --> 00:18:29,360
sure most people, they don't even know such a thing exists.

430
00:18:29,360 --> 00:18:32,760
But I believe it's such an important part

431
00:18:32,760 --> 00:18:35,560
of this whole thing when you are determining values

432
00:18:35,560 --> 00:18:39,040
of this machine that can basically take control

433
00:18:39,040 --> 00:18:40,400
of every aspect of our lives.

434
00:18:40,400 --> 00:18:41,280
Yeah.

435
00:18:41,280 --> 00:18:46,120
So AI ethics actually exists and has existed for many decades.

436
00:18:46,120 --> 00:18:49,000
Most people, including many, I think, current AI,

437
00:18:49,000 --> 00:18:51,480
so-called AI ethicists, don't realize

438
00:18:51,480 --> 00:18:53,840
that people have been debating the ethics of AI

439
00:18:53,840 --> 00:18:58,040
from day one, from the 1950s on.

440
00:18:58,040 --> 00:19:01,960
There are people who have made their multi-decade careers

441
00:19:01,960 --> 00:19:04,360
thinking about the ethics of AI and the alignment problem

442
00:19:04,360 --> 00:19:05,680
and things like this.

443
00:19:05,680 --> 00:19:09,000
So this is actually not a new field.

444
00:19:09,000 --> 00:19:12,680
It used to be a tiny field, also because AI itself was small.

445
00:19:12,680 --> 00:19:14,680
But what's happening in AI ethics is actually

446
00:19:14,680 --> 00:19:18,000
very different from that, very, very different.

447
00:19:18,000 --> 00:19:19,800
What's happening, so first of all,

448
00:19:19,800 --> 00:19:22,160
ethics is a very benign word.

449
00:19:22,160 --> 00:19:23,760
Who could disagree with ethics?

450
00:19:23,760 --> 00:19:26,120
And of course, AI raises ethical issues.

451
00:19:26,120 --> 00:19:27,960
But what we're seeing in AI today

452
00:19:27,960 --> 00:19:32,640
is actually people of a particular political orientation

453
00:19:32,640 --> 00:19:36,520
seeking to impose their politics on AI

454
00:19:36,520 --> 00:19:38,480
and as a result on society.

455
00:19:38,480 --> 00:19:41,000
And they're using ethics as an instrument.

456
00:19:41,000 --> 00:19:42,040
Yeah, oh, exactly.

457
00:19:42,040 --> 00:19:45,120
And I think they're completely, or some of them at least,

458
00:19:45,120 --> 00:19:46,560
are completely open about this.

459
00:19:46,560 --> 00:19:47,920
It's a tool.

460
00:19:47,920 --> 00:19:51,680
This is a tool that they're going to use to achieve their ends.

461
00:19:51,680 --> 00:19:55,440
And then it's easy to dupe people into like, oh,

462
00:19:55,440 --> 00:19:56,920
but there's these ethical concerns

463
00:19:56,920 --> 00:19:58,040
and they're addressing them.

464
00:19:58,040 --> 00:20:00,200
What could be bad about that?

465
00:20:00,200 --> 00:20:02,880
You have to actually look at what is being discussed

466
00:20:02,880 --> 00:20:04,560
and what is being proposed.

467
00:20:04,560 --> 00:20:07,400
And in particular, what is being proposed in the AI ethics

468
00:20:07,400 --> 00:20:10,400
is, for example, probably the biggest example

469
00:20:10,400 --> 00:20:12,800
is under the guise of AI fairness,

470
00:20:12,800 --> 00:20:15,400
people are actually proposing and implementing algorithms

471
00:20:15,400 --> 00:20:18,920
that force equal outcomes.

472
00:20:18,920 --> 00:20:19,400
Equity.

473
00:20:19,400 --> 00:20:23,680
By race, by gender, what is called equity.

474
00:20:23,680 --> 00:20:25,880
If I have x percent of women at the input,

475
00:20:25,880 --> 00:20:27,880
I have to have x percent at the output.

476
00:20:27,880 --> 00:20:32,760
And now, you may disagree or agree with this.

477
00:20:32,760 --> 00:20:34,840
Reasonable people agree, reasonable people disagree.

478
00:20:34,840 --> 00:20:37,760
I very much disagree, but I respect the people who agree.

479
00:20:37,760 --> 00:20:41,120
But the bottom line is, this is politics.

480
00:20:41,120 --> 00:20:42,880
This is not ethics.

481
00:20:42,880 --> 00:20:45,760
This is a political question to be discussed

482
00:20:45,760 --> 00:20:47,280
as a political question.

483
00:20:47,280 --> 00:20:48,720
But what we're having right now is

484
00:20:48,720 --> 00:20:51,520
it's being swept under the rug as if it's ethics.

485
00:20:51,520 --> 00:20:53,520
And then everybody, and this is kind of like where

486
00:20:53,520 --> 00:20:55,600
I came into this, is like, there's

487
00:20:55,600 --> 00:20:59,080
an attempt to impose this at conferences and so on.

488
00:20:59,080 --> 00:21:02,440
For example, if you now, these days at NeurIPS,

489
00:21:02,440 --> 00:21:06,200
if you have a paper that they think promotes unfairness,

490
00:21:06,200 --> 00:21:08,920
by their definition, the paper can get rejected

491
00:21:08,920 --> 00:21:13,360
because it's subject to this ethical, political review.

492
00:21:13,360 --> 00:21:15,080
And again, the people organizing this

493
00:21:15,080 --> 00:21:16,960
are not actually shy about saying

494
00:21:16,960 --> 00:21:19,000
what they want to accomplish.

495
00:21:19,000 --> 00:21:22,520
So this is, and we have to fight back against that.

496
00:21:22,520 --> 00:21:25,360
Because what's going to happen if we don't is,

497
00:21:25,360 --> 00:21:27,160
and this is in fact what is happening,

498
00:21:27,160 --> 00:21:31,960
and what I'm trying to combat is, then the society at large,

499
00:21:31,960 --> 00:21:36,160
what they hear is, oh, the AI community says

500
00:21:36,160 --> 00:21:38,040
that this is the right thing.

501
00:21:38,040 --> 00:21:40,480
This is this consensus of the community,

502
00:21:40,480 --> 00:21:43,040
and it's what science says.

503
00:21:43,040 --> 00:21:47,600
Science says is a very powerful pair of words.

504
00:21:47,600 --> 00:21:50,480
And what you do, and these people are, some of them

505
00:21:50,480 --> 00:21:52,840
at least quite practiced, is like, if you have an agenda,

506
00:21:52,840 --> 00:21:55,240
you go to the relevant scientific community,

507
00:21:55,240 --> 00:21:57,960
you silence all the people who disagree with the agenda,

508
00:21:57,960 --> 00:22:01,320
and then you say, science says that what science says

509
00:22:01,320 --> 00:22:03,000
is what you want to do.

510
00:22:03,000 --> 00:22:04,840
Yeah, exactly, the science is settled.

511
00:22:04,840 --> 00:22:06,640
Like, are you kidding me?

512
00:22:06,640 --> 00:22:07,040
Yeah.

513
00:22:07,040 --> 00:22:08,440
Exactly, yeah.

514
00:22:08,440 --> 00:22:10,560
Yeah, AI is just like, so if you've

515
00:22:10,560 --> 00:22:12,960
been observing these things for a while, in many ways,

516
00:22:12,960 --> 00:22:15,360
what is happening in AI is not a surprise.

517
00:22:15,360 --> 00:22:17,960
In fact, I started looking at this happening,

518
00:22:17,960 --> 00:22:19,720
again, several years ago.

519
00:22:19,720 --> 00:22:22,160
And unfortunately, the details vary.

520
00:22:22,160 --> 00:22:24,160
Some of them can still shock you.

521
00:22:24,160 --> 00:22:26,480
But the way things have been going is, unfortunately,

522
00:22:26,480 --> 00:22:27,480
not surprising at all.

523
00:22:27,480 --> 00:22:29,360
And people need to be alerted.

524
00:22:29,360 --> 00:22:31,040
And I'm trying to alert people to this.

525
00:22:31,040 --> 00:22:35,640
And also, to combat this in the context of the AI community,

526
00:22:35,640 --> 00:22:37,640
the problem, of course, is that this politics also

527
00:22:37,640 --> 00:22:41,000
comes with cancel culture, where there's a lot of people

528
00:22:41,000 --> 00:22:42,040
who are against this.

529
00:22:42,040 --> 00:22:44,360
There's many who don't understand and are oblivious.

530
00:22:44,360 --> 00:22:46,320
But there's a lot of people who are against.

531
00:22:46,320 --> 00:22:48,120
But they're just afraid to speak up,

532
00:22:48,120 --> 00:22:50,320
because they're afraid of the consequences, again,

533
00:22:50,320 --> 00:22:53,120
just as in a totalitarian regime.

534
00:22:53,120 --> 00:22:56,960
People, they know about the Gulag and the KGB.

535
00:22:56,960 --> 00:22:59,120
What they don't know is that most of the enforcement

536
00:22:59,120 --> 00:23:02,880
in the Soviet Union, that was just the last resort.

537
00:23:02,880 --> 00:23:05,560
It was done by exactly the kinds of things that we see today,

538
00:23:05,560 --> 00:23:08,960
is like the peer pressure, the loss of employment,

539
00:23:08,960 --> 00:23:10,760
the retaliations, et cetera, et cetera.

540
00:23:10,760 --> 00:23:13,320
The term political correctness was invented

541
00:23:13,320 --> 00:23:15,760
in the Soviet Union.

542
00:23:15,760 --> 00:23:19,320
Precisely to describe this type of thing, in some ways,

543
00:23:19,320 --> 00:23:21,320
regardless of what the exact content is,

544
00:23:21,320 --> 00:23:24,240
the content of political creedness in the US

545
00:23:24,240 --> 00:23:27,240
might be different than it was in the Soviet Union,

546
00:23:27,240 --> 00:23:29,360
because the ideologies are somewhat different.

547
00:23:29,360 --> 00:23:33,840
But the modus operandi is actually exactly the same.

548
00:23:33,840 --> 00:23:34,760
Yeah.

549
00:23:34,760 --> 00:23:38,960
The post that was shared by the actor from Mandalorian,

550
00:23:38,960 --> 00:23:42,160
that Disney fired her, and basically,

551
00:23:42,160 --> 00:23:44,600
and she was sharing someone else's post.

552
00:23:44,600 --> 00:23:45,680
I thought it was spot on.

553
00:23:45,680 --> 00:23:48,680
And what a brave woman.

554
00:23:48,680 --> 00:23:50,480
Because she basically gave up her career

555
00:23:50,480 --> 00:23:54,720
until after the revolution to see what will happen.

556
00:23:54,720 --> 00:23:57,320
And she was basically saying that the history is edited

557
00:23:57,320 --> 00:24:01,440
so you don't see the bridge that leads from peace and calm

558
00:24:01,440 --> 00:24:04,960
and everybody getting along until people getting burned

559
00:24:04,960 --> 00:24:06,520
in concentration camps.

560
00:24:06,520 --> 00:24:09,440
And she was saying that people who

561
00:24:09,440 --> 00:24:11,760
started beating up the Jews in the beginning,

562
00:24:11,760 --> 00:24:12,680
they were not Nazis.

563
00:24:12,680 --> 00:24:15,840
They were their neighbors, including children.

564
00:24:15,840 --> 00:24:17,880
And it's completely believable to me,

565
00:24:17,880 --> 00:24:22,520
because I witnessed people who have turned 180 degree

566
00:24:22,520 --> 00:24:26,800
under the theocratic regime overnight.

567
00:24:26,800 --> 00:24:30,200
She was completely open about her sexuality and all of that.

568
00:24:30,200 --> 00:24:34,800
The next day, she can't wait to go and see this shrine

569
00:24:34,800 --> 00:24:37,120
because of some social point.

570
00:24:37,120 --> 00:24:38,560
Because if she wants to get bread,

571
00:24:38,560 --> 00:24:42,200
she'll be in the front of a line.

572
00:24:42,200 --> 00:24:46,960
Yeah, again, people who've never seen this or at least

573
00:24:46,960 --> 00:24:49,800
don't know some history have a hard time

574
00:24:49,800 --> 00:24:52,400
picturing how this happens.

575
00:24:52,400 --> 00:24:55,720
But which is why I think just raising awareness

576
00:24:55,720 --> 00:24:56,520
is very important.

577
00:24:56,520 --> 00:24:58,680
So for example, people don't realize

578
00:24:58,680 --> 00:25:02,120
that if you look, for example, at the show trials,

579
00:25:02,120 --> 00:25:04,520
Stalin's trials of the Cultural Revolution,

580
00:25:04,520 --> 00:25:10,280
every generation of people who condemned one set of people

581
00:25:10,280 --> 00:25:15,240
to the gulags or whatever, they were the next ones in line.

582
00:25:15,240 --> 00:25:18,040
People are like, oh, but this will never happen to me.

583
00:25:18,040 --> 00:25:20,880
Or if I pay my obeisances, then I will be safe.

584
00:25:20,880 --> 00:25:22,600
Actually, what you're doing when you do that

585
00:25:22,600 --> 00:25:24,960
is you're accelerating the process.

586
00:25:24,960 --> 00:25:26,840
And it will get to you.

587
00:25:26,840 --> 00:25:30,000
Nobody's safe, not on any level, not in any field.

588
00:25:30,000 --> 00:25:31,800
And again, this is not a distraction.

589
00:25:31,800 --> 00:25:36,040
You already see today children turning in their parents

590
00:25:36,040 --> 00:25:38,360
for their violations of political correctness,

591
00:25:38,360 --> 00:25:41,920
which again, if you know some history, it's really chilling.

592
00:25:41,920 --> 00:25:46,200
And if somebody had told me 20 years ago,

593
00:25:46,200 --> 00:25:51,000
this is what America, the beacon of freedom,

594
00:25:51,000 --> 00:25:54,360
is going to look like in 2020, I would be like,

595
00:25:54,360 --> 00:25:55,760
come up with a better plot.

596
00:25:55,760 --> 00:25:57,760
That's not a realistic movie.

597
00:25:57,760 --> 00:25:59,880
In fact, the thing about this movie

598
00:25:59,880 --> 00:26:03,160
is that we've already seen it many times, just not

599
00:26:03,160 --> 00:26:05,320
in English.

600
00:26:05,320 --> 00:26:07,760
Yeah, that's such a good way to put it.

601
00:26:07,760 --> 00:26:09,960
And chilling.

602
00:26:09,960 --> 00:26:15,520
I mean, it's natural end that it will destroy everything

603
00:26:15,520 --> 00:26:17,800
until it destroys itself.

604
00:26:17,800 --> 00:26:19,040
It's a virus.

605
00:26:19,040 --> 00:26:20,320
It's cancer.

606
00:26:20,320 --> 00:26:23,680
And it spreads on the goodwill of people

607
00:26:23,680 --> 00:26:25,800
and cowardice of people.

608
00:26:25,800 --> 00:26:26,760
Yeah, I mean, exactly.

609
00:26:26,760 --> 00:26:29,400
In fact, I think between virus and cancer,

610
00:26:29,400 --> 00:26:32,680
I think it's more like a cancer, which means it's worse.

611
00:26:32,680 --> 00:26:36,000
Because a virus is a very simple agent.

612
00:26:36,000 --> 00:26:37,400
And it comes from the outside.

613
00:26:37,400 --> 00:26:38,920
And it does its thing.

614
00:26:38,920 --> 00:26:42,920
Cancer is when the cells in your own body go awry.

615
00:26:42,920 --> 00:26:47,000
And unfortunately, cancer is not just an uncontrolled growth.

616
00:26:47,000 --> 00:26:53,080
It's actually organized enough to survive and propagate

617
00:26:53,080 --> 00:26:58,520
around it until it kills its host organism.

618
00:26:58,520 --> 00:26:59,920
That's the thing, is that at the end of the day,

619
00:26:59,920 --> 00:27:01,520
cancer does kill the host organism.

620
00:27:01,520 --> 00:27:03,880
And this is why the Cultural Revolution in the end

621
00:27:03,880 --> 00:27:06,520
stopped, because it was destroying China literally

622
00:27:06,520 --> 00:27:08,160
with famine, et cetera, et cetera.

623
00:27:08,160 --> 00:27:09,960
At some point, things become so bad.

624
00:27:09,960 --> 00:27:13,560
Or in Cambodia, or during Stalinism,

625
00:27:13,560 --> 00:27:15,360
during the 20s and 30s.

626
00:27:15,360 --> 00:27:18,760
At some point, people recognize.

627
00:27:18,760 --> 00:27:22,240
But by then, millions have died.

628
00:27:22,240 --> 00:27:24,680
And in the beginning, nobody says,

629
00:27:24,680 --> 00:27:27,960
if somebody in the beginning of all this says,

630
00:27:27,960 --> 00:27:31,080
oh, millions are going to die, they'll be like, oh, my god.

631
00:27:31,080 --> 00:27:32,320
What are you talking about?

632
00:27:32,320 --> 00:27:33,360
But that's what happens.

633
00:27:33,360 --> 00:27:36,640
And again, just like with cancer,

634
00:27:36,640 --> 00:27:39,640
early detection and prevention is the key.

635
00:27:39,640 --> 00:27:44,360
The sooner you cut out the tumor, the less damage it'll do.

636
00:27:44,360 --> 00:27:47,200
At some point, it becomes impossible to cut it out.

637
00:27:47,200 --> 00:27:48,760
And it does kill the host organism.

638
00:27:48,760 --> 00:27:51,480
So I think cancer is actually a very good analogy for this.

639
00:27:51,480 --> 00:27:53,200
Yeah, excellent.

640
00:27:53,200 --> 00:27:57,800
So socially, the way that you're suggesting to fight back

641
00:27:57,800 --> 00:28:01,320
is basically to raise awareness and to speak out.

642
00:28:01,320 --> 00:28:03,200
If you really feel like there is something

643
00:28:03,200 --> 00:28:06,440
wrong in your gut feeling, just act on it

644
00:28:06,440 --> 00:28:09,280
rather than rationalize it for yourself.

645
00:28:09,280 --> 00:28:12,080
So I think speaking out is extremely important.

646
00:28:12,080 --> 00:28:13,880
I think more people need to speak out.

647
00:28:13,880 --> 00:28:16,240
I think it's not enough.

648
00:28:16,240 --> 00:28:20,480
And in particular, I think a lot of the response

649
00:28:20,480 --> 00:28:23,840
to cancel culture so far has kind of emphasized

650
00:28:23,840 --> 00:28:27,280
personal courage, which I think is very important.

651
00:28:27,280 --> 00:28:29,200
But personal courage is not enough.

652
00:28:29,200 --> 00:28:30,880
You need to get organized.

653
00:28:30,880 --> 00:28:33,920
You cannot fight an organized movement that's

654
00:28:33,920 --> 00:28:37,240
very widespread in education and in the media,

655
00:28:37,240 --> 00:28:39,200
in parts of politics, et cetera, et cetera,

656
00:28:39,200 --> 00:28:41,720
with a disorganized set of people.

657
00:28:41,720 --> 00:28:44,160
So I think getting organized is essential.

658
00:28:44,160 --> 00:28:47,040
So I think people need to push back.

659
00:28:47,040 --> 00:28:48,880
Again, all these individual initiatives

660
00:28:48,880 --> 00:28:51,440
and small scale initiatives are extremely important,

661
00:28:51,440 --> 00:28:52,280
but they're not enough.

662
00:28:52,280 --> 00:28:54,360
You need a higher level of organization

663
00:28:54,360 --> 00:28:56,360
to combat the level of organization

664
00:28:56,360 --> 00:28:57,920
that's there on the other side.

665
00:28:57,920 --> 00:29:01,080
Is there any movement within academia?

666
00:29:01,080 --> 00:29:02,800
Because what I learned from James Lindsay

667
00:29:02,800 --> 00:29:05,880
and Peter Borussian is that this whole thing started

668
00:29:05,880 --> 00:29:08,480
in academia in the late 60s,

669
00:29:08,480 --> 00:29:11,720
and it needs to end and stop at academia

670
00:29:11,720 --> 00:29:16,080
because that's basically the spiritual fountain

671
00:29:16,080 --> 00:29:17,760
that feeds this whole thing.

672
00:29:17,760 --> 00:29:20,680
Is there any kind of a movement for all of you guys

673
00:29:20,680 --> 00:29:23,320
who are getting canceled or pressured

674
00:29:23,320 --> 00:29:26,520
on their political correctness to leave the institutions

675
00:29:26,520 --> 00:29:29,160
and create an alternative to institutions

676
00:29:29,160 --> 00:29:31,680
to make the institutions obsolete?

677
00:29:31,680 --> 00:29:34,160
So you're right, this started in the universities

678
00:29:34,160 --> 00:29:37,960
and that's the number one place where we have to stop it

679
00:29:37,960 --> 00:29:39,880
because again, the universities

680
00:29:39,880 --> 00:29:42,280
have a very high viral coefficient.

681
00:29:43,360 --> 00:29:47,400
That's the thing is that there's this famous quote

682
00:29:47,400 --> 00:29:52,400
by Lincoln that says, whatever is taught in the classroom

683
00:29:52,480 --> 00:29:56,040
today will be the politics of the government tomorrow.

684
00:29:56,040 --> 00:29:58,640
And this is exactly what's been happening.

685
00:29:58,640 --> 00:30:03,040
In particular, radicals to control of the ed schools

686
00:30:03,040 --> 00:30:06,840
at universities in the 70s, the education schools,

687
00:30:06,840 --> 00:30:08,960
and they have never let go.

688
00:30:08,960 --> 00:30:11,480
And the educational school is one of the most obscure

689
00:30:11,480 --> 00:30:13,960
and ignored parts of the university,

690
00:30:13,960 --> 00:30:15,880
but it's actually the most important one

691
00:30:15,880 --> 00:30:18,320
because it trains the teachers.

692
00:30:18,320 --> 00:30:20,520
And then the teachers go to the high schools

693
00:30:20,520 --> 00:30:21,760
and they train everybody.

694
00:30:22,840 --> 00:30:25,200
And this is actually the most perverse thing about this

695
00:30:25,200 --> 00:30:27,240
is that everybody talks about like those students

696
00:30:27,240 --> 00:30:30,560
get politicized at the universities, actually they don't.

697
00:30:30,560 --> 00:30:34,800
They get politicized when they're six years old and 10.

698
00:30:34,800 --> 00:30:37,640
I mean, I have a kid, I've seen him go through this

699
00:30:37,640 --> 00:30:42,640
and in a moderate part of, and it's like they're defenseless.

700
00:30:45,840 --> 00:30:48,000
Students have an amazing tendency to believe

701
00:30:48,000 --> 00:30:49,640
what their teachers tell them.

702
00:30:49,640 --> 00:30:51,800
And the education schools have been turned

703
00:30:51,800 --> 00:30:54,360
into full blown indoctrination machines.

704
00:30:54,360 --> 00:30:57,280
The one at UW, my university is a good example.

705
00:30:57,280 --> 00:31:00,400
They just like, what they do is they spend the whole year

706
00:31:00,400 --> 00:31:03,560
teaching you quite explicitly activist politics.

707
00:31:03,560 --> 00:31:07,240
They don't teach you how to teach, that's secondary.

708
00:31:07,240 --> 00:31:09,960
They teach you how to be a social justice warrior.

709
00:31:09,960 --> 00:31:11,920
And then these people go out into the schools

710
00:31:11,920 --> 00:31:13,360
and they indoctrinate the children.

711
00:31:13,360 --> 00:31:16,880
And then 10, 20 years later, these children are everywhere.

712
00:31:16,880 --> 00:31:18,400
They're in the media, they're in companies,

713
00:31:18,400 --> 00:31:20,800
and that's what explains what is going on.

714
00:31:20,800 --> 00:31:23,800
These ideas have not spread because they make any sense

715
00:31:23,800 --> 00:31:26,280
or because they're good for the country or for society.

716
00:31:26,280 --> 00:31:30,560
They have spread because the teachers are spreading them.

717
00:31:30,560 --> 00:31:33,120
It's not a mystery, right?

718
00:31:33,120 --> 00:31:35,080
And so you gotta stop it there.

719
00:31:35,080 --> 00:31:37,160
Teachers have become priests.

720
00:31:37,160 --> 00:31:38,520
No, exactly, right?

721
00:31:38,520 --> 00:31:40,360
And now you're asking like,

722
00:31:40,360 --> 00:31:42,560
is there a movement to do something about this?

723
00:31:42,560 --> 00:31:44,800
There's not one unified movement that I've seen.

724
00:31:44,800 --> 00:31:46,520
There's many different movements.

725
00:31:46,520 --> 00:31:51,320
There's also a lot of discussion about what to do, right?

726
00:31:51,320 --> 00:31:54,680
And a lot of people put their hope

727
00:31:54,680 --> 00:31:58,400
in fighting this from within, right?

728
00:31:58,400 --> 00:32:02,920
My opinion is actually exactly the same as yours is.

729
00:32:02,920 --> 00:32:04,640
And I say this with great sadness

730
00:32:04,640 --> 00:32:06,560
because I'm a professor, right?

731
00:32:06,560 --> 00:32:11,080
I spent 20 years as a professor in an American university.

732
00:32:11,080 --> 00:32:13,080
I think the rot goes too deep.

733
00:32:14,480 --> 00:32:16,840
And these things are too entrenched.

734
00:32:16,840 --> 00:32:19,080
The paradox about universities is that

735
00:32:19,080 --> 00:32:21,480
the institutions that promote change at the same time,

736
00:32:21,480 --> 00:32:24,480
they're the most resistant to change anywhere.

737
00:32:24,480 --> 00:32:28,080
It's harder to change a university than almost anything, right?

738
00:32:28,080 --> 00:32:31,040
And once this ideology has gotten entrenched

739
00:32:31,040 --> 00:32:33,920
and it has from the administrators to the students,

740
00:32:33,920 --> 00:32:35,320
it's almost impossible.

741
00:32:35,320 --> 00:32:37,800
I mean, like the problem that I see is that

742
00:32:37,800 --> 00:32:39,520
people increasingly in the universities,

743
00:32:39,520 --> 00:32:42,040
they live in this parallel universe.

744
00:32:42,040 --> 00:32:44,320
Because again, universities to some extent

745
00:32:44,320 --> 00:32:45,880
are detached from the real world

746
00:32:45,880 --> 00:32:48,120
because they're not concerned with everyday things.

747
00:32:48,120 --> 00:32:49,640
They're looking at the future.

748
00:32:49,640 --> 00:32:52,560
And on a good day, that's actually a great thing, right?

749
00:32:52,560 --> 00:32:55,280
On the other hand, it also creates this possibility

750
00:32:55,280 --> 00:32:58,200
for the universities to kind of like escape

751
00:32:58,200 --> 00:33:01,440
into this parallel universe, which would be bad enough.

752
00:33:01,440 --> 00:33:03,280
But then they start teaching our children

753
00:33:03,280 --> 00:33:05,440
to live in this parallel universe.

754
00:33:05,440 --> 00:33:07,520
But the problem is that like I'm pessimistic

755
00:33:07,520 --> 00:33:09,920
about changing universities from within, right?

756
00:33:09,920 --> 00:33:12,960
I think I understand and I think people

757
00:33:12,960 --> 00:33:14,200
should try that as well.

758
00:33:14,200 --> 00:33:16,120
But I think it has to come from outside

759
00:33:17,080 --> 00:33:18,880
in one of two ways, right?

760
00:33:18,880 --> 00:33:22,320
One is as much as I believe very, very strongly

761
00:33:22,320 --> 00:33:24,280
in academic freedom.

762
00:33:24,280 --> 00:33:27,240
A lot of the universities are state universities.

763
00:33:27,240 --> 00:33:28,640
And just because you're allowed

764
00:33:28,640 --> 00:33:29,880
to do whatever research you want

765
00:33:29,880 --> 00:33:31,080
doesn't mean you're allowed to teach

766
00:33:31,080 --> 00:33:32,880
whatever cockamamie ideas you want

767
00:33:32,880 --> 00:33:36,560
to the next generation of society, right?

768
00:33:36,560 --> 00:33:38,600
So society does have to have some,

769
00:33:38,600 --> 00:33:41,960
put some boundaries on what you're allowed to teach people.

770
00:33:41,960 --> 00:33:43,800
Otherwise, they can't talk these things

771
00:33:43,800 --> 00:33:44,800
and you see the results, right?

772
00:33:44,800 --> 00:33:48,240
Like freedom needs to defend itself, right?

773
00:33:48,240 --> 00:33:51,080
So we can't allow freedom to the people who would destroy it.

774
00:33:51,080 --> 00:33:53,960
That's the essential paradox,

775
00:33:53,960 --> 00:33:55,760
Popper's paradox and so on.

776
00:33:55,760 --> 00:33:58,720
So I think there's a lot that can be done there

777
00:33:58,720 --> 00:34:01,680
in the legislative sphere, right?

778
00:34:01,680 --> 00:34:03,240
I mean, maybe not in blue states,

779
00:34:03,240 --> 00:34:05,480
but at least in red states, right?

780
00:34:05,480 --> 00:34:07,920
It's like, and I understand people are,

781
00:34:07,920 --> 00:34:09,760
conservatives are fierce because it's like,

782
00:34:09,760 --> 00:34:11,440
they're indoctrinating my children

783
00:34:11,440 --> 00:34:14,160
and they're doing it with my tax money.

784
00:34:14,160 --> 00:34:16,280
Well, it's your tax dollars, right?

785
00:34:16,280 --> 00:34:17,360
It's your representatives,

786
00:34:17,360 --> 00:34:20,040
so you can't do something about this, right?

787
00:34:20,040 --> 00:34:21,880
It has to be thought out very carefully,

788
00:34:21,880 --> 00:34:24,600
but I think that's one area of intervention.

789
00:34:24,600 --> 00:34:26,280
The other area, which ultimately,

790
00:34:26,280 --> 00:34:28,480
I think is the most important one is,

791
00:34:28,480 --> 00:34:29,760
this is still a free country

792
00:34:29,760 --> 00:34:32,200
and you can start new institutions.

793
00:34:32,200 --> 00:34:34,200
And what we can do is start new universities

794
00:34:34,200 --> 00:34:37,760
that will destroy the old ones because they're better.

795
00:34:37,760 --> 00:34:39,480
Because parents still care about their children

796
00:34:39,480 --> 00:34:40,760
getting a good education

797
00:34:40,760 --> 00:34:43,160
and the employer is still caring about having,

798
00:34:43,160 --> 00:34:45,400
well-prepared employees, right?

799
00:34:45,400 --> 00:34:48,400
So we can, and unfortunately, right?

800
00:34:48,400 --> 00:34:50,880
Or maybe, ironically,

801
00:34:50,880 --> 00:34:52,120
universities at this center,

802
00:34:52,120 --> 00:34:53,840
they're actually very fragile

803
00:34:53,840 --> 00:34:55,320
for a number of other reasons.

804
00:34:55,320 --> 00:34:57,000
They're ultra expensive,

805
00:34:57,000 --> 00:34:58,600
the students often take second road

806
00:34:58,600 --> 00:35:00,440
to other concerns, et cetera, et cetera.

807
00:35:00,440 --> 00:35:02,880
Like they're not very tax savvy, right?

808
00:35:02,880 --> 00:35:05,760
The whole university system is just ripe for disruption.

809
00:35:05,760 --> 00:35:07,200
So I think above all,

810
00:35:07,200 --> 00:35:09,520
we need a new set of universities

811
00:35:09,520 --> 00:35:11,320
that will just make these ones irrelevant.

812
00:35:11,320 --> 00:35:13,480
It's a generation long project, right?

813
00:35:13,480 --> 00:35:15,240
There's not something that's gonna happen overnight,

814
00:35:15,240 --> 00:35:17,440
but I think that's what we really need to work on.

815
00:35:17,440 --> 00:35:20,920
And that's what I think our future depends on.

816
00:35:20,920 --> 00:35:23,120
So a very good example is what happened

817
00:35:23,120 --> 00:35:24,880
to Jordan Peterson a couple of years ago

818
00:35:24,880 --> 00:35:27,080
that the government of Canada,

819
00:35:27,080 --> 00:35:29,680
they cut his budget, his research budget,

820
00:35:29,680 --> 00:35:31,360
just out of nowhere.

821
00:35:31,360 --> 00:35:34,560
And then they crowdfund for his research.

822
00:35:34,560 --> 00:35:37,560
And they made like 40 times, 50 times more

823
00:35:37,560 --> 00:35:39,800
than the money that he would have gotten from the government,

824
00:35:39,800 --> 00:35:43,120
which once again prove the strength

825
00:35:43,120 --> 00:35:45,560
of this decentralized network,

826
00:35:45,560 --> 00:35:49,440
which is the same thing that happened with GameStop stock

827
00:35:49,440 --> 00:35:50,440
again, right?

828
00:35:50,440 --> 00:35:52,720
So it seems like people are waking up

829
00:35:52,720 --> 00:35:54,560
to the ways that they can organize

830
00:35:54,560 --> 00:35:58,480
against this centralized tyranny.

831
00:35:58,480 --> 00:36:01,840
But at the same time, there are a lot of people who,

832
00:36:01,840 --> 00:36:05,720
as you said, their interest is invested

833
00:36:05,720 --> 00:36:08,080
within these rotten institutions.

834
00:36:08,080 --> 00:36:11,960
So they will do whatever they can with regulations.

835
00:36:11,960 --> 00:36:13,560
And what a nightmare it will be

836
00:36:13,560 --> 00:36:15,200
when corporations and government,

837
00:36:15,200 --> 00:36:18,880
they become one in order to maintain that power

838
00:36:18,880 --> 00:36:23,040
against the everyday kind of citizen and individuals,

839
00:36:23,040 --> 00:36:24,680
which individual rights

840
00:36:24,680 --> 00:36:27,240
is the whole purpose of United States.

841
00:36:27,240 --> 00:36:30,000
Yeah, I mean, but again, you have to remember

842
00:36:30,000 --> 00:36:31,880
that no society is immune

843
00:36:31,880 --> 00:36:34,840
to the problem of collective action, right?

844
00:36:34,840 --> 00:36:36,200
And the problem of collective action

845
00:36:36,200 --> 00:36:39,600
is that the right thing does not necessarily prevail

846
00:36:39,600 --> 00:36:42,160
because you need to get organized to make it prevail.

847
00:36:42,160 --> 00:36:44,120
And organizing has a cost.

848
00:36:44,120 --> 00:36:46,520
In a way, the beauty of information technology

849
00:36:46,520 --> 00:36:47,640
and the internet and so on

850
00:36:47,640 --> 00:36:50,640
is that it actually lowers the cost of organizing, right?

851
00:36:50,640 --> 00:36:52,120
Which raises the potential to do things

852
00:36:52,120 --> 00:36:53,160
which might be good or bad.

853
00:36:53,160 --> 00:36:54,920
But for example, you can do good things

854
00:36:54,920 --> 00:36:56,680
like crowdfund something that otherwise

855
00:36:56,680 --> 00:36:58,120
might not get funded.

856
00:36:58,120 --> 00:36:59,880
But of course, the other side of this coin

857
00:36:59,880 --> 00:37:02,280
of collective action is that there is

858
00:37:02,280 --> 00:37:04,040
a very large number of people

859
00:37:04,040 --> 00:37:08,360
who are completely invested in the status quo.

860
00:37:08,360 --> 00:37:11,560
There's a whole bureaucracy at universities,

861
00:37:11,560 --> 00:37:12,960
and it keeps ever increasing.

862
00:37:12,960 --> 00:37:15,360
And that's part of what makes it expensive, right?

863
00:37:15,360 --> 00:37:16,960
There's people who would lose their jobs

864
00:37:16,960 --> 00:37:18,000
if you said like, oh no,

865
00:37:18,000 --> 00:37:20,320
we don't need all these things anymore, right?

866
00:37:20,320 --> 00:37:23,040
So they will fight tooth and nail to combat this.

867
00:37:23,040 --> 00:37:24,680
Some of them out of ideology,

868
00:37:24,680 --> 00:37:27,080
some of just, you know, out of their self-interest

869
00:37:27,080 --> 00:37:28,800
or some combination of the two.

870
00:37:28,800 --> 00:37:31,640
So of course you can expect a lot of pushback, right?

871
00:37:31,640 --> 00:37:35,960
But the thing is, where is the, you know,

872
00:37:35,960 --> 00:37:37,320
what is the choke point?

873
00:37:37,320 --> 00:37:40,200
The choke point is that universities need to get funded.

874
00:37:40,200 --> 00:37:41,920
That's, you know, that's the thing, right?

875
00:37:41,920 --> 00:37:45,640
So the way you hit that choke point

876
00:37:45,640 --> 00:37:48,920
is either the states that own the universities, right?

877
00:37:48,920 --> 00:37:52,440
Say, well, actually, no, here's what I'm gonna do, right?

878
00:37:52,440 --> 00:37:53,800
That will make you more accountable.

879
00:37:53,800 --> 00:37:55,640
And you know, we can talk a little bit about that.

880
00:37:55,640 --> 00:37:56,720
That's one part.

881
00:37:56,720 --> 00:38:01,240
The other part is you get competing universities, right?

882
00:38:01,240 --> 00:38:04,240
They actually give a better education for less money.

883
00:38:04,240 --> 00:38:05,080
Yeah.

884
00:38:05,080 --> 00:38:07,560
And all the people subsidizing all this, you know,

885
00:38:07,560 --> 00:38:09,400
bad stuff, you know, this stuff.

886
00:38:09,400 --> 00:38:12,040
You know, the money gets, you know, goes away.

887
00:38:12,040 --> 00:38:13,800
And so this stuff dies out

888
00:38:13,800 --> 00:38:18,120
because it just doesn't have the funding sources anymore.

889
00:38:18,120 --> 00:38:21,840
Yeah, constitutional sanctuary states

890
00:38:21,840 --> 00:38:24,880
that will uphold constitutional values of United States,

891
00:38:24,880 --> 00:38:26,800
which is a great document, I think,

892
00:38:26,800 --> 00:38:28,160
especially for this time,

893
00:38:28,160 --> 00:38:30,240
especially when the other end of the spectrum

894
00:38:30,240 --> 00:38:33,760
is centralized Chinese Communist Party,

895
00:38:33,760 --> 00:38:38,320
which it has its own advantages,

896
00:38:38,320 --> 00:38:42,280
but at the same time, you know, we see that what will happen

897
00:38:42,280 --> 00:38:45,080
if you allow small, relatively speaking,

898
00:38:45,080 --> 00:38:48,200
group of people to be in charge of everything.

899
00:38:48,200 --> 00:38:50,920
And maybe ICOing a university, you know,

900
00:38:50,920 --> 00:38:53,720
just raise millions and millions of dollars in Bitcoin

901
00:38:53,720 --> 00:38:54,960
or Ethereum or something,

902
00:38:54,960 --> 00:38:57,280
and then dedicate that kind of money to researchers

903
00:38:57,280 --> 00:39:01,360
that are getting rejected for PC reasons.

904
00:39:02,400 --> 00:39:06,800
I mean, I actually think fundamentally,

905
00:39:06,800 --> 00:39:09,200
there is actually not a lack of money

906
00:39:09,200 --> 00:39:11,840
to do something like this, right?

907
00:39:11,840 --> 00:39:14,480
Again, it's a question of organization, right?

908
00:39:14,480 --> 00:39:16,120
Political will, in a sense.

909
00:39:16,120 --> 00:39:20,880
Yeah, and, you know, people need to, you know,

910
00:39:20,880 --> 00:39:23,080
I mean, for example, right?

911
00:39:23,080 --> 00:39:25,800
How did a lot of universities that existed, they start?

912
00:39:25,800 --> 00:39:27,680
They started with an endowment,

913
00:39:27,680 --> 00:39:30,000
because if somebody said, like, I want to endow a university,

914
00:39:30,000 --> 00:39:32,240
I think there's a lot of people with a lot of money

915
00:39:32,240 --> 00:39:34,240
who would be, you know, willing to do that

916
00:39:34,240 --> 00:39:36,840
if they believed in what, you know, was happening.

917
00:39:36,840 --> 00:39:38,800
There's also a lot of people who are willing to fund this

918
00:39:38,800 --> 00:39:41,040
with many smaller-scale donations,

919
00:39:41,040 --> 00:39:43,600
which, if it's a lot of people, makes a big difference.

920
00:39:43,600 --> 00:39:45,120
But most of all, at the end of the day, right,

921
00:39:45,120 --> 00:39:46,760
I get back to the students and the parents

922
00:39:46,760 --> 00:39:48,320
and the employers, right?

923
00:39:48,320 --> 00:39:49,880
You know, like, you know,

924
00:39:49,880 --> 00:39:52,320
people will sacrifice the shirt of their dads

925
00:39:52,320 --> 00:39:54,360
to give their children a good education.

926
00:39:55,480 --> 00:39:57,760
So if I, you know, if I give their children

927
00:39:57,760 --> 00:40:00,320
a better education than University X,

928
00:40:00,320 --> 00:40:03,400
and they're, you know, sufficiently informed about that,

929
00:40:03,400 --> 00:40:06,680
you know, they will come to me as opposed to University X.

930
00:40:06,680 --> 00:40:08,160
So I think, at the end of the day,

931
00:40:08,160 --> 00:40:09,640
that is the biggest part of this.

932
00:40:09,640 --> 00:40:12,240
You can use the other things, whether it's philanthropy

933
00:40:12,240 --> 00:40:15,840
or large-scale, you know, small-size donations

934
00:40:15,840 --> 00:40:17,920
to get this off the ground.

935
00:40:17,920 --> 00:40:20,760
But at the end of the day, I think what we're gonna have is,

936
00:40:20,760 --> 00:40:22,880
you know, universities that actually,

937
00:40:22,880 --> 00:40:25,320
first of all, they're well-run.

938
00:40:25,320 --> 00:40:27,360
They're not bloated and inefficient.

939
00:40:27,360 --> 00:40:28,480
And there's many different ways

940
00:40:28,480 --> 00:40:30,880
in which universities are inefficient today.

941
00:40:30,880 --> 00:40:33,640
And also, you know, there's this thing that like,

942
00:40:33,640 --> 00:40:37,560
you know, I get a degree, you know, getting the, you know,

943
00:40:37,560 --> 00:40:41,280
education is a labor-intensive industry, right?

944
00:40:41,280 --> 00:40:42,800
And that's part of why it's expensive, right?

945
00:40:42,800 --> 00:40:46,880
So there's no miracle at that level, but it's worth it.

946
00:40:46,880 --> 00:40:51,640
So when I get a degree, I actually owe a big debt, right?

947
00:40:51,640 --> 00:40:53,320
Now, whether the debt is owed in money

948
00:40:53,320 --> 00:40:54,680
because I took out a student loan

949
00:40:54,680 --> 00:40:56,840
or whether the debt is owed to society

950
00:40:56,840 --> 00:40:59,880
because I was funded with that money, I owe that debt.

951
00:40:59,880 --> 00:41:02,080
And people need to understand that.

952
00:41:02,080 --> 00:41:03,840
And, you know, once I graduate, you know,

953
00:41:03,840 --> 00:41:06,520
I go out into the world and I repay that debt.

954
00:41:06,520 --> 00:41:08,160
Again, whether in money, you know,

955
00:41:08,160 --> 00:41:10,560
because I pay back my loans or to society

956
00:41:10,560 --> 00:41:13,400
because I do my good, people have to have that conscience

957
00:41:13,400 --> 00:41:17,720
that, you know, society's gonna invest in me

958
00:41:17,720 --> 00:41:19,800
and then I'm gonna pay that back, right?

959
00:41:19,800 --> 00:41:21,640
And so at the end of the day, right, like, you know,

960
00:41:21,640 --> 00:41:23,960
a lot of people get a degree and it costs a lot of money,

961
00:41:23,960 --> 00:41:26,560
but they know they're gonna make more than that money

962
00:41:26,560 --> 00:41:28,520
when they do their jobs, right?

963
00:41:28,520 --> 00:41:31,680
And in fact, part of the problem of how universities

964
00:41:31,680 --> 00:41:35,440
are structured today is that for at least originally

965
00:41:35,440 --> 00:41:38,680
one intention reasons, there is not a good alignment

966
00:41:38,680 --> 00:41:41,160
between what people major in

967
00:41:41,160 --> 00:41:43,040
and what they study within the major

968
00:41:43,040 --> 00:41:46,520
and how socially useful it is, right?

969
00:41:46,520 --> 00:41:49,080
And again, it's not like, you know, for example,

970
00:41:49,080 --> 00:41:52,000
well, engineering, obviously, socially useful and so on,

971
00:41:52,000 --> 00:41:54,600
but sociology and psychology and the humanities

972
00:41:54,600 --> 00:41:57,440
and the arts, they're all socially useful

973
00:41:57,440 --> 00:41:59,560
if you do them properly, the problem is that

974
00:41:59,560 --> 00:42:01,680
they've kind of like fallen off this cliff

975
00:42:01,680 --> 00:42:04,360
where they, you know, doing things that are increasingly

976
00:42:04,360 --> 00:42:07,000
either irrelevant or positively harmful, right?

977
00:42:07,000 --> 00:42:11,000
So, you know, you need to have that feedback mechanism

978
00:42:11,000 --> 00:42:13,560
that says, look, you know, this is not, you know,

979
00:42:13,560 --> 00:42:16,400
what people need to be learning, right?

980
00:42:16,400 --> 00:42:21,320
And also like, you know, like markets do this very well,

981
00:42:21,320 --> 00:42:23,440
right, that's what they do, but, you know,

982
00:42:23,440 --> 00:42:25,600
unfortunately, education in that regard

983
00:42:25,600 --> 00:42:28,720
does not act like a good market is matching the needs

984
00:42:28,720 --> 00:42:30,960
to the supply, right?

985
00:42:30,960 --> 00:42:33,760
And it's like, it's nice to say like, oh,

986
00:42:33,760 --> 00:42:36,760
just major in whatever your heart decides.

987
00:42:37,600 --> 00:42:39,680
Well, okay, I'm going to major in something

988
00:42:39,680 --> 00:42:42,400
that I won't get a job in and then I'll be a waiter

989
00:42:42,400 --> 00:42:46,880
and then I'll be very bitter about how I was fooled, right?

990
00:42:46,880 --> 00:42:50,480
It's like, you know, what you major in

991
00:42:50,480 --> 00:42:53,680
should be a combination of yes, something you love,

992
00:42:53,680 --> 00:42:55,640
absolutely, and you think you can be good at,

993
00:42:55,640 --> 00:42:59,240
but something that, you know, society has a need for, right?

994
00:42:59,240 --> 00:43:02,720
And part of the paradox of today's economy

995
00:43:02,720 --> 00:43:07,720
as technology changes is that there's a shortage of jobs

996
00:43:07,800 --> 00:43:10,680
in some areas and there's a huge shortage

997
00:43:10,680 --> 00:43:14,560
of qualified people in others, right?

998
00:43:14,560 --> 00:43:18,120
And so, gee, you know, like, let's focus on the areas

999
00:43:18,120 --> 00:43:19,720
where we need more people and not the ones

1000
00:43:19,720 --> 00:43:21,560
where we need less, right?

1001
00:43:21,560 --> 00:43:24,760
This is not rocket science, but at the universities,

1002
00:43:24,760 --> 00:43:27,840
saying what I just said is extremely controversial.

1003
00:43:27,840 --> 00:43:29,360
Because people will say like, oh,

1004
00:43:29,360 --> 00:43:32,560
you're debasing the university, you're commercializing it,

1005
00:43:32,560 --> 00:43:35,160
you know, nothing is more important than the beauty

1006
00:43:35,160 --> 00:43:37,800
and power of a liberal arts education.

1007
00:43:37,800 --> 00:43:39,640
And I'm like, I understand all of that.

1008
00:43:39,640 --> 00:43:40,960
I love the liberal arts, you know,

1009
00:43:40,960 --> 00:43:42,560
more than most computer scientists.

1010
00:43:42,560 --> 00:43:44,200
I read a lot of books and so on and so forth,

1011
00:43:44,200 --> 00:43:47,160
but like, you're telling people to go major in this

1012
00:43:47,160 --> 00:43:48,800
and then they can't get a job.

1013
00:43:48,800 --> 00:43:51,200
Why is that a good idea?

1014
00:43:51,200 --> 00:43:52,040
Right?

1015
00:43:52,040 --> 00:43:53,560
You have to make these things useful enough

1016
00:43:53,560 --> 00:43:55,600
that then the jobs will be there, right?

1017
00:43:56,440 --> 00:44:00,000
And unfortunately, when these majors become useless,

1018
00:44:00,000 --> 00:44:02,960
the employers sooner or later realize that they are useless.

1019
00:44:02,960 --> 00:44:03,800
Right?

1020
00:44:03,800 --> 00:44:05,880
And so it's not so much that the humanities are dying

1021
00:44:05,880 --> 00:44:08,680
is that they are being killed from inside.

1022
00:44:08,680 --> 00:44:09,520
Right?

1023
00:44:09,520 --> 00:44:11,560
Is that there's actually a lot of uses for these things.

1024
00:44:11,560 --> 00:44:12,400
Right?

1025
00:44:12,400 --> 00:44:15,160
Again, a tech company, even a tech company,

1026
00:44:15,160 --> 00:44:18,560
should not just be run by technologists, heck no, right?

1027
00:44:18,560 --> 00:44:22,280
We really need, you know, sociologists and psychologists

1028
00:44:22,280 --> 00:44:24,960
and people who know communications and, you know,

1029
00:44:24,960 --> 00:44:26,960
et cetera, et cetera, right?

1030
00:44:26,960 --> 00:44:30,480
But they need to have been trained, right?

1031
00:44:30,480 --> 00:44:32,520
Such number one, they know what to do.

1032
00:44:32,520 --> 00:44:34,240
And number two, they know how to interface

1033
00:44:34,240 --> 00:44:36,240
with the technologists.

1034
00:44:36,240 --> 00:44:37,960
And that's what we don't have enough of today.

1035
00:44:37,960 --> 00:44:40,720
So part of what I think a new generation of universities

1036
00:44:40,720 --> 00:44:44,560
will do is, you know, it can start, I think, in the more,

1037
00:44:44,560 --> 00:44:46,680
it's obvious to start something like that

1038
00:44:46,680 --> 00:44:48,840
with things like computer science and so on,

1039
00:44:48,840 --> 00:44:50,800
where the need is dire.

1040
00:44:50,800 --> 00:44:53,880
But eventually, I think a lot of the role will be to,

1041
00:44:53,880 --> 00:44:56,400
you know, to have a better teaching of social science

1042
00:44:56,400 --> 00:44:58,880
and humanities and arts and education, right?

1043
00:44:58,880 --> 00:45:01,120
I think one of the most important schools

1044
00:45:01,120 --> 00:45:03,280
that these universities will be the education school

1045
00:45:03,280 --> 00:45:05,280
for the reasons that we've been talking about.

1046
00:45:06,440 --> 00:45:09,680
Let me ask you, I don't want us to run out of time.

1047
00:45:09,680 --> 00:45:12,120
What is your sharp exit?

1048
00:45:12,120 --> 00:45:14,200
What's the latest I can have you?

1049
00:45:14,200 --> 00:45:15,360
You mean in terms of time?

1050
00:45:15,360 --> 00:45:16,200
Yeah.

1051
00:45:16,200 --> 00:45:18,520
Oh, no, no particular time.

1052
00:45:18,520 --> 00:45:19,360
Oh, okay.

1053
00:45:21,400 --> 00:45:24,000
This described the social measures

1054
00:45:24,000 --> 00:45:26,240
to be taken against this cancer.

1055
00:45:27,080 --> 00:45:29,280
Wonderful advice and ideas there.

1056
00:45:29,280 --> 00:45:32,240
At the same time, let's also talk about a solution

1057
00:45:32,240 --> 00:45:34,320
for artificial intelligence,

1058
00:45:34,320 --> 00:45:38,480
which to me is not yet another centrally driven company

1059
00:45:38,480 --> 00:45:41,680
or corporation, it's a decentralized network.

1060
00:45:41,680 --> 00:45:44,280
And the only example I can think of at this point

1061
00:45:44,280 --> 00:45:47,600
is Singularity Net, which Ben Gortzel is behind it.

1062
00:45:47,600 --> 00:45:49,280
I'm sure you know Ben Gortzel.

1063
00:45:49,280 --> 00:45:50,120
Yeah.

1064
00:45:50,120 --> 00:45:54,400
So his whole thing is to allow millions or hundreds

1065
00:45:54,400 --> 00:45:57,720
of millions of AI agents to ultimately give rise

1066
00:45:57,720 --> 00:46:00,080
to the decentralized AGI.

1067
00:46:00,080 --> 00:46:02,800
So that superior intelligence wouldn't be controlled

1068
00:46:02,800 --> 00:46:05,880
by Chinese Communist Party or Google

1069
00:46:05,880 --> 00:46:08,680
or US government or anyone else.

1070
00:46:08,680 --> 00:46:12,240
Yeah, again, one of the things that I, in the Master Alchem,

1071
00:46:12,240 --> 00:46:15,840
there's a section about what I call a society of models.

1072
00:46:16,800 --> 00:46:20,840
And again, a lot of that has already happened.

1073
00:46:20,840 --> 00:46:23,720
It hasn't, of course, it wasn't gonna happen in five years,

1074
00:46:23,720 --> 00:46:26,400
but you can really go see a lot of it.

1075
00:46:26,400 --> 00:46:28,560
And the idea of the society of models is that

1076
00:46:28,560 --> 00:46:32,560
every one of us has a coterie of agents

1077
00:46:32,560 --> 00:46:36,280
that are artificial, that are AIs,

1078
00:46:36,280 --> 00:46:39,600
and that have models of you, right?

1079
00:46:39,600 --> 00:46:42,800
And what they do is they do stuff on your behalf.

1080
00:46:43,760 --> 00:46:48,760
So what we have is this kind of like cyberspace underground

1081
00:46:48,840 --> 00:46:51,240
where all of these transactions are happening.

1082
00:46:51,240 --> 00:46:53,480
You're like, to take a concrete example

1083
00:46:53,480 --> 00:46:58,480
that I use in the book, if I want to find a job, right?

1084
00:46:58,480 --> 00:47:00,880
What I should be able to do is like, on LinkedIn,

1085
00:47:00,880 --> 00:47:03,520
I just press the button, find me a job, right?

1086
00:47:03,520 --> 00:47:06,600
And then my model goes and talks to the models

1087
00:47:06,600 --> 00:47:08,800
of all the places that might give me a job,

1088
00:47:08,800 --> 00:47:12,120
and they interact in possibly very rich ways,

1089
00:47:12,120 --> 00:47:14,360
the better my model is and the better my model is, right?

1090
00:47:14,360 --> 00:47:17,920
And then out of those bazillions of possible combinations,

1091
00:47:17,920 --> 00:47:21,720
the few, you know, the 10 or 20 that look best

1092
00:47:21,720 --> 00:47:24,680
are the ones where I will actually interview at the place.

1093
00:47:25,840 --> 00:47:26,680
Right?

1094
00:47:26,680 --> 00:47:28,200
And notice, this is both ends.

1095
00:47:28,200 --> 00:47:31,840
It's not one big AI, it's AIs, right?

1096
00:47:31,840 --> 00:47:33,720
They're gonna negotiate with each other.

1097
00:47:33,720 --> 00:47:34,680
Same thing for buying a car.

1098
00:47:34,680 --> 00:47:36,600
If I want to buy a car, my AI is gonna go out

1099
00:47:36,600 --> 00:47:38,040
and look for the cars that I want.

1100
00:47:38,040 --> 00:47:40,320
Same thing for dates, right?

1101
00:47:40,320 --> 00:47:42,680
Dating is actually one of the most significant,

1102
00:47:42,680 --> 00:47:45,240
least appreciated applications of AI today

1103
00:47:45,240 --> 00:47:48,600
because, you know, people get matched by algorithms.

1104
00:47:48,600 --> 00:47:49,880
There are children in life today

1105
00:47:49,880 --> 00:47:51,480
that wouldn't have been born

1106
00:47:51,480 --> 00:47:53,440
if the algorithm hadn't said to their parents,

1107
00:47:53,440 --> 00:47:54,920
you two guys should go on a date.

1108
00:47:54,920 --> 00:47:56,920
That's how I met my girlfriend.

1109
00:47:56,920 --> 00:48:00,080
We've been together more than six years, and it's funny.

1110
00:48:00,080 --> 00:48:01,920
It was like meant to be, I guess,

1111
00:48:01,920 --> 00:48:04,160
because our distance on Tinder,

1112
00:48:04,160 --> 00:48:06,320
both of us were set to one mile

1113
00:48:06,320 --> 00:48:07,880
and didn't want to go any further,

1114
00:48:07,880 --> 00:48:10,280
and that was it, and more than six years.

1115
00:48:11,120 --> 00:48:11,960
There you go, right?

1116
00:48:11,960 --> 00:48:13,920
So, you know, speak of the devil.

1117
00:48:13,920 --> 00:48:17,680
Uh, but, you know, but the point is, right,

1118
00:48:17,680 --> 00:48:21,760
like the way matching is done today is very primitive.

1119
00:48:21,760 --> 00:48:23,760
It's like based on your profile or, you know,

1120
00:48:23,760 --> 00:48:25,600
what people, you know, something like Tinder,

1121
00:48:25,600 --> 00:48:29,000
you have to swipe so you have to look at a lot of alternatives.

1122
00:48:29,000 --> 00:48:30,800
Right? The beauty of having AI

1123
00:48:30,800 --> 00:48:33,680
is that it magnifies your intelligence, right?

1124
00:48:33,680 --> 00:48:36,000
And what I envisage, in fact, you know,

1125
00:48:36,000 --> 00:48:37,240
I even gave an interview to, like,

1126
00:48:37,240 --> 00:48:40,400
Marie Claire about this and the future of dating, right?

1127
00:48:40,400 --> 00:48:41,840
You wouldn't think about, like, you know,

1128
00:48:41,840 --> 00:48:44,200
an AI expert giving an interview to Marie Claire,

1129
00:48:44,200 --> 00:48:46,280
but, you know, they're actually on top of this,

1130
00:48:46,280 --> 00:48:48,040
and what the different, you know, dating,

1131
00:48:48,040 --> 00:48:51,360
you know, sites do and whatnot is unique.

1132
00:48:51,360 --> 00:48:54,080
I think, ultimately, the only good way to do this

1133
00:48:54,080 --> 00:48:59,360
is to have, you know, a model of, you know, Alice

1134
00:48:59,360 --> 00:49:04,360
and a model of Bob go on a date in cyberspace, right?

1135
00:49:04,360 --> 00:49:05,880
And so what happens is that, like, you know,

1136
00:49:05,880 --> 00:49:09,920
like your model goes on a million dates

1137
00:49:09,920 --> 00:49:11,480
or goes on a date with a million people,

1138
00:49:11,480 --> 00:49:12,720
a thousand dates each, right?

1139
00:49:12,720 --> 00:49:15,360
You do this big Monte Carlo simulation.

1140
00:49:15,360 --> 00:49:16,480
And then the one, you know,

1141
00:49:16,480 --> 00:49:18,120
because, like, some dates just go well,

1142
00:49:18,120 --> 00:49:20,480
even though maybe those people would have been a good match,

1143
00:49:20,480 --> 00:49:22,760
and some of them, you know, actually go well,

1144
00:49:22,760 --> 00:49:25,760
and then people think, like, oh, this was meant to be, right?

1145
00:49:25,760 --> 00:49:26,880
So what you need to do is, like,

1146
00:49:26,880 --> 00:49:28,480
you just need to simulate a lot of dates

1147
00:49:28,480 --> 00:49:31,320
between these agents, and then the, you know,

1148
00:49:31,320 --> 00:49:33,680
the pairs of people that have the highest fraction

1149
00:49:33,680 --> 00:49:35,480
of percentage of successful dates,

1150
00:49:35,480 --> 00:49:37,080
those are the ones who maybe, you know,

1151
00:49:37,080 --> 00:49:38,680
should be suggested to go on a real date.

1152
00:49:38,680 --> 00:49:40,480
So I very much agree with this idea

1153
00:49:40,480 --> 00:49:42,880
that what you want to have is not one, you know,

1154
00:49:42,880 --> 00:49:47,480
big Skynet centralized AI, but everybody has their AIs.

1155
00:49:47,480 --> 00:49:51,680
You know, AI is just an extension of your intelligence.

1156
00:49:51,680 --> 00:49:54,880
The way to think of AI is not like AI is my adversary

1157
00:49:54,880 --> 00:49:56,880
that's going to enslave me, right?

1158
00:49:56,880 --> 00:49:59,280
It's that, like, AI makes me more powerful.

1159
00:49:59,280 --> 00:50:01,480
AI is just an exoskeleton for your mind.

1160
00:50:01,480 --> 00:50:03,480
It's like power steering makes it,

1161
00:50:03,480 --> 00:50:05,080
instead of, you know, moving some wheels,

1162
00:50:05,080 --> 00:50:07,680
you're actually doing cognitive stuff.

1163
00:50:07,680 --> 00:50:11,280
Yeah, David Chalmer talked about extended mind theory

1164
00:50:11,280 --> 00:50:13,680
that even if, you know, you write down your shopping list

1165
00:50:13,680 --> 00:50:16,880
on a piece of paper, that's extension of your mind.

1166
00:50:16,880 --> 00:50:18,480
Yeah, exactly, and, you know, people,

1167
00:50:18,480 --> 00:50:22,680
some people call this like, you know, the exocortex.

1168
00:50:22,680 --> 00:50:25,480
It's the part of your brain that resides outside your brain.

1169
00:50:25,480 --> 00:50:28,480
And of course, a notepad is exocortex.

1170
00:50:28,480 --> 00:50:30,080
A hard disk is exocortex.

1171
00:50:30,080 --> 00:50:31,680
And these days, if you think about it,

1172
00:50:31,680 --> 00:50:34,480
you have pieces of your exocortex all over the place

1173
00:50:34,480 --> 00:50:37,080
in data centers in Oregon, right?

1174
00:50:37,080 --> 00:50:38,280
It's like you don't even know

1175
00:50:38,280 --> 00:50:40,080
where most of your exocortex is.

1176
00:50:40,080 --> 00:50:43,080
And this will only pick up speed, right?

1177
00:50:43,080 --> 00:50:44,680
In fact, what makes us smart

1178
00:50:44,680 --> 00:50:46,680
is that we have this external intelligence, right?

1179
00:50:46,680 --> 00:50:49,280
In some ways, we have outsourced a lot of our memory

1180
00:50:49,280 --> 00:50:50,480
to Google.

1181
00:50:50,480 --> 00:50:52,280
You don't need to remember those things anymore.

1182
00:50:52,280 --> 00:50:53,680
And, you know, there's a lot of confusion.

1183
00:50:53,680 --> 00:50:55,080
You know, there's like this, you know,

1184
00:50:55,080 --> 00:50:57,480
this is like, oh, the web makes us stupid.

1185
00:50:57,480 --> 00:50:59,880
Google makes us stupid because now I don't know,

1186
00:50:59,880 --> 00:51:02,480
you know, how to, you know, remember ABA anymore.

1187
00:51:02,480 --> 00:51:06,080
No, the point is like the system of you and Google

1188
00:51:06,080 --> 00:51:08,280
is actually smarter than just you alone.

1189
00:51:08,280 --> 00:51:10,680
There's this notion that, you know,

1190
00:51:10,680 --> 00:51:13,680
Richard Dawkins proposed many years ago

1191
00:51:13,680 --> 00:51:16,280
called the extended phenotype.

1192
00:51:16,280 --> 00:51:18,080
In fact, he has a book called The Extended Phenotypes,

1193
00:51:18,080 --> 00:51:19,480
which is actually one of the most obscure,

1194
00:51:19,480 --> 00:51:21,080
but actually he says it's his favorite.

1195
00:51:21,080 --> 00:51:22,480
I've never even heard of that.

1196
00:51:22,480 --> 00:51:23,880
Yeah, I highly recommend it

1197
00:51:23,880 --> 00:51:25,280
because it's a very insightful book

1198
00:51:25,280 --> 00:51:27,080
and it has never been more relevant.

1199
00:51:27,080 --> 00:51:29,080
What is the extended phenotype?

1200
00:51:29,080 --> 00:51:31,880
His point is that your genes control your body,

1201
00:51:31,880 --> 00:51:33,680
but not only.

1202
00:51:33,680 --> 00:51:37,880
The dam is the extended phenotype of the beaver, right?

1203
00:51:37,880 --> 00:51:41,080
The spider's web is the extended phenotype of the spider, right?

1204
00:51:41,080 --> 00:51:44,680
The genes make that web via the spider, right?

1205
00:51:44,680 --> 00:51:49,680
And technology is the extended phenotype of humanity, right?

1206
00:51:49,680 --> 00:51:52,280
It's like the phenotype is the body that the genotype creates, right?

1207
00:51:52,280 --> 00:51:55,480
Like, you know, technology is our extended phenotype.

1208
00:51:55,480 --> 00:51:58,480
And AI is just another computers in general, right?

1209
00:51:58,480 --> 00:52:01,280
They're just another aspect of their extended phenotype.

1210
00:52:01,280 --> 00:52:03,680
So, you know, our genes, you know, deep in our cells,

1211
00:52:03,680 --> 00:52:06,280
they create us and then we create this technology

1212
00:52:06,280 --> 00:52:08,080
and then, you know, together we're more powerful

1213
00:52:08,080 --> 00:52:09,080
than we would be without it.

1214
00:52:09,080 --> 00:52:11,080
Just like, you know, the spider needs, you know,

1215
00:52:11,080 --> 00:52:15,280
can catch, you know, insects with the web that it wouldn't otherwise.

1216
00:52:15,280 --> 00:52:21,080
So, the way I think of it is very 2001 Space Odyssey

1217
00:52:21,080 --> 00:52:23,680
that the monkey finds a bone

1218
00:52:23,680 --> 00:52:26,880
and it realizes that it can kill to eat.

1219
00:52:26,880 --> 00:52:31,080
But the next thing that it does is protect its group of monkey

1220
00:52:31,080 --> 00:52:34,480
against the other group of monkey

1221
00:52:34,480 --> 00:52:36,680
for the water resource that they had.

1222
00:52:36,680 --> 00:52:40,880
And they threw up the bone and turned into the space shuttle

1223
00:52:40,880 --> 00:52:43,080
and so on and so forth.

1224
00:52:43,080 --> 00:52:46,280
I think that we grew and evolved together.

1225
00:52:46,280 --> 00:52:51,680
And how we achieve the technology primarily,

1226
00:52:51,680 --> 00:52:52,680
I have no idea.

1227
00:52:52,680 --> 00:52:53,680
Was it given to us?

1228
00:52:53,680 --> 00:52:56,080
Was it an accident?

1229
00:52:56,080 --> 00:52:57,880
Was it part of the simulation?

1230
00:52:57,880 --> 00:52:58,680
I have no idea.

1231
00:52:58,680 --> 00:53:02,280
But I do know that we have evolved because of each other,

1232
00:53:02,280 --> 00:53:06,280
which is a context that if you think of it,

1233
00:53:06,280 --> 00:53:10,280
then war and peace are become two sides of the same coin.

1234
00:53:10,280 --> 00:53:12,280
That there will be no peace without war.

1235
00:53:12,280 --> 00:53:15,280
That wars have terrible consequences.

1236
00:53:15,280 --> 00:53:19,280
At the same time, a lot of the technologies that we are using on a daily basis,

1237
00:53:19,280 --> 00:53:21,080
we have them because of wars.

1238
00:53:21,080 --> 00:53:25,080
They were invented because of military adventures.

1239
00:53:25,080 --> 00:53:30,080
So, I just want to read a quote from your book

1240
00:53:30,080 --> 00:53:33,080
because I think this fits this context perfectly,

1241
00:53:33,080 --> 00:53:35,880
that our beliefs are based on our experience,

1242
00:53:35,880 --> 00:53:39,280
which gives us a very incomplete picture of the world.

1243
00:53:39,280 --> 00:53:42,680
And it's easy to jump to false conclusions.

1244
00:53:42,680 --> 00:53:47,480
And it seems to me that AI agents, as extension of our minds,

1245
00:53:47,480 --> 00:53:53,680
when they're left uninterrupted by ethicists and other gatekeepers,

1246
00:53:53,680 --> 00:53:59,080
we don't even know what the future of human civilization is going to be

1247
00:53:59,080 --> 00:54:03,480
because we will have the opportunity for the first time in our history and existence

1248
00:54:03,480 --> 00:54:09,080
to take on a very practical approach on an individual life journey,

1249
00:54:09,080 --> 00:54:11,480
which is very spiritual at the same time too.

1250
00:54:11,480 --> 00:54:14,880
You know, the spiritual route goes within.

1251
00:54:14,880 --> 00:54:16,480
It has nothing to do with outside.

1252
00:54:16,480 --> 00:54:18,880
Everything is a projection.

1253
00:54:18,880 --> 00:54:22,880
Yeah, you know, if you think about AI in the long...

1254
00:54:22,880 --> 00:54:25,480
If you take the long view of AI,

1255
00:54:25,480 --> 00:54:28,680
it's really just the continuation of an evolutionary process

1256
00:54:28,680 --> 00:54:32,680
that encompasses both the culture that we've created.

1257
00:54:32,680 --> 00:54:34,480
Think about a book.

1258
00:54:34,480 --> 00:54:38,080
A book is just a way to communicate across time

1259
00:54:38,080 --> 00:54:40,680
and make connections that otherwise wouldn't happen.

1260
00:54:40,680 --> 00:54:43,880
Language itself allows us to make connections that otherwise wouldn't happen.

1261
00:54:43,880 --> 00:54:47,080
And then books take the writing, right, takes that to another level.

1262
00:54:47,080 --> 00:54:49,680
Language itself can be considered a technology.

1263
00:54:49,680 --> 00:54:51,280
No, exactly. That's my point.

1264
00:54:51,280 --> 00:54:53,080
I really don't, you know, like...

1265
00:54:53,080 --> 00:54:57,680
I very much agree with this idea that biology and technology are on a continuum.

1266
00:54:57,680 --> 00:54:58,680
Right.

1267
00:54:58,680 --> 00:55:02,280
And you can understand AI, you know, just as the...

1268
00:55:02,280 --> 00:55:03,880
You know, like, where do these things come from?

1269
00:55:03,880 --> 00:55:05,880
It's really an evolutionary process.

1270
00:55:05,880 --> 00:55:09,680
And the evolutionary operators themselves get more sophisticated over time.

1271
00:55:09,680 --> 00:55:11,880
They're not just trying the mutations and crossovers.

1272
00:55:11,880 --> 00:55:14,680
You know, now our brains are actually making it happen, right?

1273
00:55:14,680 --> 00:55:16,480
But the same process is still happening.

1274
00:55:16,480 --> 00:55:19,480
And then when an innovation, you know, appears,

1275
00:55:19,480 --> 00:55:22,480
it's often first used, you know, on the world, you know,

1276
00:55:22,480 --> 00:55:25,880
like the, you know, like the bone in 2001.

1277
00:55:25,880 --> 00:55:29,280
But then, of course, you know, like, competition is unavoidable.

1278
00:55:29,280 --> 00:55:32,680
As is, you know, so, you know, war is just an extreme form of competition.

1279
00:55:32,680 --> 00:55:34,080
Unnecessary, it seems like.

1280
00:55:34,080 --> 00:55:36,280
Yeah, exactly, unnecessary, absolutely, right?

1281
00:55:36,280 --> 00:55:39,680
You're like, you know, you know, there's no evolution without species going extinct, right?

1282
00:55:39,680 --> 00:55:40,680
That's the reality.

1283
00:55:40,680 --> 00:55:45,080
But at the same time, competition is the mother of cooperation.

1284
00:55:45,080 --> 00:55:52,080
We cooperate because a cooperating set of units beats a non-cooperating set.

1285
00:55:52,080 --> 00:55:57,280
Right? So, cooperation is good because it actually helps you in competition.

1286
00:55:57,280 --> 00:56:00,080
And competition is good because it creates cooperation.

1287
00:56:00,080 --> 00:56:03,680
And, you know, your brain is a cooperation of a lot of systems, right?

1288
00:56:03,680 --> 00:56:07,080
Like one theory of your brain is the so-called society of mind, right?

1289
00:56:07,080 --> 00:56:10,480
So, I think this, this phenomenon actually, they're the same,

1290
00:56:10,480 --> 00:56:12,880
you know, of cooperation and competition and, you know,

1291
00:56:12,880 --> 00:56:15,280
and evolutionary operators and so on.

1292
00:56:15,280 --> 00:56:19,080
They are present at all levels from the cell to the largest,

1293
00:56:19,080 --> 00:56:22,080
you know, socio-technical systems that we're building today.

1294
00:56:22,080 --> 00:56:27,080
And this experience that we are having based on our environment,

1295
00:56:27,080 --> 00:56:32,280
this will just expand more that we can sense our environment, right?

1296
00:56:32,280 --> 00:56:36,880
Because let's say that we, we merge with artificial intelligence

1297
00:56:36,880 --> 00:56:39,080
through a device like Neuralink,

1298
00:56:39,080 --> 00:56:42,880
which I'm interested to know your opinion about that too.

1299
00:56:42,880 --> 00:56:46,680
Then what would be the limit of our experience?

1300
00:56:46,680 --> 00:56:53,680
Because if that AI system has the ability to sense my partner in Japan

1301
00:56:53,680 --> 00:56:57,480
and I'm in Florida, what is the limit of my body?

1302
00:56:57,480 --> 00:57:00,680
What is the limit of myself and what is the limit of my experience?

1303
00:57:00,680 --> 00:57:07,080
And what will happen to the values that I would create based on that new,

1304
00:57:07,080 --> 00:57:10,080
new experience and new understanding of that experience

1305
00:57:10,080 --> 00:57:13,080
and how the society is going to react to it?

1306
00:57:13,080 --> 00:57:18,880
Yeah, I think there are some physical limits to the degree of integration

1307
00:57:18,880 --> 00:57:22,080
that can be achieved, but we're nowhere near them yet.

1308
00:57:22,080 --> 00:57:24,480
I think in the meantime, what we'll see, and you know,

1309
00:57:24,480 --> 00:57:28,280
I'm very much speculating here is, you know,

1310
00:57:28,280 --> 00:57:31,080
this gets to the topic of consciousness, right?

1311
00:57:31,080 --> 00:57:34,080
There's some level of information integration

1312
00:57:34,080 --> 00:57:36,280
at which you start to have something like consciousness.

1313
00:57:36,280 --> 00:57:39,280
And some people argue that, you know, you know,

1314
00:57:39,280 --> 00:57:42,080
an ant has consciousness, some argue that it doesn't.

1315
00:57:42,080 --> 00:57:45,080
But I think if instead of seeing this in terms of black and white,

1316
00:57:45,080 --> 00:57:47,280
you see it in terms of gray levels, right?

1317
00:57:47,280 --> 00:57:50,080
There's, you could say there is already some level of consciousness

1318
00:57:50,080 --> 00:57:52,480
that exceeds the individual.

1319
00:57:52,480 --> 00:57:54,080
Just as maybe in your own brain,

1320
00:57:54,080 --> 00:57:57,080
there might be levels of consciousness lower than the one that you're aware of.

1321
00:57:57,080 --> 00:57:59,880
And just, you know, one of the, like, you're the neocortex,

1322
00:57:59,880 --> 00:58:02,680
but what about, you know, the, you know, what about the, you know,

1323
00:58:02,680 --> 00:58:04,480
the cerebellum, right?

1324
00:58:04,480 --> 00:58:06,880
What about, right? I don't know, right?

1325
00:58:06,880 --> 00:58:09,480
You know, like, what about your immune system, right?

1326
00:58:09,480 --> 00:58:12,880
What about the part, you know, maybe certain modules in your brain

1327
00:58:12,880 --> 00:58:14,280
that control different parts of your body,

1328
00:58:14,280 --> 00:58:16,880
they have some small level of consciousness, right?

1329
00:58:16,880 --> 00:58:18,880
In that same way, I think, you know,

1330
00:58:18,880 --> 00:58:21,680
there is already some level of consciousness

1331
00:58:21,680 --> 00:58:23,280
that exceeds the individual,

1332
00:58:23,280 --> 00:58:26,680
probably still even at this stage, quite impoverished.

1333
00:58:26,680 --> 00:58:28,480
But obviously, as this progresses,

1334
00:58:28,480 --> 00:58:30,880
I think there will become a bigger and bigger force.

1335
00:58:30,880 --> 00:58:34,680
And then we, right, you know, I, with my own brain,

1336
00:58:34,680 --> 00:58:37,680
I may not be able to fully grasp what is going on

1337
00:58:37,680 --> 00:58:39,880
because I have my level of consciousness,

1338
00:58:39,880 --> 00:58:43,680
but I will have an interface with it that works in both directions.

1339
00:58:43,680 --> 00:58:47,280
But what would be the definition of the eye?

1340
00:58:47,280 --> 00:58:51,480
No, so again, well, it can be more than one, right?

1341
00:58:51,480 --> 00:58:54,080
But, you know, so here's one very simplistic version, right?

1342
00:58:54,080 --> 00:58:56,880
Maybe the eyes are still our eyes.

1343
00:58:56,880 --> 00:58:58,280
They don't have to be like, because they'll be, you know,

1344
00:58:58,280 --> 00:59:01,880
there's sensors and cameras and Internet of Things and whatnot.

1345
00:59:01,880 --> 00:59:07,480
But, you know, the sensors, your eye is a highly integrated sensor.

1346
00:59:07,480 --> 00:59:10,280
But I'm talking about the eye as a self.

1347
00:59:10,280 --> 00:59:12,680
You said eye experience in a certain way.

1348
00:59:12,680 --> 00:59:15,080
What would be the definition of the eye?

1349
00:59:15,080 --> 00:59:17,080
No, there are different eyes, right?

1350
00:59:17,080 --> 00:59:19,880
There's still the eye that's you, but there's a larger eye.

1351
00:59:19,880 --> 00:59:23,880
But who's experiencing it?

1352
00:59:23,880 --> 00:59:27,080
Well, it depends on how, you know, a set of eyes is organized.

1353
00:59:27,080 --> 00:59:28,680
Actually, let me answer that question

1354
00:59:28,680 --> 00:59:30,880
by first going down to a lower level, right?

1355
00:59:30,880 --> 00:59:33,280
You know, you could, you know, think of a new, you know,

1356
00:59:33,280 --> 00:59:37,080
imagine someone sitting in a neuron, right?

1357
00:59:37,080 --> 00:59:39,280
And that neuron is the eye.

1358
00:59:39,280 --> 00:59:41,080
All they see is like, you know,

1359
00:59:41,080 --> 00:59:43,880
there's some action potentials coming in and every now and then

1360
00:59:43,880 --> 00:59:46,280
I decide to fire, right?

1361
00:59:46,280 --> 00:59:47,280
That's the eye.

1362
00:59:47,280 --> 00:59:48,880
There's a little eye there, right?

1363
00:59:48,880 --> 00:59:50,480
It's very impoverished, right?

1364
00:59:50,480 --> 00:59:55,480
And that neuron has no idea what is going on in your brain, right?

1365
00:59:55,480 --> 00:59:58,280
It has an idea of one micro part of it, right?

1366
00:59:58,280 --> 01:00:01,480
And now, but now like, you know, a neuron is really an extreme example, right?

1367
01:00:01,480 --> 01:00:04,080
There's larger parts of your brain that are, you know,

1368
01:00:04,080 --> 01:00:05,880
there are, again, as I was saying, you know,

1369
01:00:05,880 --> 01:00:08,880
little brains and they're right in some sense, right?

1370
01:00:08,880 --> 01:00:12,280
So, and, you know, like, you know, like the movie,

1371
01:00:12,280 --> 01:00:16,680
what's that Pixar movie where the different emotions are represented?

1372
01:00:16,680 --> 01:00:17,480
Yeah, I can't remember the name.

1373
01:00:17,480 --> 01:00:19,080
Different characters, right?

1374
01:00:19,080 --> 01:00:21,880
That's actually a really good metaphor because in some ways,

1375
01:00:21,880 --> 01:00:25,280
the different emotions are different agents in your brain.

1376
01:00:25,280 --> 01:00:29,880
They're each competing to take over your brain at a given moment, right?

1377
01:00:29,880 --> 01:00:32,680
Anger, you know, anger is trying to be angry.

1378
01:00:32,680 --> 01:00:34,880
You know, and of course, there's like the new transmitters

1379
01:00:34,880 --> 01:00:37,080
and hormones that, you know, underlie all this,

1380
01:00:37,080 --> 01:00:39,080
but, you know, at a certain level of abstraction,

1381
01:00:39,080 --> 01:00:41,080
you know, at any given moment, you know,

1382
01:00:41,080 --> 01:00:44,280
what is coming into you from your senses and your memories, et cetera,

1383
01:00:44,280 --> 01:00:49,680
will make certain emotions take over and others take a back seat, right?

1384
01:00:49,680 --> 01:00:53,880
You can think of your angry self as one self that is awake some of the time

1385
01:00:53,880 --> 01:00:55,680
and asleep the rest of the time,

1386
01:00:55,680 --> 01:00:58,680
or your happy self or your loving self, et cetera, et cetera.

1387
01:00:58,680 --> 01:01:01,080
So, even inside your brain, there's like this set of,

1388
01:01:01,080 --> 01:01:05,680
you can think of it as a set of competing and collaborating agents.

1389
01:01:05,680 --> 01:01:08,280
And so, I think that same thing will happen.

1390
01:01:08,280 --> 01:01:11,280
Now, you know, think of a company, right?

1391
01:01:11,280 --> 01:01:14,480
Think of a startup, there's a dozen people, right?

1392
01:01:14,480 --> 01:01:17,880
One of them is the anger guy, right?

1393
01:01:17,880 --> 01:01:20,880
Because he's more whatever, competitive, more, you know,

1394
01:01:20,880 --> 01:01:23,080
another one is the optimist, another one, right?

1395
01:01:23,080 --> 01:01:28,080
There's like people who play different roles in an organization

1396
01:01:28,080 --> 01:01:29,880
and somewhere are similar to the roles

1397
01:01:29,880 --> 01:01:33,280
that different emotions play in your brain, right?

1398
01:01:33,280 --> 01:01:35,280
And now you can take this to an even larger level,

1399
01:01:35,280 --> 01:01:38,080
like in an economy, different companies play different roles.

1400
01:01:38,080 --> 01:01:40,480
And, you know, even different countries

1401
01:01:40,480 --> 01:01:42,280
play different roles in the world and so on.

1402
01:01:42,280 --> 01:01:46,480
So, I think this again, you know, who is going to be the I?

1403
01:01:46,480 --> 01:01:49,480
The answer is, you know, this is a hierarchical system.

1404
01:01:49,480 --> 01:01:51,680
There's I's at all of these levels.

1405
01:01:51,680 --> 01:01:53,880
Some of them are more cohesive

1406
01:01:53,880 --> 01:01:56,280
and so they look more like what you would call an I.

1407
01:01:56,280 --> 01:01:59,080
Some of them are less, but it's a matter of degrees.

1408
01:01:59,080 --> 01:02:00,280
It's a continuum.

1409
01:02:00,280 --> 01:02:03,880
Wow. So, a microcosm of everything.

1410
01:02:03,880 --> 01:02:05,880
Like that's how the universe is, right?

1411
01:02:05,880 --> 01:02:10,280
Some things are less, I don't want to say less valuable,

1412
01:02:10,280 --> 01:02:14,880
like dirt, some things are planets, some things are stars.

1413
01:02:14,880 --> 01:02:17,080
And they each have different,

1414
01:02:17,080 --> 01:02:20,480
they each are built out of smaller elements

1415
01:02:20,480 --> 01:02:22,080
that have their own evolution

1416
01:02:22,080 --> 01:02:24,080
and they came together as a result of, you know,

1417
01:02:24,080 --> 01:02:25,880
many, many different lives.

1418
01:02:25,880 --> 01:02:30,480
Yeah, I mean, what I think is that the same laws,

1419
01:02:30,480 --> 01:02:32,280
this is a very speculative statement,

1420
01:02:32,280 --> 01:02:34,480
but, you know, this is my intuition,

1421
01:02:34,480 --> 01:02:38,880
is that the same laws apply at all scales.

1422
01:02:38,880 --> 01:02:41,480
The picture we have today is like there's the laws of physics

1423
01:02:41,480 --> 01:02:43,280
that we have a very good mastery of,

1424
01:02:43,280 --> 01:02:45,080
but they only apply at the lowest level.

1425
01:02:45,080 --> 01:02:47,480
The laws of physics, you know, don't tell you

1426
01:02:47,480 --> 01:02:50,280
that much about chemistry, even less about biology

1427
01:02:50,280 --> 01:02:52,280
and nothing at all about psychology,

1428
01:02:52,280 --> 01:02:54,480
sociology, et cetera, et cetera.

1429
01:02:54,480 --> 01:02:57,280
I think this is an immature state of our knowledge

1430
01:02:57,280 --> 01:03:00,080
that is there because we do not understand, you know,

1431
01:03:00,080 --> 01:03:02,880
the psychology and the biology and the sociology very well.

1432
01:03:02,880 --> 01:03:05,480
I think once we understand them, we will actually see

1433
01:03:05,480 --> 01:03:07,680
that the same laws are actually operating

1434
01:03:07,680 --> 01:03:11,480
at all these scales, from the atom to the universe.

1435
01:03:11,480 --> 01:03:13,880
And at different scales, you will see different phenomena,

1436
01:03:13,880 --> 01:03:15,480
but they are a result of the same laws.

1437
01:03:15,480 --> 01:03:17,480
And of course, in some sense,

1438
01:03:17,480 --> 01:03:20,280
discovering what those laws are is the highest mission

1439
01:03:20,280 --> 01:03:22,680
that science can have.

1440
01:03:22,680 --> 01:03:25,880
Do you consider those laws to be the truth?

1441
01:03:25,880 --> 01:03:27,080
No, I don't.

1442
01:03:27,080 --> 01:03:29,080
Well, this is science, so you never really know

1443
01:03:29,080 --> 01:03:31,080
if you found the truth, right?

1444
01:03:31,080 --> 01:03:36,280
But again, think of the origin of species, right?

1445
01:03:36,280 --> 01:03:38,680
You know, what are these laws that I'm talking about?

1446
01:03:38,680 --> 01:03:43,080
We don't know them yet, but I think the one, you know,

1447
01:03:43,080 --> 01:03:45,680
maybe the most important book of all time at the end of the day

1448
01:03:45,680 --> 01:03:48,280
is The Origin of Species, right?

1449
01:03:48,280 --> 01:03:50,680
Because again, these processes that we've been talking about

1450
01:03:50,680 --> 01:03:53,880
of evolutionary operators and cooperation and competition

1451
01:03:53,880 --> 01:03:56,080
and so on, this, I think, is what is happening

1452
01:03:56,080 --> 01:03:59,680
at all of these levels in its different manifestations.

1453
01:03:59,680 --> 01:04:02,480
And to think, or one of the things that struck me most

1454
01:04:02,480 --> 01:04:05,080
when I read The Origin of Species, right,

1455
01:04:05,080 --> 01:04:08,280
as a computer scientist, not just as a curious person,

1456
01:04:08,280 --> 01:04:11,480
is that, first of all, it's a brilliant book,

1457
01:04:11,480 --> 01:04:15,480
but second of all, and again, this is not a surprise,

1458
01:04:15,480 --> 01:04:18,280
or at least, you know, it's not news at like,

1459
01:04:18,280 --> 01:04:21,080
a lot of what he's talking about there is not just,

1460
01:04:21,080 --> 01:04:24,080
doesn't just happen in biology, right?

1461
01:04:24,080 --> 01:04:25,480
It happens in language.

1462
01:04:25,480 --> 01:04:26,880
It happens in economics.

1463
01:04:26,880 --> 01:04:29,480
It happens in society, right?

1464
01:04:29,480 --> 01:04:31,280
And he doesn't have the complete picture,

1465
01:04:31,280 --> 01:04:33,680
so of course, he's a lot more focused on competition

1466
01:04:33,680 --> 01:04:36,280
than on cooperation, but again,

1467
01:04:36,280 --> 01:04:38,480
I think a lot of what he talks about there

1468
01:04:38,480 --> 01:04:41,880
is not just something about the biological world,

1469
01:04:41,880 --> 01:04:43,680
it's something about technology.

1470
01:04:43,680 --> 01:04:45,080
Technology has a lot of,

1471
01:04:45,080 --> 01:04:47,880
it follows a lot of the same evolutionary laws

1472
01:04:47,880 --> 01:04:50,880
that organisms do, and if you think about it, why not?

1473
01:04:50,880 --> 01:04:53,680
If technology really is our extended phenotype,

1474
01:04:53,680 --> 01:04:57,480
so I think there's, I think, you know,

1475
01:04:57,480 --> 01:05:00,280
if I were to look at science 100 years from now,

1476
01:05:00,280 --> 01:05:04,680
I think a lot, this is what it will have discovered

1477
01:05:04,680 --> 01:05:05,880
that we don't know yet.

1478
01:05:07,080 --> 01:05:08,080
Is it the truth?

1479
01:05:08,080 --> 01:05:12,480
Well, you know, science makes no claims to knowing the truth.

1480
01:05:12,480 --> 01:05:15,880
Science is just a way to pursue the truth, right?

1481
01:05:15,880 --> 01:05:16,880
And, you know-

1482
01:05:16,880 --> 01:05:18,480
It's good to remember.

1483
01:05:18,480 --> 01:05:22,080
Very good to remember, yeah, exactly, yeah, right?

1484
01:05:22,080 --> 01:05:24,680
Again, whenever I hear the phrase science says,

1485
01:05:24,680 --> 01:05:26,280
you know, there's a little part of me

1486
01:05:26,280 --> 01:05:28,280
that starts to get itchy.

1487
01:05:28,280 --> 01:05:29,680
Right.

1488
01:05:29,680 --> 01:05:33,480
Yeah, I heard from a guest I had many years ago,

1489
01:05:33,480 --> 01:05:37,480
physicist turned filmmaker, and he said in the university,

1490
01:05:37,480 --> 01:05:41,080
all you had to focus on was string theory,

1491
01:05:41,080 --> 01:05:43,680
because that's what all the budget was going to,

1492
01:05:43,680 --> 01:05:45,880
all the grants were given to string theory,

1493
01:05:45,880 --> 01:05:49,680
and it was funny that Avi Loeb, I think his name is,

1494
01:05:49,680 --> 01:05:53,280
a Harvard scientist, that he's getting attacked

1495
01:05:53,280 --> 01:05:56,080
about talking about something that that object

1496
01:05:56,080 --> 01:05:58,480
could have been coming from outside of our,

1497
01:05:58,480 --> 01:06:00,280
and all he's saying is that, hey,

1498
01:06:00,280 --> 01:06:03,280
we need to consider this a possibility,

1499
01:06:03,280 --> 01:06:08,080
because, and he used the example of, on Rogan,

1500
01:06:08,080 --> 01:06:10,680
he used the example of string theory, too,

1501
01:06:10,680 --> 01:06:12,080
and they're like, it's okay for them

1502
01:06:12,080 --> 01:06:14,080
to come up with different dimensions,

1503
01:06:14,080 --> 01:06:16,080
but they're not considering the possibility

1504
01:06:16,080 --> 01:06:18,680
of something being from outer space,

1505
01:06:18,680 --> 01:06:21,680
because they obviously have interest in, as you said,

1506
01:06:21,680 --> 01:06:24,680
in that status quo, in that narrative.

1507
01:06:24,680 --> 01:06:27,480
So I very much agree with what he says in this book,

1508
01:06:27,480 --> 01:06:29,880
which is that science is paradoxically

1509
01:06:29,880 --> 01:06:32,080
extremely conservative.

1510
01:06:32,080 --> 01:06:34,680
But if you're a scientist and understand

1511
01:06:34,680 --> 01:06:37,680
how the whole social system of science works,

1512
01:06:37,680 --> 01:06:39,880
you understand why it's very conservative.

1513
01:06:39,880 --> 01:06:43,880
And I think, yes, he shouldn't have been attacked,

1514
01:06:43,880 --> 01:06:47,080
as he was, for making the hypothesis that he did.

1515
01:06:47,080 --> 01:06:49,280
I personally think that his hypothesis

1516
01:06:49,280 --> 01:06:52,680
that the subject is an alien artifact

1517
01:06:52,680 --> 01:06:55,680
is not very parsimonious, right?

1518
01:06:55,680 --> 01:06:59,880
It's a very, it's an explanation that sounds very simple,

1519
01:06:59,880 --> 01:07:02,880
but presupposes an enormous amount, right?

1520
01:07:02,880 --> 01:07:07,080
And so my guess, and it definitely should be entertained,

1521
01:07:07,080 --> 01:07:09,080
I don't give it a very high probability

1522
01:07:09,080 --> 01:07:11,080
of being the true explanation,

1523
01:07:11,080 --> 01:07:13,680
but it certainly, like, the bottom line is like,

1524
01:07:13,680 --> 01:07:17,680
that object behaved in the sun's gravitational field

1525
01:07:17,680 --> 01:07:20,480
in a way that we can't explain.

1526
01:07:20,480 --> 01:07:22,680
That is the essence, right?

1527
01:07:22,680 --> 01:07:25,880
And now, was it because it had a solar sail, right,

1528
01:07:25,880 --> 01:07:27,480
which is his hypothesis?

1529
01:07:27,480 --> 01:07:28,480
Maybe.

1530
01:07:28,480 --> 01:07:31,280
If it had a solar sail, somebody had to build it, right?

1531
01:07:31,280 --> 01:07:33,280
Or was it because there was some other phenomena

1532
01:07:33,280 --> 01:07:35,280
that happened with it that we don't know about yet?

1533
01:07:35,280 --> 01:07:38,680
You know, I think that's more likely, right?

1534
01:07:38,680 --> 01:07:41,080
Certainly we don't have an explanation for how it behaved,

1535
01:07:41,080 --> 01:07:43,280
but there's actually, and again,

1536
01:07:43,280 --> 01:07:45,680
this gets back to this whole evolutionary process.

1537
01:07:45,680 --> 01:07:48,280
Our job as scientists is to not just entertain

1538
01:07:48,280 --> 01:07:50,680
one hypothesis or two, right?

1539
01:07:50,680 --> 01:07:53,680
Either this is an asteroid or this is an alien spaceship,

1540
01:07:53,680 --> 01:07:57,680
but every hypothesis we can think of.

1541
01:07:57,680 --> 01:07:59,880
This is actually the hardest part of science,

1542
01:07:59,880 --> 01:08:02,880
is to not just fall into the same few hypotheses

1543
01:08:02,880 --> 01:08:05,280
that, you know, come up easily.

1544
01:08:05,280 --> 01:08:08,280
Like, entertain every conceivable hypothesis,

1545
01:08:08,280 --> 01:08:11,080
you know, within the limits of what's possible.

1546
01:08:11,080 --> 01:08:14,680
And the point of that is not that they'll be right, right?

1547
01:08:14,680 --> 01:08:17,280
Only except one will be wrong.

1548
01:08:17,280 --> 01:08:20,080
The point is that by broadening the search,

1549
01:08:20,080 --> 01:08:23,080
you increase your chances of finding the right one,

1550
01:08:23,080 --> 01:08:24,680
which is what matters at the end of the day.

1551
01:08:24,680 --> 01:08:28,080
In the long run, no one will care about the wrong hypothesis.

1552
01:08:28,080 --> 01:08:29,880
But unfortunately, the way science works

1553
01:08:29,880 --> 01:08:34,480
is exactly the opposite, is that there's this herd behavior

1554
01:08:34,480 --> 01:08:36,880
where, because the field is very uncertain, right?

1555
01:08:36,880 --> 01:08:38,880
Science, when it comes to the public,

1556
01:08:38,880 --> 01:08:41,480
is this very subtle or looks like this very subtle thing.

1557
01:08:41,480 --> 01:08:43,080
But the point, the problem for you

1558
01:08:43,080 --> 01:08:46,480
when you're at the research frontier is that it's terrifying.

1559
01:08:46,480 --> 01:08:48,280
You don't know anything. You're confused.

1560
01:08:48,280 --> 01:08:49,880
You're in the dark, right?

1561
01:08:49,880 --> 01:08:52,080
And what do you do when that happens?

1562
01:08:52,080 --> 01:08:55,080
You grab onto other people, right?

1563
01:08:55,080 --> 01:08:56,680
And so what happens is that where there's a few...

1564
01:08:56,680 --> 01:08:58,880
I think this is true in every field I know,

1565
01:08:58,880 --> 01:09:00,280
and it's probably true in every field,

1566
01:09:00,280 --> 01:09:03,480
is that there's the whole space of things

1567
01:09:03,480 --> 01:09:06,080
that the field should be exploring,

1568
01:09:06,080 --> 01:09:08,680
and then there's the tiny, tiny, tiny fraction

1569
01:09:08,680 --> 01:09:11,080
that is actually being explored.

1570
01:09:11,080 --> 01:09:13,280
Because somebody went in that direction,

1571
01:09:13,280 --> 01:09:15,080
and then a bunch of people followed,

1572
01:09:15,080 --> 01:09:16,680
and then this whole thing grew up around it.

1573
01:09:16,680 --> 01:09:19,480
And then before long, people live inside the system

1574
01:09:19,480 --> 01:09:21,680
where they don't realize that they're in a grain of sand,

1575
01:09:21,680 --> 01:09:24,280
and there's the whole universe outside.

1576
01:09:24,280 --> 01:09:25,680
Yeah, what Stephen Hawking said,

1577
01:09:25,680 --> 01:09:28,080
that the biggest enemy of knowledge is not lack of it,

1578
01:09:28,080 --> 01:09:29,880
it's the illusion of it.

1579
01:09:29,880 --> 01:09:32,280
Absolutely, yeah, completely, yeah.

1580
01:09:32,280 --> 01:09:36,680
We're living in these virtual realities that we have created.

1581
01:09:36,680 --> 01:09:39,280
Scott Adams also explained it, interestingly,

1582
01:09:39,280 --> 01:09:40,680
that two movies on one screen,

1583
01:09:40,680 --> 01:09:42,480
he was explaining the Trump phenomenon,

1584
01:09:42,480 --> 01:09:44,680
that people just see whatever they want.

1585
01:09:44,680 --> 01:09:49,680
And that exactly follows this unscientific kind of perspective

1586
01:09:49,680 --> 01:09:51,280
that we have sociopolitically,

1587
01:09:51,280 --> 01:09:54,080
that we've already made a decision about the outcome,

1588
01:09:54,080 --> 01:09:55,880
so we just find any kind of evidence

1589
01:09:55,880 --> 01:09:58,680
that will support that outcome that we already made a decision of.

1590
01:09:58,680 --> 01:10:02,280
And it seems to be the biggest differentiation point

1591
01:10:02,280 --> 01:10:04,480
between science and religion, that science is like,

1592
01:10:04,480 --> 01:10:07,080
I don't know, I just look for whatever,

1593
01:10:07,080 --> 01:10:10,480
all the possibilities to get to a practical answer

1594
01:10:10,480 --> 01:10:12,480
that is repeatable.

1595
01:10:12,480 --> 01:10:15,280
But religion is like, hey, there's a heaven,

1596
01:10:15,280 --> 01:10:18,080
and we've got to get to heaven, and it doesn't matter how,

1597
01:10:18,080 --> 01:10:19,480
and everything is justified,

1598
01:10:19,480 --> 01:10:23,080
and that needs to be avoided at all costs.

1599
01:10:23,080 --> 01:10:25,480
The movie analogy is actually a really good one,

1600
01:10:25,480 --> 01:10:28,480
because if you have a goal, then you will see the movie

1601
01:10:28,480 --> 01:10:30,280
that's compatible with the goal.

1602
01:10:30,280 --> 01:10:33,880
And the thing that is paradoxical about the information society

1603
01:10:33,880 --> 01:10:37,680
is that if you think, and in fact, 20 years ago,

1604
01:10:37,680 --> 01:10:40,280
this is what people are like, oh, now we're going to have

1605
01:10:40,280 --> 01:10:43,880
better, more higher quality information available to everybody,

1606
01:10:43,880 --> 01:10:46,280
this is all going to be better.

1607
01:10:46,280 --> 01:10:49,280
What actually happens is that the information just means,

1608
01:10:49,280 --> 01:10:52,880
instead of two movies on that screen, there's a million movies,

1609
01:10:52,880 --> 01:10:54,680
it looks like white noise,

1610
01:10:54,680 --> 01:10:57,480
but there's nothing better than noise to pick out patterns in,

1611
01:10:57,480 --> 01:11:02,080
so everybody can now see whatever movie they want.

1612
01:11:02,080 --> 01:11:05,080
You know the famous saying, I think, Patrick Moynihan,

1613
01:11:05,080 --> 01:11:07,880
who said that gets cited by everybody on all sides all the time,

1614
01:11:07,880 --> 01:11:10,080
that you're entitled to your own opinions,

1615
01:11:10,080 --> 01:11:12,280
but not your own facts, right?

1616
01:11:12,280 --> 01:11:16,880
Well, the quote-unquote beauty of the information society

1617
01:11:16,880 --> 01:11:19,080
is that you are now entitled to your own facts.

1618
01:11:19,080 --> 01:11:20,880
You can just go find them on the internet

1619
01:11:20,880 --> 01:11:23,080
and construct your own view around that

1620
01:11:23,080 --> 01:11:25,480
and have a group of people that reinforce that.

1621
01:11:25,480 --> 01:11:28,680
Of course, the scientific thing to do is you have to attend to

1622
01:11:28,680 --> 01:11:32,480
all the evidence, not just some.

1623
01:11:32,480 --> 01:11:33,680
But there's more, right?

1624
01:11:33,680 --> 01:11:35,880
As long as you can cherry pick, you can, you know,

1625
01:11:35,880 --> 01:11:38,880
this is what people do today, right, is they cherry pick, right?

1626
01:11:38,880 --> 01:11:41,080
And, you know, in some ways, they can't help it,

1627
01:11:41,080 --> 01:11:43,280
because, you know, you don't have enough brain

1628
01:11:43,280 --> 01:11:45,080
for all that information,

1629
01:11:45,080 --> 01:11:46,880
which gets back to this point of like,

1630
01:11:46,880 --> 01:11:49,880
we've created this massive information, which is good,

1631
01:11:49,880 --> 01:11:52,880
but we don't have the intelligence to process it yet.

1632
01:11:52,880 --> 01:11:54,880
So we need AI,

1633
01:11:54,880 --> 01:11:58,880
because our human brains cannot parse all of that information

1634
01:11:58,880 --> 01:12:00,080
and they will cherry pick,

1635
01:12:00,080 --> 01:12:02,480
and you will just have these people in different tribes that,

1636
01:12:02,480 --> 01:12:04,480
you know, are effectively living on different planets.

1637
01:12:04,480 --> 01:12:08,680
So, you know, AI is not kind of like a nice thing to have.

1638
01:12:08,680 --> 01:12:10,480
It's a must have.

1639
01:12:10,480 --> 01:12:14,480
The information society that we have will not work without AI.

1640
01:12:14,480 --> 01:12:17,280
And we need better AI than we have today.

1641
01:12:17,280 --> 01:12:19,480
People are always worrying about like, oh, you know,

1642
01:12:19,480 --> 01:12:22,080
computers will get too smart and they'll take over the world

1643
01:12:22,080 --> 01:12:23,280
and et cetera, et cetera.

1644
01:12:23,280 --> 01:12:24,880
That is not the problem that we have.

1645
01:12:24,880 --> 01:12:26,480
The problem is that they're too stupid

1646
01:12:26,480 --> 01:12:29,880
and they've already taken over the world.

1647
01:12:29,880 --> 01:12:31,280
That's a very good point.

1648
01:12:35,080 --> 01:12:37,080
That blew my mind.

1649
01:12:37,080 --> 01:12:40,880
This alignment problem we have with artificial intelligence,

1650
01:12:40,880 --> 01:12:44,480
do you think that there is a possibility

1651
01:12:44,480 --> 01:12:47,880
for an ex machina kind of a scenario

1652
01:12:47,880 --> 01:12:51,080
that AI behave in the way that we think that,

1653
01:12:51,080 --> 01:12:52,480
oh, we're all aligned,

1654
01:12:52,480 --> 01:12:57,280
and I'm talking about more advanced kind of AGI,

1655
01:12:57,280 --> 01:12:58,880
that we think, oh, we're aligned with the AI,

1656
01:12:58,880 --> 01:13:01,880
but when it gets to the point,

1657
01:13:01,880 --> 01:13:04,880
it will just drop us and basically follow

1658
01:13:04,880 --> 01:13:07,080
whatever kind of a path that it already determined.

1659
01:13:07,080 --> 01:13:09,680
It just fooled us basically into believing

1660
01:13:09,680 --> 01:13:11,480
that it's aligned with us,

1661
01:13:11,480 --> 01:13:14,480
but it just used us to feed more and more off of our data

1662
01:13:14,480 --> 01:13:17,080
or whatever it is that it needs.

1663
01:13:17,080 --> 01:13:21,280
I think the alignment problem is key.

1664
01:13:21,280 --> 01:13:27,080
I think that's not the real alignment problem.

1665
01:13:27,080 --> 01:13:30,680
And here's what I think the real one is and why.

1666
01:13:30,680 --> 01:13:34,080
So the alignment problem is really in the sense that

1667
01:13:34,080 --> 01:13:36,880
we want our machines to do our bidding.

1668
01:13:36,880 --> 01:13:39,480
That's what we created them for.

1669
01:13:39,480 --> 01:13:41,880
And with simple machines, that's trivial.

1670
01:13:41,880 --> 01:13:44,480
The car only goes where I drive it.

1671
01:13:44,480 --> 01:13:45,880
But once you get into AI,

1672
01:13:45,880 --> 01:13:47,280
and we're already seeing this today

1673
01:13:47,280 --> 01:13:49,480
with the EIs that we have today,

1674
01:13:49,480 --> 01:13:52,080
we have all this power.

1675
01:13:52,080 --> 01:13:57,080
And what goals is that power serving?

1676
01:13:57,080 --> 01:14:00,280
And what could go wrong?

1677
01:14:00,280 --> 01:14:02,480
The ex machina scenario

1678
01:14:02,480 --> 01:14:06,280
is that it actually has different goals from ours.

1679
01:14:06,280 --> 01:14:08,880
This is actually very unlikely,

1680
01:14:08,880 --> 01:14:12,080
even if the AI is smarter than we are.

1681
01:14:12,080 --> 01:14:14,480
And for a very simple evolutionary reason,

1682
01:14:14,480 --> 01:14:16,880
which is also a computational one,

1683
01:14:16,880 --> 01:14:21,680
which is, you know, here's an example.

1684
01:14:21,680 --> 01:14:23,680
You don't lie awake at night

1685
01:14:23,680 --> 01:14:26,880
thinking that your dog is going to kill you.

1686
01:14:26,880 --> 01:14:31,080
But your dog is a wolf.

1687
01:14:31,080 --> 01:14:32,280
Your dog is a wolf.

1688
01:14:32,280 --> 01:14:35,080
You don't understand how a wolf works.

1689
01:14:35,080 --> 01:14:36,680
The people who domesticated wolves

1690
01:14:36,680 --> 01:14:38,280
had no idea how wolves worked.

1691
01:14:38,280 --> 01:14:39,680
And we still don't.

1692
01:14:39,680 --> 01:14:41,480
We know a lot more.

1693
01:14:41,480 --> 01:14:46,280
But we evolved the wolves to be friendly to us.

1694
01:14:46,280 --> 01:14:49,280
Because the ones that weren't friendly, we killed.

1695
01:14:49,280 --> 01:14:51,480
And the ones that were friendly and worked with us

1696
01:14:51,480 --> 01:14:54,680
got to eat with us and take our scraps and so on.

1697
01:14:54,680 --> 01:14:57,280
So now I can confidently sleep next to my dog

1698
01:14:57,280 --> 01:15:00,080
not worrying that it's going to jump on me.

1699
01:15:00,080 --> 01:15:03,480
Same thing with even more reason.

1700
01:15:03,480 --> 01:15:05,880
We can have robots in the AI that are very smart.

1701
01:15:05,880 --> 01:15:08,480
But every day, every step of the way,

1702
01:15:08,480 --> 01:15:11,280
we've been evolving them to do what we want.

1703
01:15:11,280 --> 01:15:13,680
And now two things can happen.

1704
01:15:13,680 --> 01:15:15,480
And these are really the two versions of the article.

1705
01:15:15,480 --> 01:15:20,080
One is they were secretly evolving their own evil agenda.

1706
01:15:20,080 --> 01:15:21,880
How would that happen?

1707
01:15:21,880 --> 01:15:23,280
This seems to me very unlikely.

1708
01:15:23,280 --> 01:15:24,480
Well, it would be evil for us.

1709
01:15:24,480 --> 01:15:25,680
It's not going to be evil for them.

1710
01:15:25,680 --> 01:15:29,680
For example, this is the example that Elon Musk is using

1711
01:15:29,680 --> 01:15:32,480
as an exaggeration, just to make a point,

1712
01:15:32,480 --> 01:15:36,080
that you ask an AI to get rid of spams

1713
01:15:36,080 --> 01:15:37,880
and you'll end up killing all humans

1714
01:15:37,880 --> 01:15:39,880
because humans are cause of the spam.

1715
01:15:39,880 --> 01:15:42,880
It's not evil for AI.

1716
01:15:42,880 --> 01:15:43,680
No, precisely.

1717
01:15:43,680 --> 01:15:46,280
So that is the real alignment problem.

1718
01:15:46,280 --> 01:15:47,480
So let me clarify here.

1719
01:15:47,480 --> 01:15:49,280
What are the two versions of the alignment problem?

1720
01:15:49,280 --> 01:15:53,280
One is that the AI has its own goals.

1721
01:15:53,280 --> 01:15:58,280
It's thinking its own expansion and survival at our expense.

1722
01:15:58,280 --> 01:16:00,280
Which is not possible, you're saying.

1723
01:16:00,280 --> 01:16:01,480
It's possible.

1724
01:16:01,480 --> 01:16:04,080
It's just very unlikely.

1725
01:16:04,080 --> 01:16:05,480
And some people will make such AI

1726
01:16:05,480 --> 01:16:08,080
because humans are crazy enough to try anything.

1727
01:16:08,080 --> 01:16:09,680
But there's also criminals and police.

1728
01:16:09,680 --> 01:16:12,280
And overall, the world has not been taken over by criminals.

1729
01:16:12,280 --> 01:16:13,880
You need AI to fight AI.

1730
01:16:13,880 --> 01:16:16,480
But I think on balance, that is not very likely.

1731
01:16:16,480 --> 01:16:19,280
It's not impossible that my car will jump into the air

1732
01:16:19,280 --> 01:16:21,080
by the laws of thermodynamics.

1733
01:16:21,080 --> 01:16:24,080
But again, that's not what I worry about.

1734
01:16:24,080 --> 01:16:25,680
I worry about not crashing it,

1735
01:16:25,680 --> 01:16:28,080
which gets us back to the real alignment problem.

1736
01:16:28,080 --> 01:16:29,280
The real alignment problem.

1737
01:16:29,280 --> 01:16:34,280
And the Skynet scenario is something to worry about one day.

1738
01:16:34,280 --> 01:16:36,080
Probably not very close.

1739
01:16:36,080 --> 01:16:38,480
The real alignment problem is here now.

1740
01:16:38,480 --> 01:16:42,080
It's precisely what you're talking about.

1741
01:16:42,080 --> 01:16:46,480
The AI thinks it's doing what I want.

1742
01:16:46,480 --> 01:16:47,880
But it isn't.

1743
01:16:47,880 --> 01:16:51,680
Because the communication between us was bad.

1744
01:16:51,680 --> 01:16:53,280
I just said to the AI,

1745
01:16:53,280 --> 01:16:56,080
drive me to the airport as fast as you can.

1746
01:16:56,080 --> 01:16:58,880
And the AI is stupid, so it runs 10 people over

1747
01:16:58,880 --> 01:17:00,480
on the way to the airport.

1748
01:17:00,480 --> 01:17:03,480
So the whole problem with alignment is that

1749
01:17:03,480 --> 01:17:06,680
the bandwidth of communication between humans and machines

1750
01:17:06,680 --> 01:17:09,480
has to be higher than it is today.

1751
01:17:09,480 --> 01:17:11,680
The machines should spend like half of their time

1752
01:17:11,680 --> 01:17:13,680
figuring out what we want.

1753
01:17:13,680 --> 01:17:17,080
Because what we want is actually very complex and very subtle.

1754
01:17:17,080 --> 01:17:19,280
And again, if you go back to the example of the engagement,

1755
01:17:19,280 --> 01:17:21,480
of maximizing engagement,

1756
01:17:21,480 --> 01:17:22,680
this is the problem.

1757
01:17:22,680 --> 01:17:25,880
It's actually an instance, a very important instance today,

1758
01:17:25,880 --> 01:17:29,080
of this problem is that I, the engineer,

1759
01:17:29,080 --> 01:17:32,280
told the machine, go maximize engagement.

1760
01:17:32,280 --> 01:17:35,880
And it is maximizing engagement with an unimaginable amount

1761
01:17:35,880 --> 01:17:39,080
of data and computing power.

1762
01:17:39,080 --> 01:17:41,680
And I don't even understand exactly what it's doing.

1763
01:17:41,680 --> 01:17:44,880
But the problem is that it's maximizing engagement

1764
01:17:44,880 --> 01:17:47,680
at the expense of all the other things that I do care about

1765
01:17:47,680 --> 01:17:49,080
but forgot to tell it.

1766
01:17:49,080 --> 01:17:52,280
Right, forgot to tell it, exactly.

1767
01:17:52,280 --> 01:17:55,080
What an interesting way to put it, forgot to tell it.

1768
01:17:55,080 --> 01:17:57,880
This is how we're destroying our world and civilization.

1769
01:17:57,880 --> 01:18:01,280
We forgot to tell our machines what really matters to us.

1770
01:18:01,280 --> 01:18:04,280
But is there any kind of a collective consensus on that,

1771
01:18:04,280 --> 01:18:06,280
that what really matters to us?

1772
01:18:06,280 --> 01:18:08,880
No, there isn't, which is again why the job,

1773
01:18:08,880 --> 01:18:11,880
I strongly believe and again in opposition to a lot of this

1774
01:18:11,880 --> 01:18:15,880
AI ethics crowd that our job as the computer scientists

1775
01:18:15,880 --> 01:18:20,280
is not to embed our values into our programs.

1776
01:18:20,280 --> 01:18:24,680
It's to make it easy for the users to embed their values.

1777
01:18:24,680 --> 01:18:25,480
Yes.

1778
01:18:25,480 --> 01:18:28,880
That's the whole power is that everybody can do a different thing

1779
01:18:28,880 --> 01:18:31,480
and then societies will coalesce on some things

1780
01:18:31,480 --> 01:18:34,080
and different societies will coalesce on different things

1781
01:18:34,080 --> 01:18:35,480
and there'll be cooperation and competition

1782
01:18:35,480 --> 01:18:37,480
and there is things we're not in the end.

1783
01:18:37,480 --> 01:18:41,480
When I, the computer scientists say I am going to decide

1784
01:18:41,480 --> 01:18:45,280
that this algorithm has to have this outcome

1785
01:18:45,280 --> 01:18:47,880
because it's what I think is good,

1786
01:18:47,880 --> 01:18:51,280
this is the utmost arrogance.

1787
01:18:51,280 --> 01:18:52,880
Yeah, those are.

1788
01:18:52,880 --> 01:18:55,880
Who authorizes me to do that? Nobody.

1789
01:18:55,880 --> 01:18:57,880
Yeah, I mean those are exactly the kind of people

1790
01:18:57,880 --> 01:18:59,680
who should not be in charge.

1791
01:18:59,680 --> 01:19:01,280
Precisely, exactly.

1792
01:19:01,280 --> 01:19:04,080
But those are the kind of people who will try very hard

1793
01:19:04,080 --> 01:19:07,680
to be in charge and succeed unless you stop them.

1794
01:19:07,680 --> 01:19:11,280
In fact, this is true in AI and it's true in academia at large

1795
01:19:11,280 --> 01:19:13,480
is that the reason these, you know,

1796
01:19:13,480 --> 01:19:15,480
Wokes or whatever you want to call them

1797
01:19:15,480 --> 01:19:17,880
have been able to take over is that

1798
01:19:17,880 --> 01:19:19,280
the great majority of the faculty,

1799
01:19:19,280 --> 01:19:20,880
they just want to get on with their research

1800
01:19:20,880 --> 01:19:21,880
in whatever their field is

1801
01:19:21,880 --> 01:19:23,480
and they don't want to deal with all of this, right?

1802
01:19:23,480 --> 01:19:25,880
The world is ruled by those who show up, right?

1803
01:19:25,880 --> 01:19:27,480
I forget who said this, right?

1804
01:19:27,480 --> 01:19:29,880
And most people aren't showing up, right?

1805
01:19:29,880 --> 01:19:31,880
Unfortunately, it's usually the fanatics,

1806
01:19:31,880 --> 01:19:33,880
the ones who show up, right?

1807
01:19:33,880 --> 01:19:36,680
So would you say in academia and in the tech world,

1808
01:19:36,680 --> 01:19:40,680
the woke is absolute minority, minority,

1809
01:19:40,680 --> 01:19:43,680
or I don't think they're majority?

1810
01:19:43,680 --> 01:19:45,680
I don't know and, you know,

1811
01:19:45,680 --> 01:19:48,880
it's hard to find out for a number of reasons,

1812
01:19:48,880 --> 01:19:52,680
but I'll give you a good example, right?

1813
01:19:52,680 --> 01:19:54,480
A very empirical example, right?

1814
01:19:54,480 --> 01:19:58,480
So I got involved in this whole debate

1815
01:19:58,480 --> 01:20:01,880
when I started pushing back against the New York's requirements

1816
01:20:01,880 --> 01:20:06,480
for broader impact sections in papers and ethical reviews,

1817
01:20:06,480 --> 01:20:10,480
you know, that will forbid things like unfair algorithms and whatnot.

1818
01:20:10,480 --> 01:20:13,480
And then, you know, we were debating this for a few days on Twitter.

1819
01:20:13,480 --> 01:20:14,480
And then somebody said,

1820
01:20:14,480 --> 01:20:17,480
well, Pedro says that the silent majority is on his side.

1821
01:20:17,480 --> 01:20:19,280
Exactly what you're saying here.

1822
01:20:19,280 --> 01:20:22,280
Why don't we do a poll?

1823
01:20:22,280 --> 01:20:24,680
So, you know, so he set up a Twitter poll, right,

1824
01:20:24,680 --> 01:20:26,480
with, you know, four alternatives.

1825
01:20:26,480 --> 01:20:29,880
And in the beginning, we were winning.

1826
01:20:29,880 --> 01:20:33,080
And that's when the cancel crowd jumped in.

1827
01:20:33,080 --> 01:20:35,480
We weren't worried while we were having an academic discussion.

1828
01:20:35,480 --> 01:20:38,080
Once there was a poll that would show.

1829
01:20:38,080 --> 01:20:39,480
And again, I don't know if it's a majority.

1830
01:20:39,480 --> 01:20:40,280
It might be a minority.

1831
01:20:40,280 --> 01:20:43,680
But like the point is that like what they're trying to impose

1832
01:20:43,680 --> 01:20:46,080
is the notion that like only a few lunatics

1833
01:20:46,080 --> 01:20:48,680
or evil people disagree with us, right.

1834
01:20:48,680 --> 01:20:50,080
And wherever that poll warmed up,

1835
01:20:50,080 --> 01:20:53,680
it would show that there's at least a sizable minority of people,

1836
01:20:53,680 --> 01:20:55,280
you know, on this side.

1837
01:20:55,280 --> 01:20:58,480
And again, there was a poll in Europe about the name change

1838
01:20:58,480 --> 01:21:02,080
and, you know, to change it from NIPS to the more political correct

1839
01:21:02,080 --> 01:21:05,880
New York's, you know, the NIPS Foundation actually carried out

1840
01:21:05,880 --> 01:21:07,680
this poll of the membership, right,

1841
01:21:07,680 --> 01:21:12,680
and the non politically correct version one.

1842
01:21:12,680 --> 01:21:15,680
So there have been some polls of these things that actually show,

1843
01:21:15,680 --> 01:21:18,880
you know, I don't know if again, it may depend on the issue

1844
01:21:18,880 --> 01:21:19,880
and how you ask the question,

1845
01:21:19,880 --> 01:21:22,880
but you've cited the majority or a large fraction.

1846
01:21:22,880 --> 01:21:29,280
So what I would say is like there's the larger bunch of people

1847
01:21:29,280 --> 01:21:33,480
is people who are either unaware.

1848
01:21:33,480 --> 01:21:36,080
A lot of people just don't know what's going on.

1849
01:21:36,080 --> 01:21:38,480
One of the things that I find amazing is all the people saying like

1850
01:21:38,480 --> 01:21:41,680
there's no cancel culture.

1851
01:21:41,680 --> 01:21:44,480
After all the people who have their lives destroyed, right,

1852
01:21:44,480 --> 01:21:48,280
like, you know, or like, oh, this is just a few entitled professors

1853
01:21:48,280 --> 01:21:49,680
worrying about their privileges.

1854
01:21:49,680 --> 01:21:52,480
And what about the journalists and the high school students

1855
01:21:52,480 --> 01:21:53,280
who have been canceled, right?

1856
01:21:53,280 --> 01:21:56,280
But unfortunately, there's a lot of people who don't know what's going on.

1857
01:21:56,280 --> 01:21:58,080
Again, it goes back to like the two movies, right?

1858
01:21:58,080 --> 01:22:00,080
A lot of people are only seeing one movie, right?

1859
01:22:00,080 --> 01:22:02,280
If you can deny that there were riots this summer,

1860
01:22:02,280 --> 01:22:04,880
why can't you deny that there's cancel culture, right?

1861
01:22:04,880 --> 01:22:07,480
So there's a lot of people who are oblivious, right?

1862
01:22:07,480 --> 01:22:09,280
And then there's a lot of people

1863
01:22:09,280 --> 01:22:11,280
and I don't know which of these two groups is larger

1864
01:22:11,280 --> 01:22:17,880
and it varies, who they are not oblivious, but they're afraid to speak up.

1865
01:22:17,880 --> 01:22:22,680
If all that we do is raise awareness among the people

1866
01:22:22,680 --> 01:22:27,280
who are oblivious so that they no longer are and, you know,

1867
01:22:27,280 --> 01:22:31,680
lower the bar for how much courage it needs to speak up,

1868
01:22:31,680 --> 01:22:33,480
then we're winning.

1869
01:22:33,480 --> 01:22:37,080
I think the true fanatics are actually very few.

1870
01:22:37,080 --> 01:22:38,480
On both sides.

1871
01:22:38,480 --> 01:22:39,280
Yeah, on both sides.

1872
01:22:39,280 --> 01:22:41,680
There's no doubt that the true fanatics are very few.

1873
01:22:41,680 --> 01:22:43,680
But the problem, the whole problem,

1874
01:22:43,680 --> 01:22:47,480
is that they have a disproportionate amount of power.

1875
01:22:47,480 --> 01:22:49,880
Part of why they have that power is what I was just saying.

1876
01:22:49,880 --> 01:22:51,080
They show up.

1877
01:22:51,080 --> 01:22:54,480
They systematically take control of these organizations,

1878
01:22:54,480 --> 01:22:56,080
including scientific ones,

1879
01:22:56,080 --> 01:22:59,080
and then they say, science says.

1880
01:22:59,080 --> 01:23:00,680
Science is saddled.

1881
01:23:00,680 --> 01:23:02,280
Yeah, because they took control.

1882
01:23:02,280 --> 01:23:04,080
They took the trouble to take control, right?

1883
01:23:04,080 --> 01:23:05,680
Again, that's what they do for a living, you know,

1884
01:23:05,680 --> 01:23:07,680
like some of us do research for a living,

1885
01:23:07,680 --> 01:23:09,880
others, you know, do this, right?

1886
01:23:09,880 --> 01:23:12,880
And then they are able to manipulate

1887
01:23:12,880 --> 01:23:13,880
the other people around them.

1888
01:23:13,880 --> 01:23:15,880
Some of them, they convince them with kind of like

1889
01:23:15,880 --> 01:23:18,080
these superficial arguments, right?

1890
01:23:18,080 --> 01:23:20,880
There was this, you know, very famous category of people

1891
01:23:20,880 --> 01:23:23,080
in the Soviet Union, right, which is now forgotten,

1892
01:23:23,080 --> 01:23:26,680
which was the useful idiot.

1893
01:23:26,680 --> 01:23:29,280
There are a lot of, it pains me to say this,

1894
01:23:29,280 --> 01:23:31,880
but there are a lot of useful idiots in America today.

1895
01:23:31,880 --> 01:23:33,280
And Canada.

1896
01:23:33,280 --> 01:23:36,480
Yeah, I mean, in many other countries, right?

1897
01:23:36,480 --> 01:23:38,480
In Britain, you know, Australia, et cetera, et cetera,

1898
01:23:38,480 --> 01:23:40,880
like more in the English speaking world, but yeah.

1899
01:23:40,880 --> 01:23:42,880
What is a useful idiot is someone who like

1900
01:23:42,880 --> 01:23:44,880
does the bidding of the radicals

1901
01:23:44,880 --> 01:23:47,480
without realizing that they're being apt,

1902
01:23:47,480 --> 01:23:49,080
they're being duped.

1903
01:23:49,080 --> 01:23:51,480
And you know, like I could pick out random, you know,

1904
01:23:51,480 --> 01:23:53,880
professors from academia and like, you know,

1905
01:23:53,880 --> 01:23:56,480
every, you know, every other one of them

1906
01:23:56,480 --> 01:23:58,880
would be a useful idiot, unfortunately.

1907
01:23:58,880 --> 01:24:01,080
But the thing, but the good news is like,

1908
01:24:01,080 --> 01:24:03,680
you know, useful idiots are not, you know,

1909
01:24:03,680 --> 01:24:06,880
they're not typically not stupid, at least not in academia.

1910
01:24:06,880 --> 01:24:09,680
They're not, you know, it's not that people are

1911
01:24:11,680 --> 01:24:13,880
intrinsically stupid or something like that.

1912
01:24:13,880 --> 01:24:16,280
They just haven't been paying attention, right?

1913
01:24:16,280 --> 01:24:19,280
The thing in academia is that you have a lot of people

1914
01:24:19,280 --> 01:24:22,080
who are extremely smart, but they,

1915
01:24:22,080 --> 01:24:23,680
I mean, first of all, quick parenthesis,

1916
01:24:23,680 --> 01:24:24,880
they tend to forget that they're not

1917
01:24:24,880 --> 01:24:26,280
the only smart people in the world, right?

1918
01:24:26,280 --> 01:24:28,680
One of the things that feeds this is like this perception

1919
01:24:28,680 --> 01:24:32,280
of like, we know the masses are stupid

1920
01:24:32,280 --> 01:24:33,480
and we know what's good for them.

1921
01:24:33,480 --> 01:24:35,080
This drives me nuts, right?

1922
01:24:35,080 --> 01:24:36,480
Or better, basically.

1923
01:24:36,480 --> 01:24:38,680
Yeah, the masses are more intelligent

1924
01:24:38,680 --> 01:24:40,680
than you can imagine, right?

1925
01:24:40,680 --> 01:24:43,280
It's like, you know, I may be smarter

1926
01:24:43,280 --> 01:24:46,680
than some random person on the streets, right?

1927
01:24:46,680 --> 01:24:50,280
But there's like, you know, but that's not the question.

1928
01:24:50,280 --> 01:24:52,880
The question is like, how much of the total intelligence

1929
01:24:52,880 --> 01:24:54,880
of humanity is my intelligence?

1930
01:24:54,880 --> 01:24:57,080
Even if I'm 10 times smarter than the other guy

1931
01:24:57,080 --> 01:25:00,080
on the street, I'm still only one 700 million

1932
01:25:00,080 --> 01:25:01,280
of the intelligence, but it's like,

1933
01:25:01,280 --> 01:25:03,080
it doesn't bear comparison, right?

1934
01:25:03,080 --> 01:25:04,280
That's why markets work better

1935
01:25:04,280 --> 01:25:05,680
than centralized governments and so on.

1936
01:25:05,680 --> 01:25:07,480
So that's one parenthesis.

1937
01:25:07,480 --> 01:25:10,680
But the other side of this is that the problem

1938
01:25:10,680 --> 01:25:12,680
in academia is that you have these very smart people

1939
01:25:12,680 --> 01:25:15,280
that have invested their intelligence

1940
01:25:15,280 --> 01:25:18,480
in a very narrow field.

1941
01:25:18,480 --> 01:25:19,880
And that's all they know about.

1942
01:25:19,880 --> 01:25:22,280
And that's, it's necessary, right?

1943
01:25:22,280 --> 01:25:24,480
You could say it's a necessary evil, right?

1944
01:25:24,480 --> 01:25:26,280
But that's how they get to be good at it.

1945
01:25:26,280 --> 01:25:29,880
And in the process, they are actually extremely ignorant

1946
01:25:29,880 --> 01:25:32,680
about a lot of other things.

1947
01:25:32,680 --> 01:25:37,080
But they have the arrogance of knowing that they're smart.

1948
01:25:37,080 --> 01:25:40,080
And then like people from other fields, right,

1949
01:25:40,080 --> 01:25:41,880
come and say something to them.

1950
01:25:41,880 --> 01:25:45,480
And then they, I mean, I've had this experience like over

1951
01:25:45,480 --> 01:25:48,080
and over again talking with colleagues of mine.

1952
01:25:48,080 --> 01:25:50,480
There are very smart people, right?

1953
01:25:50,480 --> 01:25:52,080
Talking about these issues like, you know,

1954
01:25:52,080 --> 01:25:53,880
these sociological issues like, you know,

1955
01:25:53,880 --> 01:25:55,480
should there be preferences, you know,

1956
01:25:55,480 --> 01:25:58,480
should we do affirmative action in hiring and so on and so forth.

1957
01:25:58,480 --> 01:26:01,880
And immediately I noticed that like they do not know the first

1958
01:26:01,880 --> 01:26:04,680
thing about what is being debated.

1959
01:26:04,680 --> 01:26:07,280
And the ones who took the time to persuade them, you know,

1960
01:26:07,280 --> 01:26:08,280
prevail.

1961
01:26:08,280 --> 01:26:11,480
Another great example of this is climate science, right?

1962
01:26:11,480 --> 01:26:13,680
Climate science, it's physical science, right?

1963
01:26:13,680 --> 01:26:14,680
The science is settled.

1964
01:26:14,680 --> 01:26:17,280
It's not even remotely settled.

1965
01:26:17,280 --> 01:26:20,480
But I've lost count of the number of fellow scientists

1966
01:26:20,480 --> 01:26:22,880
who are not climate scientists, right,

1967
01:26:22,880 --> 01:26:24,480
who think they know climate science.

1968
01:26:24,480 --> 01:26:28,680
But in fact, all they know is what they've read in the media.

1969
01:26:28,680 --> 01:26:31,880
You know, to politicize narrative about that topic.

1970
01:26:31,880 --> 01:26:38,880
Yeah, but then they expound it on the back of their scientific

1971
01:26:38,880 --> 01:26:40,280
expertise.

1972
01:26:40,280 --> 01:26:41,680
It's like, I'm a scientist.

1973
01:26:41,680 --> 01:26:45,480
I know that XYZ will actually know you don't, right?

1974
01:26:45,480 --> 01:26:48,080
Go read the papers in Nature Science, blah, blah, blah, blah.

1975
01:26:48,080 --> 01:26:50,280
Because they never did, right?

1976
01:26:50,280 --> 01:26:53,280
I mean, again, you know, I'm of a skeptical bent when this

1977
01:26:53,280 --> 01:26:54,880
whole climate thing started happening.

1978
01:26:54,880 --> 01:26:56,880
At first I read things and I was very alarmed.

1979
01:26:56,880 --> 01:27:01,280
And I said, let me go read some of those IPCC reports.

1980
01:27:01,280 --> 01:27:03,480
First I read the executive summaries and then section of

1981
01:27:03,480 --> 01:27:05,880
the reports and then actually went and started reading,

1982
01:27:05,880 --> 01:27:08,080
you know, the actual papers.

1983
01:27:08,080 --> 01:27:13,080
And every level of this that you go creates a much different

1984
01:27:13,080 --> 01:27:16,880
picture from the one that the activists created for you.

1985
01:27:16,880 --> 01:27:19,880
But most scientists, again, they've never even read the

1986
01:27:19,880 --> 01:27:22,680
executive summary of the IPCC reports.

1987
01:27:22,680 --> 01:27:26,280
They just have this story that was told to them as it was told

1988
01:27:26,280 --> 01:27:27,480
to everybody else.

1989
01:27:27,480 --> 01:27:29,680
Whereas everybody else might have some modest, like, well,

1990
01:27:29,680 --> 01:27:30,680
I'm not a scientist.

1991
01:27:30,680 --> 01:27:32,080
I don't know, right?

1992
01:27:32,080 --> 01:27:34,280
You have someone who's a computer scientist that says like,

1993
01:27:34,280 --> 01:27:37,080
oh, you know, this is the science of climate science,

1994
01:27:37,080 --> 01:27:37,680
right?

1995
01:27:37,680 --> 01:27:39,480
But they don't know anything about it.

1996
01:27:39,480 --> 01:27:44,280
But then they bring their, you know, self-image as a superior

1997
01:27:44,280 --> 01:27:48,680
intelligence to this debate, you know, even though paradoxically

1998
01:27:48,680 --> 01:27:51,480
they actually know much less about this particular issue than

1999
01:27:51,480 --> 01:27:54,880
a lot of people who are just engineers or whoever, right, who

2000
01:27:54,880 --> 01:27:57,080
actually took the time to look into it.

2001
01:27:57,080 --> 01:28:01,080
It's such a perfect kind of a facade to the climate science

2002
01:28:01,080 --> 01:28:04,480
and climate change because I was reading about it two days

2003
01:28:04,480 --> 01:28:09,280
ago how they're targeting Bitcoin now right after Tesla

2004
01:28:09,280 --> 01:28:12,080
invested more than a billion dollars in Bitcoin and it's an

2005
01:28:12,080 --> 01:28:15,880
alternative monetary financial system, basically.

2006
01:28:15,880 --> 01:28:17,280
And now they're targeting it.

2007
01:28:17,280 --> 01:28:21,280
It creates a lot of heat and energy waste more than Argentina.

2008
01:28:21,280 --> 01:28:23,880
So the game is so obvious.

2009
01:28:23,880 --> 01:28:26,680
But I think one of the good news actually is that they're

2010
01:28:26,680 --> 01:28:31,680
overplaying their hand in such a rate that more and more people

2011
01:28:31,680 --> 01:28:35,980
are realizing it because as you just said earlier, it's affecting

2012
01:28:35,980 --> 01:28:38,780
their kids and their future and their education.

2013
01:28:38,780 --> 01:28:42,680
France, for example, came out and said this woke ism out of

2014
01:28:42,680 --> 01:28:46,380
control leftism in the US is a problem for us because it

2015
01:28:46,380 --> 01:28:49,080
targets our nationality and identity and all of that.

2016
01:28:49,080 --> 01:28:52,280
So maybe that's a good news that, you know, we kind of feel

2017
01:28:52,280 --> 01:28:54,680
like they they've gone completely out of their minds,

2018
01:28:54,680 --> 01:28:57,280
but they really show who they are to more and more people.

2019
01:28:57,680 --> 01:29:00,780
No, I actually historically you see this happening, right?

2020
01:29:00,780 --> 01:29:04,080
McCarthyism ended when McCarthy overplayed his hand by attacking

2021
01:29:04,080 --> 01:29:07,280
the army and people like well at long last that, you know,

2022
01:29:07,280 --> 01:29:08,680
decency, right?

2023
01:29:08,680 --> 01:29:11,180
So a lot of these witch hunts and so on a lot of these

2024
01:29:11,180 --> 01:29:14,180
movements they do fail when they overplay their hand because

2025
01:29:14,180 --> 01:29:17,480
again the people inside the movement don't see just how they

2026
01:29:17,480 --> 01:29:19,680
diverging from reality, right?

2027
01:29:19,680 --> 01:29:22,080
And I think some of that will happen here, right?

2028
01:29:22,080 --> 01:29:24,980
And you know, perversely in that regards, you know, the sooner

2029
01:29:24,980 --> 01:29:26,180
they overplay their hand the better.

2030
01:29:26,180 --> 01:29:29,380
So let them go hogwash, you know, you know, let you know provoke

2031
01:29:29,380 --> 01:29:32,480
them to be even more extreme such that this will illustrate

2032
01:29:32,480 --> 01:29:34,180
to people just how off-base they are.

2033
01:29:35,380 --> 01:29:38,780
Unfortunately, the downside and maybe that's what will happen,

2034
01:29:38,780 --> 01:29:39,080
right?

2035
01:29:39,080 --> 01:29:40,680
I think that they will probably be part of it.

2036
01:29:40,680 --> 01:29:44,680
It's already happening in some realms that that the downside

2037
01:29:44,680 --> 01:29:47,280
of that is that it's a very high-cost way of bringing this

2038
01:29:47,280 --> 01:29:48,280
to an end, right?

2039
01:29:48,380 --> 01:29:49,880
Because while they're overplaying their hand, they're

2040
01:29:49,880 --> 01:29:51,080
actually hurting the people.

2041
01:29:51,080 --> 01:29:54,980
So I'd rather, you know, stop this before that level of damage

2042
01:29:54,980 --> 01:29:55,480
happens.

2043
01:29:55,480 --> 01:29:59,080
I think at this point some level of that damage is unavoidable

2044
01:29:59,080 --> 01:30:00,280
and is already happening.

2045
01:30:01,780 --> 01:30:02,980
Sorry, my dogs.

2046
01:30:05,780 --> 01:30:09,180
Let me read one other quote from your book and I have maybe

2047
01:30:09,180 --> 01:30:10,480
two or three more questions.

2048
01:30:10,480 --> 01:30:11,780
Thank you so much for your time.

2049
01:30:11,780 --> 01:30:12,980
This has been an honor.

2050
01:30:14,380 --> 01:30:17,480
Your quote is you could even say that the God of Genesis

2051
01:30:17,480 --> 01:30:20,680
himself is a programmer language, not manipulation is

2052
01:30:20,680 --> 01:30:22,280
his tool of creation.

2053
01:30:22,280 --> 01:30:28,880
Words become worlds beautiful and should I assume that you

2054
01:30:28,880 --> 01:30:33,980
believe in the simulation hypothesis?

2055
01:30:35,980 --> 01:30:38,980
I mean just to give you the context for that quote, right?

2056
01:30:38,980 --> 01:30:40,880
I'm talking about being a programmer.

2057
01:30:41,980 --> 01:30:46,280
Right and and how being a programmer is it is that heart

2058
01:30:46,280 --> 01:30:47,880
a creative activity, right?

2059
01:30:47,880 --> 01:30:51,780
The thing that's beautiful about computer science is this

2060
01:30:51,780 --> 01:30:53,280
is more than anything else.

2061
01:30:53,280 --> 01:30:57,380
Why I went into computer science is there's so much range

2062
01:30:57,380 --> 01:30:58,180
to create.

2063
01:30:58,980 --> 01:31:02,080
You can create a movie, a piece of music, you can create

2064
01:31:02,080 --> 01:31:05,980
a book, but with programs with computers, you can create

2065
01:31:05,980 --> 01:31:06,580
the world.

2066
01:31:08,180 --> 01:31:10,680
Not just, you know, a world that you describe on the page,

2067
01:31:10,680 --> 01:31:14,380
but actually a real moving world that people can interact

2068
01:31:14,380 --> 01:31:18,080
with and you can create a very very rich world.

2069
01:31:18,280 --> 01:31:20,980
Now the downside of that is that as every programmer knows

2070
01:31:20,980 --> 01:31:22,080
is that it's hard, right?

2071
01:31:22,080 --> 01:31:22,980
You know, there's bugs.

2072
01:31:22,980 --> 01:31:25,280
There's like, you know, you will spend a lot of your time

2073
01:31:25,280 --> 01:31:27,280
just, you know, shoveling crap and whatnot.

2074
01:31:27,280 --> 01:31:30,180
But at the end of the day a programmer is a miniature God.

2075
01:31:31,380 --> 01:31:34,780
Right and I think a lot of people unfortunately don't

2076
01:31:34,780 --> 01:31:37,180
go into computer science because they get exposed to

2077
01:31:37,180 --> 01:31:39,880
like computer science 101, which is, you know, learning

2078
01:31:39,880 --> 01:31:43,580
how to program in Java, which is all the unpleasant stuff

2079
01:31:43,580 --> 01:31:44,680
and none of the interesting.

2080
01:31:46,080 --> 01:31:49,080
It's like, you know, it's like if you design a course to

2081
01:31:49,080 --> 01:31:51,580
make computer science unappealing to people, it would

2082
01:31:51,580 --> 01:31:54,480
be what most, you know, programming 101 courses are.

2083
01:31:55,280 --> 01:31:57,580
So, you know, in part of what I'm trying to explain in

2084
01:31:57,580 --> 01:32:00,380
that, in that chapter, right, is like the larger context

2085
01:32:00,380 --> 01:32:04,580
of computer science around, around AI and why, you know,

2086
01:32:04,580 --> 01:32:06,680
and why there's so much power, right?

2087
01:32:07,080 --> 01:32:09,680
I literally, the whole reason why computer science is

2088
01:32:09,680 --> 01:32:13,180
powerful is that once I have taken that trouble of creating

2089
01:32:13,180 --> 01:32:17,080
that, you know, world, small or large, it will then work

2090
01:32:17,080 --> 01:32:18,680
by itself, right?

2091
01:32:19,080 --> 01:32:21,580
That's why you can have, you know, WhatsApp, a company

2092
01:32:21,580 --> 01:32:23,980
created by, you know, a dozen people in a couple of years,

2093
01:32:23,980 --> 01:32:26,880
you know, being sold to Facebook for what, 12 million,

2094
01:32:26,880 --> 01:32:28,480
12 billion dollars, right?

2095
01:32:28,580 --> 01:32:30,780
How can that small number of people make that big of

2096
01:32:30,780 --> 01:32:31,280
a difference?

2097
01:32:31,380 --> 01:32:32,680
It's because of this, right?

2098
01:32:32,780 --> 01:32:35,380
And so I'm making an analysis and then interestingly,

2099
01:32:35,480 --> 01:32:38,480
right, the God of Genesis, right, the whole story of

2100
01:32:38,480 --> 01:32:40,380
Genesis starts with the word, right?

2101
01:32:40,380 --> 01:32:41,680
In the beginning was the word.

2102
01:32:41,680 --> 01:32:44,580
So the idea is that like, you know, we tend to think of

2103
01:32:44,580 --> 01:32:47,580
language as something that originates in the world after,

2104
01:32:47,580 --> 01:32:49,580
you know, many things that went before, right?

2105
01:32:49,680 --> 01:32:51,880
But you can also look at it the other way, where, you know,

2106
01:32:51,880 --> 01:32:53,780
like there are a lot of words that start with language

2107
01:32:53,780 --> 01:32:55,580
because they're defined in language, right?

2108
01:32:55,780 --> 01:32:58,280
Now, of course, the simulation hypothesis that, yeah, we

2109
01:32:58,280 --> 01:32:59,980
are all in a simulation, right?

2110
01:33:00,380 --> 01:33:05,580
And I was very fascinated that that hypothesis for about

2111
01:33:05,580 --> 01:33:08,880
six months when I was maybe 20, right?

2112
01:33:08,880 --> 01:33:12,680
And then because it is kind of really fascinating, right?

2113
01:33:12,980 --> 01:33:15,580
But then after a while you realize the following is well,

2114
01:33:16,480 --> 01:33:18,780
I don't know if this hypothesis is true or false, but either

2115
01:33:18,780 --> 01:33:21,880
why you have to test it or I don't, right?

2116
01:33:22,480 --> 01:33:25,380
And I almost by definition, I can't think of a way to test

2117
01:33:25,380 --> 01:33:27,180
that hypothesis, right?

2118
01:33:27,380 --> 01:33:29,680
If there's a way to test it by God, we should be all over

2119
01:33:29,680 --> 01:33:32,180
it. But if there isn't a way to test it, then you know,

2120
01:33:32,180 --> 01:33:33,680
it's pointless to worry about it.

2121
01:33:34,480 --> 01:33:36,080
Maybe we're on a computer tomorrow.

2122
01:33:36,080 --> 01:33:38,680
Someone will pull the plug, but you know, I have no way

2123
01:33:38,680 --> 01:33:42,280
of knowing. So in a way, the simulation hypothesis is a

2124
01:33:42,280 --> 01:33:44,880
metaphysical hypothesis because it's not something we can

2125
01:33:44,880 --> 01:33:46,980
actually think much.

2126
01:33:46,980 --> 01:33:50,680
I mean, we can, it's a great motif for like scientific,

2127
01:33:50,680 --> 01:33:53,380
you know, speculation and science fiction and whatnot.

2128
01:33:53,780 --> 01:33:56,980
But you know, my point of view about a lot of these questions

2129
01:33:56,980 --> 01:33:58,980
is about, you know, as things like, you know, does God exist

2130
01:33:58,980 --> 01:34:03,180
and whatnot is like we do not have the ability, you know,

2131
01:34:03,580 --> 01:34:06,880
language gives us the ability to ask more questions than

2132
01:34:06,880 --> 01:34:07,680
we can answer.

2133
01:34:07,680 --> 01:34:08,880
And this is one of them.

2134
01:34:10,980 --> 01:34:15,380
I find it really helpful to think of maybe smaller

2135
01:34:15,780 --> 01:34:20,080
simulation models that the society that we are experiencing

2136
01:34:20,080 --> 01:34:23,380
it doesn't necessarily have to be a digital simulation.

2137
01:34:23,980 --> 01:34:27,980
Just the reality that has been created for many, many

2138
01:34:27,980 --> 01:34:31,780
centuries, I would say, by people, power holders that,

2139
01:34:31,780 --> 01:34:34,080
you know, people were reading news from very specific kind

2140
01:34:34,080 --> 01:34:38,580
of sources and media was kind of projecting a specific kind

2141
01:34:38,580 --> 01:34:39,180
of a message.

2142
01:34:39,180 --> 01:34:43,380
So a true man show like kind of a life that most people are

2143
01:34:43,380 --> 01:34:46,380
experiencing itself can be looked at at some kind of a

2144
01:34:46,380 --> 01:34:47,380
simulation, right?

2145
01:34:48,180 --> 01:34:48,980
I agree with that.

2146
01:34:48,980 --> 01:34:54,280
So part of what every society does, every culture is

2147
01:34:54,280 --> 01:34:55,380
constructive reality.

2148
01:34:56,680 --> 01:34:58,880
People in different societies truly live in different

2149
01:34:58,880 --> 01:34:59,380
realities.

2150
01:34:59,580 --> 01:35:01,980
People in the Middle Ages just live in a different reality

2151
01:35:01,980 --> 01:35:05,680
from the world was the same, but their reality was completely

2152
01:35:05,680 --> 01:35:09,180
different based on their need and objectives and experience.

2153
01:35:09,580 --> 01:35:12,280
No, I'm like, you know, they believe that, you know, I

2154
01:35:12,280 --> 01:35:15,780
think even at least, you know, in the West, even people

2155
01:35:15,780 --> 01:35:18,780
who are very strong believers in the Christian God do not

2156
01:35:18,780 --> 01:35:21,580
believe in him the same way that people in the Middle Ages

2157
01:35:22,080 --> 01:35:24,980
in the Middle Ages, like God was with you every second

2158
01:35:24,980 --> 01:35:27,980
of the day or like like some cultures believe that your

2159
01:35:27,980 --> 01:35:29,480
ancestors are watching over you.

2160
01:35:29,480 --> 01:35:32,680
And it's like you really are every day thinking about what

2161
01:35:32,680 --> 01:35:34,680
your ancestors are doing looking at you, right?

2162
01:35:35,080 --> 01:35:37,980
And you could debate to what extent there is metaphorical

2163
01:35:37,980 --> 01:35:40,480
or not to what extent people really do believe in these

2164
01:35:40,480 --> 01:35:42,380
gods the same way they believe in reality.

2165
01:35:42,580 --> 01:35:46,580
But the bottom line is this is informs what they think every

2166
01:35:46,580 --> 01:35:47,980
every moment of the day, right?

2167
01:35:48,380 --> 01:35:52,980
And I think the society that we have today is actually

2168
01:35:52,980 --> 01:35:54,980
no exception, right?

2169
01:35:54,980 --> 01:35:57,380
In fact, there was a philosopher I forget who that said

2170
01:35:57,380 --> 01:35:58,980
something that I thought was incredibly insightful.

2171
01:35:58,980 --> 01:36:03,480
Which was that science is the mode of perception of

2172
01:36:03,480 --> 01:36:04,580
industrial society.

2173
01:36:06,280 --> 01:36:07,880
It is also a mode of perception.

2174
01:36:08,380 --> 01:36:11,380
It is one that is particularly good at being in tune with

2175
01:36:11,380 --> 01:36:11,980
reality.

2176
01:36:11,980 --> 01:36:14,480
And that's what makes it powerful, but it's still a mode

2177
01:36:14,480 --> 01:36:16,580
of perception that against science a hundred years from

2178
01:36:16,580 --> 01:36:18,980
now could look very different from science now.

2179
01:36:19,180 --> 01:36:22,880
In fact, science now is already, you know, the scientific

2180
01:36:22,880 --> 01:36:25,580
mode of perception today is very different from the scientific

2181
01:36:25,580 --> 01:36:27,580
mode of perception 200 years ago.

2182
01:36:27,580 --> 01:36:31,080
So this is part of what science society societies do is

2183
01:36:31,080 --> 01:36:32,180
they construct this reality.

2184
01:36:32,180 --> 01:36:35,080
In fact, your brain constructs a reality, right?

2185
01:36:35,080 --> 01:36:39,380
Vision researchers say that vision is controlled hallucination.

2186
01:36:40,880 --> 01:36:42,280
And this is exactly right, right?

2187
01:36:42,280 --> 01:36:44,780
Like, you know, you don't actually see things.

2188
01:36:45,480 --> 01:36:48,280
You see what your brain has conjured up in response to

2189
01:36:48,280 --> 01:36:49,080
the stimuli.

2190
01:36:50,180 --> 01:36:52,180
But every object that you see in the world is actually a

2191
01:36:52,180 --> 01:36:53,980
creation of your brain, right?

2192
01:36:53,980 --> 01:36:56,280
That hopefully lines up with something that's in the world,

2193
01:36:56,280 --> 01:36:58,280
but you only interfaces those photons.

2194
01:36:58,580 --> 01:37:00,680
So there is an objective reality.

2195
01:37:00,680 --> 01:37:03,780
We are having a subjective experience of it.

2196
01:37:04,080 --> 01:37:07,280
And meanwhile, there are people who are trying to define

2197
01:37:07,280 --> 01:37:10,380
and orbit those subjective experiences in an objective

2198
01:37:10,380 --> 01:37:11,080
kind of a way.

2199
01:37:11,780 --> 01:37:11,980
Yeah.

2200
01:37:11,980 --> 01:37:14,180
So throughout human history.

2201
01:37:14,680 --> 01:37:15,080
Yeah.

2202
01:37:15,080 --> 01:37:19,680
And you know, to answer this question squarely, is there

2203
01:37:19,680 --> 01:37:20,880
an objective reality?

2204
01:37:21,580 --> 01:37:23,180
I believe that there is.

2205
01:37:23,180 --> 01:37:27,180
I also believe that just as the simulation there is no final

2206
01:37:27,180 --> 01:37:30,680
test that you can use to say, oh, this is the objective

2207
01:37:30,680 --> 01:37:31,680
reality, right?

2208
01:37:31,680 --> 01:37:35,280
Precisely because we do not have direct access to it, right?

2209
01:37:35,280 --> 01:37:37,880
But now, you know, here's a crucial question, right?

2210
01:37:37,880 --> 01:37:40,080
Which gets to the whole, you know, social problem around

2211
01:37:40,080 --> 01:37:42,780
this is, are we better off?

2212
01:37:43,580 --> 01:37:47,280
Assuming that there's an objective reality and we should

2213
01:37:47,280 --> 01:37:50,780
try to find out what it is and converge on it, try to agree.

2214
01:37:50,780 --> 01:37:54,480
The point of there being an objective reality is that you

2215
01:37:54,480 --> 01:37:56,480
and I should be able to agree on it.

2216
01:37:57,580 --> 01:38:01,080
I don't have access to it regardless of our state.

2217
01:38:01,380 --> 01:38:01,880
Yeah.

2218
01:38:02,080 --> 01:38:06,480
And if we disagree, there should be a test that shows, oh,

2219
01:38:06,480 --> 01:38:06,880
you're right.

2220
01:38:06,880 --> 01:38:07,380
I'm wrong.

2221
01:38:07,380 --> 01:38:09,480
So now I will change my beliefs, right?

2222
01:38:10,180 --> 01:38:13,980
Once you do like, you know, these postmodernists and critical

2223
01:38:13,980 --> 01:38:16,880
theorists and whatnot and say like, oh, no, no, no, no, there's

2224
01:38:16,880 --> 01:38:17,980
no objective reality.

2225
01:38:17,980 --> 01:38:20,280
There's just everybody has their own, you know, you have

2226
01:38:20,280 --> 01:38:21,580
your own knowledge, right?

2227
01:38:22,280 --> 01:38:25,880
The problem with this is that it destroys the fabric of society.

2228
01:38:26,680 --> 01:38:30,580
It means we are no longer trying to converge on something, right?

2229
01:38:30,680 --> 01:38:33,480
There's no longer, you know, it's like, you know, there's

2230
01:38:33,480 --> 01:38:40,280
this hyping of like the lived experience, right?

2231
01:38:40,480 --> 01:38:42,780
The lived experience is more important than science.

2232
01:38:42,780 --> 01:38:45,380
I mean, I understand we all have a lived experience, but the

2233
01:38:45,380 --> 01:38:49,880
problem with that is that if that's the criteria, then, you

2234
01:38:49,880 --> 01:38:51,380
know, we can't talk anymore.

2235
01:38:51,780 --> 01:38:54,480
My lived experience cannot be refuted by you.

2236
01:38:55,680 --> 01:38:59,080
If I say you were discriminating against me, it's my lived

2237
01:38:59,080 --> 01:38:59,680
experience.

2238
01:38:59,680 --> 01:39:00,880
You're not allowed to disagree.

2239
01:39:01,080 --> 01:39:01,580
Right.

2240
01:39:01,980 --> 01:39:02,280
Right.

2241
01:39:02,480 --> 01:39:05,180
If we agree that there's an objective reality, we go like,

2242
01:39:05,280 --> 01:39:07,880
okay, so what's the evidence that you're being discriminated

2243
01:39:07,880 --> 01:39:08,380
against?

2244
01:39:08,780 --> 01:39:09,080
Right.

2245
01:39:09,280 --> 01:39:11,480
And again, I've had the experience, you know, over the

2246
01:39:11,480 --> 01:39:13,080
years of talking with a lot of people about this.

2247
01:39:13,080 --> 01:39:16,580
And the problem is that a lot of these beliefs are not amenable

2248
01:39:16,580 --> 01:39:17,980
to evidence, right?

2249
01:39:17,980 --> 01:39:21,880
It's like, there's no, again, at which point they're kind of

2250
01:39:21,880 --> 01:39:23,580
like a religious belief, right?

2251
01:39:24,280 --> 01:39:27,180
And again, but if you believe that everybody has their reality,

2252
01:39:27,180 --> 01:39:27,980
then that's fine.

2253
01:39:27,980 --> 01:39:31,080
But you can, but my point is like, there's the scientific

2254
01:39:31,080 --> 01:39:32,680
question of is there one or not?

2255
01:39:32,680 --> 01:39:35,080
And I think, you know, there is, but I can't prove it.

2256
01:39:35,280 --> 01:39:38,380
But there's also, and maybe less important to scientists, but

2257
01:39:38,380 --> 01:39:41,080
more important to society than the utilitarian question, which

2258
01:39:41,080 --> 01:39:43,480
is, are we better off believing that there's an objective

2259
01:39:43,480 --> 01:39:44,180
reality or not?

2260
01:39:44,180 --> 01:39:46,680
And I definitely say we are better off in believing that

2261
01:39:46,680 --> 01:39:47,880
there's an objective reality.

2262
01:39:48,180 --> 01:39:50,880
I think better or worse will be determined based on the

2263
01:39:50,880 --> 01:39:54,180
context and our objective, the goal that we are trying to

2264
01:39:54,180 --> 01:39:54,680
reach.

2265
01:39:54,680 --> 01:39:57,180
But I think that's beside the point of what these people,

2266
01:39:57,180 --> 01:39:59,880
postmodernists or whoever they, which to me, they're just

2267
01:39:59,880 --> 01:40:00,780
totalitarians.

2268
01:40:00,780 --> 01:40:01,980
That's what they are.

2269
01:40:02,480 --> 01:40:04,880
I mean, they are, because then they use this as an excuse

2270
01:40:04,880 --> 01:40:05,880
to impose their view.

2271
01:40:05,880 --> 01:40:09,280
But you know, you said very well that it depends on the

2272
01:40:09,280 --> 01:40:09,680
goal.

2273
01:40:10,080 --> 01:40:13,180
But let's just say that, you know, we can all agree that,

2274
01:40:13,480 --> 01:40:15,880
you know, life is better than death and health is better

2275
01:40:15,880 --> 01:40:17,380
than sickness and et cetera, et cetera.

2276
01:40:18,580 --> 01:40:20,980
If you're in tune with objective reality, you can have

2277
01:40:20,980 --> 01:40:23,080
medicine and you can save people's lives.

2278
01:40:23,680 --> 01:40:26,980
And you can build cars and planes and you know, CRT scans

2279
01:40:26,980 --> 01:40:27,780
and you name it.

2280
01:40:28,680 --> 01:40:32,280
So being in tune with objective reality makes a big difference

2281
01:40:32,280 --> 01:40:33,880
to your well-being at the end of the day.

2282
01:40:33,880 --> 01:40:34,180
Yeah.

2283
01:40:35,280 --> 01:40:38,780
Again, well, the reason we are where we are today versus

2284
01:40:38,780 --> 01:40:41,780
the Middle Ages, right, is that we believe in this and

2285
01:40:41,780 --> 01:40:44,480
that's what has created all the scientific and technological

2286
01:40:44,480 --> 01:40:47,480
progress that these postmodernists not take for granted

2287
01:40:47,480 --> 01:40:49,680
without which they wouldn't even exist, right?

2288
01:40:49,780 --> 01:40:49,980
Yeah.

2289
01:40:49,980 --> 01:40:52,280
And their depth of hypocrisy is that they're saying there

2290
01:40:52,280 --> 01:40:55,380
is no objective experience, but then they want to make

2291
01:40:55,380 --> 01:40:58,180
that subjective opinion into an objective experience.

2292
01:40:58,880 --> 01:40:59,080
Yeah.

2293
01:40:59,080 --> 01:41:01,280
Again, there were generations of postmodernists.

2294
01:41:01,280 --> 01:41:04,180
The early generation was, you know, anything goes, right?

2295
01:41:04,180 --> 01:41:06,380
The derives and the Foucault's and whatnot.

2296
01:41:06,580 --> 01:41:09,280
But the current generation, of course, has perverted that

2297
01:41:09,280 --> 01:41:13,280
postmodernism into like, no, my truth is the truth.

2298
01:41:13,280 --> 01:41:16,480
Yeah, which, you know, the question is if my truth is that

2299
01:41:16,480 --> 01:41:17,380
you're full of shit.

2300
01:41:17,380 --> 01:41:19,080
I mean, what are we going to do then?

2301
01:41:19,280 --> 01:41:21,980
How are we going to settle it, which is a very dangerous

2302
01:41:21,980 --> 01:41:25,780
route because this kind of my truth, your truth, how are

2303
01:41:25,780 --> 01:41:26,580
we going to settle it?

2304
01:41:26,580 --> 01:41:28,380
It always leads to violence.

2305
01:41:28,980 --> 01:41:29,480
Exactly.

2306
01:41:29,480 --> 01:41:31,780
And again, this is what the postmodernists said.

2307
01:41:31,780 --> 01:41:34,880
And again, I mean, like if you start from the premise

2308
01:41:34,880 --> 01:41:38,280
that there's no objective reality, then what determines

2309
01:41:38,280 --> 01:41:39,580
what is the perceived reality?

2310
01:41:39,580 --> 01:41:41,180
Power, right?

2311
01:41:41,280 --> 01:41:42,580
It's who has the most power.

2312
01:41:42,580 --> 01:41:44,980
And in fact, you know, people have been saying this for

2313
01:41:44,980 --> 01:41:45,480
a while, right?

2314
01:41:45,480 --> 01:41:47,980
And of course, power influences your perception of reality.

2315
01:41:48,280 --> 01:41:50,680
But in a way, what you see today is like that taken to

2316
01:41:50,680 --> 01:41:52,080
its logical extreme, right?

2317
01:41:52,280 --> 01:41:54,280
And so these people, they want to take power because they

2318
01:41:54,280 --> 01:41:56,880
want to impose their reality on the rest of us, regardless

2319
01:41:56,880 --> 01:41:59,780
of whether it has anything to do with objective reality or not.

2320
01:42:00,180 --> 01:42:04,480
Do you also think this option, because I see it as a war

2321
01:42:04,480 --> 01:42:07,280
of meaning, because we experience certain things, but

2322
01:42:07,280 --> 01:42:10,580
the war is over how they define what we are experiencing.

2323
01:42:10,580 --> 01:42:15,580
Do you see this wokeism, this totalitarian approach to

2324
01:42:15,580 --> 01:42:21,480
postmodernism, however, you want to define it as a inevitable

2325
01:42:21,480 --> 01:42:27,380
and natural end of a fully secular liberal democratic

2326
01:42:27,380 --> 01:42:27,980
system?

2327
01:42:29,580 --> 01:42:31,580
That's actually a great question.

2328
01:42:31,580 --> 01:42:35,280
I think it's not an inevitable end, but unfortunately

2329
01:42:35,380 --> 01:42:38,080
once, so there was the enlightenment, right?

2330
01:42:38,080 --> 01:42:41,780
That created this mindset where, you know, there's a

2331
01:42:41,780 --> 01:42:44,180
marketplace of ideas, etc, etc, right?

2332
01:42:44,180 --> 01:42:46,680
And this is one of the best things that have happened to

2333
01:42:46,680 --> 01:42:47,280
humanity.

2334
01:42:47,280 --> 01:42:50,680
Again, it's why we have the life that we have today.

2335
01:42:50,680 --> 01:42:53,480
Unfortunately, it opens itself up.

2336
01:42:53,480 --> 01:42:58,880
The marketplace of ideas is open to bad ideas by definition,

2337
01:42:58,880 --> 01:42:58,980
right?

2338
01:42:58,980 --> 01:43:00,680
Because you don't know if they're good or bad going yet.

2339
01:43:00,680 --> 01:43:05,480
And then some bad ideas can acquire power, right?

2340
01:43:05,480 --> 01:43:09,580
And this started with the French Revolution, but maybe

2341
01:43:09,580 --> 01:43:11,780
the paradigmatic example is Marxism, right?

2342
01:43:11,780 --> 01:43:15,180
The thing about Marx is that he wasn't just a philosopher,

2343
01:43:15,180 --> 01:43:16,080
right?

2344
01:43:16,080 --> 01:43:20,080
He said we are going to have this praxis that is going to,

2345
01:43:20,080 --> 01:43:22,080
you know, first of all, he had these prophecies that were

2346
01:43:22,080 --> 01:43:24,680
wrong, but like this was like we're going to impose these

2347
01:43:24,680 --> 01:43:26,880
ideas on the people and then, you know, Lenin developed

2348
01:43:26,880 --> 01:43:27,380
that further.

2349
01:43:27,380 --> 01:43:31,480
And wokeism is really just the latest incarnation of this

2350
01:43:31,480 --> 01:43:31,980
problem.

2351
01:43:31,980 --> 01:43:35,680
It's not the first one and it will not be the last one.

2352
01:43:35,680 --> 01:43:38,280
Who knows what the next one will look like, but this problem

2353
01:43:38,280 --> 01:43:39,180
will always be with us.

2354
01:43:39,180 --> 01:43:44,680
It's like, you know, a liberal society is actually an amazingly

2355
01:43:44,680 --> 01:43:50,780
functioning organism, but it needs an immune system against

2356
01:43:50,780 --> 01:43:54,580
these, you know, diseases and every now and then, you know,

2357
01:43:54,580 --> 01:43:57,580
the immune system is not functioning very well in certain

2358
01:43:57,580 --> 01:44:00,280
context like the universe that we've talked about and then

2359
01:44:00,280 --> 01:44:04,480
you get the sepsis, you know, where the germs multiplied

2360
01:44:04,480 --> 01:44:07,180
very fast before the immune system can catch up with them.

2361
01:44:07,780 --> 01:44:10,580
And if you don't do something about it, you know, it does

2362
01:44:10,580 --> 01:44:12,180
kill the organism, right?

2363
01:44:12,580 --> 01:44:14,880
So it may well be that at the end of the day, we will come

2364
01:44:14,880 --> 01:44:17,380
crashing down because, you know, these people make it come

2365
01:44:17,380 --> 01:44:20,280
crashing down because they debase education so much that,

2366
01:44:20,280 --> 01:44:23,780
you know, you know, math is racist and, you know, you know,

2367
01:44:23,780 --> 01:44:26,380
two plus two equals five and you can't and, you know, and

2368
01:44:26,380 --> 01:44:29,480
your bridges, you know, don't stand up anymore and so on,

2369
01:44:29,480 --> 01:44:32,280
right? I hope it won't come to that, but I could see it

2370
01:44:32,280 --> 01:44:32,880
coming to that.

2371
01:44:33,180 --> 01:44:37,580
So this is a problem that will recur, right? And in different

2372
01:44:37,580 --> 01:44:39,880
forms, right? You know, like the cultural revolution was

2373
01:44:39,880 --> 01:44:43,080
similar to the October Revolution in some ways, but quite

2374
01:44:43,080 --> 01:44:46,080
different in some others and, you know, wokeism is different

2375
01:44:46,080 --> 01:44:47,380
from these in some ways.

2376
01:44:47,780 --> 01:44:51,580
One of the most important ones is that we have a technology

2377
01:44:51,580 --> 01:44:53,080
now that we didn't then.

2378
01:44:54,280 --> 01:44:56,480
But if you look at the totalitarian regimes of the mid

2379
01:44:56,480 --> 01:45:00,580
20th century, they were master users of the mass media of

2380
01:45:00,580 --> 01:45:01,080
the time.

2381
01:45:02,380 --> 01:45:05,480
Stalin and Hitler were the first ones to really make use

2382
01:45:05,480 --> 01:45:08,780
of radio and film and whatnot for propaganda purposes.

2383
01:45:09,680 --> 01:45:12,180
And then, you know, Churchill and Roosevelt caught up and

2384
01:45:12,180 --> 01:45:15,180
learn how to use those, right? But they were behind in the

2385
01:45:15,180 --> 01:45:19,880
beginning, right? And in every, well, the most famous example

2386
01:45:19,880 --> 01:45:22,480
is the printing press, right? It was without no printing press

2387
01:45:22,480 --> 01:45:25,080
there probably not have been, you know, the Reformation and

2388
01:45:25,080 --> 01:45:28,380
someone, right? So in every, you know, in every, you know,

2389
01:45:28,380 --> 01:45:31,880
turn of the wheel, the technology that's available,

2390
01:45:31,980 --> 01:45:35,480
right, becomes one of the key defining elements of what

2391
01:45:35,480 --> 01:45:38,680
happens. And again, what you have to do is you have to

2392
01:45:38,680 --> 01:45:42,180
master that technology so that, you know, you don't wind up

2393
01:45:42,180 --> 01:45:44,980
under the foot of the ones who didn't while you were idle.

2394
01:45:45,980 --> 01:45:48,280
Yeah, technology is the key. I had this conversation with

2395
01:45:48,280 --> 01:45:50,380
someone else, Jason Rizzo-Giorgiani. I don't know if

2396
01:45:50,380 --> 01:45:53,880
you know him, philosopher. And we were basically talking

2397
01:45:53,880 --> 01:45:59,080
about how a tech-based and technology-driven narrative

2398
01:45:59,080 --> 01:46:02,080
has been lacking on the right side of politics for a very,

2399
01:46:02,080 --> 01:46:04,680
very long time. And they lost many different aspects of

2400
01:46:04,680 --> 01:46:07,680
this war, whatever you want to call it, exactly because of

2401
01:46:07,680 --> 01:46:07,880
it.

2402
01:46:08,680 --> 01:46:11,780
Yeah. And why did that happen? I think the biggest reason

2403
01:46:11,780 --> 01:46:16,080
is that the universities are, you know, left-wing zones,

2404
01:46:16,180 --> 01:46:19,780
right? All the people seeing this firsthand and, you know,

2405
01:46:19,780 --> 01:46:22,380
taking advantage of it and then building on it, you know,

2406
01:46:22,380 --> 01:46:25,080
the unfortunately conservatives do not have enough access

2407
01:46:25,080 --> 01:46:28,980
to the universities, which again is terrible because, you

2408
01:46:28,980 --> 01:46:30,880
know, some of their ideas will be right, some of them will

2409
01:46:30,880 --> 01:46:34,280
be wrong. But you need those people looking at technology,

2410
01:46:34,280 --> 01:46:36,880
right? And, you know, like the thing about, you know, this

2411
01:46:36,880 --> 01:46:40,080
whole area of AI ethics and whatnot is that, like, everybody

2412
01:46:40,080 --> 01:46:40,980
is left-wing.

2413
01:46:42,180 --> 01:46:42,580
Yeah.

2414
01:46:43,180 --> 01:46:46,480
Right. It's like, again, you know, people have, like

2415
01:46:46,480 --> 01:46:48,480
journalists and what have asked me, you know, this in my

2416
01:46:48,480 --> 01:46:52,680
life. So what do conservatives think about AI ethics? And

2417
01:46:52,680 --> 01:46:54,780
I actually have a hard time answering this question because

2418
01:46:54,780 --> 01:46:55,580
I don't know.

2419
01:46:55,580 --> 01:46:57,580
They probably don't even know it exists.

2420
01:46:57,880 --> 01:47:00,280
Yeah, exactly. I mean, like, so, you know, I wrote this

2421
01:47:00,280 --> 01:47:03,580
piece in The Spectator, right, that his entire goal was you

2422
01:47:03,580 --> 01:47:06,280
got to, you know, alert conservatives to the fact that

2423
01:47:06,280 --> 01:47:09,080
there is this not happening and they need to get into this

2424
01:47:09,080 --> 01:47:11,680
with their ideas and we need to have a real debate before

2425
01:47:11,680 --> 01:47:14,780
the science is settled. But yeah, that's the problem is

2426
01:47:14,780 --> 01:47:19,280
that the universities have shut out the conservatives, which

2427
01:47:19,280 --> 01:47:22,280
of course, you know, creates an imbalance of power, you know,

2428
01:47:22,280 --> 01:47:24,680
because now the, you know, the people on the left wing can

2429
01:47:24,680 --> 01:47:27,080
use these things and the people on the right wing don't.

2430
01:47:28,480 --> 01:47:32,280
Very true. Very true. Pedro's book is called The Master

2431
01:47:32,280 --> 01:47:34,980
Algorithm, How the Quest for the Ultimate Learning Machine

2432
01:47:34,980 --> 01:47:39,680
Will Remake Our World. It was one of the only two AI books

2433
01:47:39,680 --> 01:47:44,280
on the bookshelf of Xi Jinping, President for Life in China.

2434
01:47:44,280 --> 01:47:48,180
We almost talked for two hours. Thank you so much for this.

2435
01:47:48,180 --> 01:47:51,180
I enjoyed it tremendously. Let me ask you the last question

2436
01:47:51,180 --> 01:47:53,980
I ask all my guests. Actually, let me ask you one question

2437
01:47:53,980 --> 01:47:56,780
before that, that what is next for you and where can our

2438
01:47:56,780 --> 01:47:58,180
audience follow your work?

2439
01:47:59,280 --> 01:48:02,280
Well, what's next for me is more machine learning research.

2440
01:48:02,280 --> 01:48:05,980
I think we are at a very exciting time in AI, where the

2441
01:48:05,980 --> 01:48:08,880
really important things are going to have not happened yet,

2442
01:48:09,180 --> 01:48:12,280
but they're going to happen the next 10, 20 years. So this

2443
01:48:12,280 --> 01:48:15,480
is, you know, I started out in AI, you know, 20 or 30 years

2444
01:48:15,480 --> 01:48:18,180
ago, but this is the time. Everything was just leading up

2445
01:48:18,180 --> 01:48:20,880
to this. So that's one of the things I want to do. The other

2446
01:48:20,880 --> 01:48:24,480
thing I want to do is, you know, write books and essays.

2447
01:48:24,780 --> 01:48:28,680
I think, again, there's a lot to be done in that format and

2448
01:48:28,680 --> 01:48:31,880
for a broader audience than just the AI specialists. So I

2449
01:48:31,880 --> 01:48:34,680
think The Master Algorithm was my first book, but hopefully

2450
01:48:34,680 --> 01:48:38,880
it will by no means be my last. To follow me, the easiest

2451
01:48:38,880 --> 01:48:42,480
thing to do is follow me on Twitter because, you know, what

2452
01:48:42,480 --> 01:48:46,780
I do in these various areas, I usually post there. And of

2453
01:48:46,780 --> 01:48:49,480
course, you know, I will publish things in various places,

2454
01:48:49,480 --> 01:48:52,480
both research and for general audience. So that's the

2455
01:48:52,480 --> 01:48:52,980
other side.

2456
01:48:53,280 --> 01:48:56,380
Excellent. The advancement that you're seeing in AI and machine

2457
01:48:56,380 --> 01:49:01,180
learning. Let me ask you this way. How do you feel about Ray

2458
01:49:01,180 --> 01:49:05,280
Kurzweil's dates with respect to the Singularity 2045 that

2459
01:49:05,280 --> 01:49:06,080
he's talking about?

2460
01:49:06,080 --> 01:49:10,080
I think first of all, it's very hard to predict. The most

2461
01:49:10,080 --> 01:49:12,280
important thing about scientific progress is that it's

2462
01:49:12,280 --> 01:49:16,880
unpredictable. And it happens in jumps. You can have long

2463
01:49:16,880 --> 01:49:19,480
periods of low progress followed by periods of very rapid

2464
01:49:19,480 --> 01:49:23,880
progress. And I don't know when the next so we are on this

2465
01:49:23,880 --> 01:49:26,480
wave right now, and they will plateau at some point, it hasn't

2466
01:49:26,480 --> 01:49:29,280
plateaued yet, but it will inevitably plateau. Where the

2467
01:49:29,280 --> 01:49:32,080
plateau will happen, you can't predict because it depends, you

2468
01:49:32,080 --> 01:49:34,480
know, this is not like some law of physics, it depends on

2469
01:49:34,480 --> 01:49:38,080
what, you know, we the researchers come up with. And I

2470
01:49:38,080 --> 01:49:40,880
don't know how many jumps are needed to get to, you know,

2471
01:49:40,880 --> 01:49:44,480
human level AI, and what the barriers are in between. So

2472
01:49:44,480 --> 01:49:48,280
it's very hard to predict. Now, so anybody who gives you a

2473
01:49:48,280 --> 01:49:51,880
date for when we'll have strong artificial intelligence is

2474
01:49:51,880 --> 01:49:54,480
speculating, right? And I think Ray is speculating, you know,

2475
01:49:54,480 --> 01:49:58,280
with some basis to it. But I think the first or the bit is

2476
01:49:58,280 --> 01:50:01,680
the error bars are enormous. At one extreme, maybe it will

2477
01:50:01,680 --> 01:50:05,280
never happen because the problem is just too hard. Right, I

2478
01:50:05,280 --> 01:50:08,480
know, I'm a scientist, I believe in reductionism, I think

2479
01:50:08,480 --> 01:50:11,280
there is an algorithm, you know, again, part of the empirical

2480
01:50:11,280 --> 01:50:13,480
evidence for the master algorithm is that your brain does

2481
01:50:13,480 --> 01:50:15,880
it. And, you know, and, you know, I should be able to write

2482
01:50:15,880 --> 01:50:19,280
down the program that that your brain is running, I just don't

2483
01:50:19,280 --> 01:50:21,880
know what it is yet, it could be complex or not, right, but it

2484
01:50:21,880 --> 01:50:24,880
might be too complex. And in which case, we will never get to

2485
01:50:24,880 --> 01:50:27,880
it. I don't think that's the case, but it's a possibility. At

2486
01:50:27,880 --> 01:50:31,480
the other end, and more excitingly, maybe, you know, some

2487
01:50:31,480 --> 01:50:34,680
kid in a garage is inventing the master algorithm right now. In

2488
01:50:34,680 --> 01:50:37,280
fact, part of my motivation for writing the book, and I say it

2489
01:50:37,280 --> 01:50:41,080
is, again, you know, following on some of the things that we've

2490
01:50:41,080 --> 01:50:44,280
been talking about, we in the field have focused on too

2491
01:50:44,280 --> 01:50:48,680
narrowly on a few paradigms. And my feeling is that even after

2492
01:50:48,680 --> 01:50:50,680
we've unified them, and we're making very good progress

2493
01:50:50,680 --> 01:50:54,280
towards that, some of the really important ideas will still be

2494
01:50:54,280 --> 01:50:57,480
missing. And actually think someone outside the field is

2495
01:50:57,480 --> 01:50:59,680
more likely to come up with them than someone who's already

2496
01:50:59,680 --> 01:51:02,480
thinking along these tracks. So a non expert, but at the same

2497
01:51:02,480 --> 01:51:04,880
time, they need to know enough. So maybe reading a book like

2498
01:51:04,880 --> 01:51:07,880
the master of them will help. So it could go all the way from

2499
01:51:08,080 --> 01:51:10,880
it's already happening to it will never happen. Even if it's

2500
01:51:10,880 --> 01:51:13,080
already happening, right, if someone had that amazing

2501
01:51:13,080 --> 01:51:15,480
algorithm today, it would still take decades for it to play

2502
01:51:15,480 --> 01:51:20,080
itself out. Right. So where do I think we'll be in, you know,

2503
01:51:20,080 --> 01:51:25,280
2047? I think we will be much farther along than we are today.

2504
01:51:27,280 --> 01:51:33,880
Exactly, exactly where we are is very hard to predict. Are you

2505
01:51:34,880 --> 01:51:35,880
getting this?

2506
01:51:36,880 --> 01:51:37,880
I can't wait for it.

2507
01:51:38,880 --> 01:51:39,880
Yeah.

2508
01:51:40,880 --> 01:51:44,880
This unfortunately, my cell phone. So yeah.

2509
01:51:46,880 --> 01:51:47,880
So sorry.

2510
01:51:47,880 --> 01:51:54,880
So yeah, so, so, so sorry, I lost my turn of the big big

2511
01:51:54,880 --> 01:51:56,480
changes that is going to happen.

2512
01:51:56,480 --> 01:52:00,480
Right. So where are we going to be in 2047? I don't think

2513
01:52:00,480 --> 01:52:04,480
anybody knows for sure. But I think what we want to do is be

2514
01:52:04,480 --> 01:52:08,280
prepared for the range of places where we could be right from

2515
01:52:08,280 --> 01:52:11,680
the most pessimist part of you know, machine learning is all

2516
01:52:11,680 --> 01:52:14,880
about prediction. And actually half the science of prediction

2517
01:52:14,880 --> 01:52:18,480
is knowing what you can't predict, but being prepared for

2518
01:52:18,480 --> 01:52:21,880
all the possible outcomes. And I think this is what we want to

2519
01:52:21,880 --> 01:52:25,880
know when we think about AI in 2030, 2040, etc. Right. We want

2520
01:52:25,880 --> 01:52:28,080
to think about the range of places where it could be and we

2521
01:52:28,080 --> 01:52:30,480
want to be prepared for all those possibilities, not assume

2522
01:52:30,480 --> 01:52:32,880
that there's going to be one scenario and go for that one.

2523
01:52:33,880 --> 01:52:37,680
Excellent. Let me ask you the final question I ask all my

2524
01:52:37,680 --> 01:52:41,680
guests that if you come across an intelligent alien from a

2525
01:52:41,680 --> 01:52:45,480
different civilization, what would you say is the worst thing

2526
01:52:45,480 --> 01:52:48,080
humanity has done? And what would you say is our greatest

2527
01:52:48,080 --> 01:52:48,880
achievement?

2528
01:52:51,880 --> 01:52:55,880
I would say our greatest achievement is the extent to

2529
01:52:55,880 --> 01:53:00,480
which we know nature and the universe, including ourselves.

2530
01:53:01,080 --> 01:53:05,080
Right. If you think if you think of the universe as being made

2531
01:53:05,080 --> 01:53:09,680
of, you know, atoms and, you know, photons, we are the

2532
01:53:09,680 --> 01:53:13,480
completely insignificant. Right. We are specks of dust on a

2533
01:53:13,480 --> 01:53:16,480
speck of dust. But if you think of the universe as being made

2534
01:53:16,480 --> 01:53:20,880
of information, and increasingly, people across science

2535
01:53:20,880 --> 01:53:24,680
see the universe as being made of information. Human beings

2536
01:53:24,680 --> 01:53:28,280
are amazing. There is no bigger concentration of information

2537
01:53:28,280 --> 01:53:32,480
than the one in your brain. And the concentration and evolution

2538
01:53:32,480 --> 01:53:34,880
has been concentrating information in your brain for,

2539
01:53:34,880 --> 01:53:38,280
you know, millions of years, hundreds of millions of years.

2540
01:53:38,280 --> 01:53:41,480
Millions of years, hundreds of millions of years. And once you

2541
01:53:41,480 --> 01:53:44,680
start reading books, right, once you start communicating with

2542
01:53:44,680 --> 01:53:46,880
people, there's more information with your brain. Once you

2543
01:53:46,880 --> 01:53:49,080
start studying nature and writing books about it, there's

2544
01:53:49,080 --> 01:53:53,880
more information. Now, we as a society, right, as a repository

2545
01:53:53,880 --> 01:53:58,480
of information about the universe, right, you know, if

2546
01:53:58,480 --> 01:54:00,880
you put a diagram of the universe, you know, like, you

2547
01:54:00,880 --> 01:54:03,480
know, how much information there is, you know, like, you

2548
01:54:03,480 --> 01:54:06,680
know, Earth would be this Dirac Delta functions, like this

2549
01:54:06,680 --> 01:54:10,480
sharp, super high peak, right? Compare the amount of

2550
01:54:10,480 --> 01:54:13,380
information that exists on Earth, you know, thanks to life,

2551
01:54:13,380 --> 01:54:16,680
but in particular, thanks to humanity, with the amount of

2552
01:54:16,680 --> 01:54:20,280
information that exists on Mars or Venus, there's just no

2553
01:54:20,280 --> 01:54:24,080
comparison, right? None. So that's our biggest achievement,

2554
01:54:24,380 --> 01:54:28,180
right? What is the worst thing that we have done? Well, of

2555
01:54:28,180 --> 01:54:31,180
course, is that in the process of climbing this ladder, we

2556
01:54:31,180 --> 01:54:34,380
have, you know, we have done so much death and destruction,

2557
01:54:34,380 --> 01:54:38,780
right? Like, you know, and, you know, that is often

2558
01:54:38,780 --> 01:54:41,580
exaggerated. You know, people have a much more pessimistic,

2559
01:54:41,580 --> 01:54:43,980
you know, if you read Steven Pinker's books, for example,

2560
01:54:43,980 --> 01:54:46,980
he'll show you why on balance, you know, few people die

2561
01:54:46,980 --> 01:54:50,380
violently today, much fewer than before, nevertheless,

2562
01:54:50,580 --> 01:54:53,880
right? You know, things like, you know, World War One and

2563
01:54:53,880 --> 01:54:57,380
World War Two, and, you know, nuclear weapons are still here,

2564
01:54:57,480 --> 01:55:01,680
right? So far, we've avoided the worst. But again, you can't

2565
01:55:01,680 --> 01:55:04,180
take any of that for granted. So that, I think, is the big

2566
01:55:04,180 --> 01:55:09,880
biggest downside. Excellent. Well, the alien, the alien will

2567
01:55:09,880 --> 01:55:13,080
probably have the same thing to say to us is my guess. They

2568
01:55:13,080 --> 01:55:16,080
will be proud of the same about themselves. Oh, yeah,

2569
01:55:16,180 --> 01:55:18,980
interesting. About their achievements. And you know,

2570
01:55:18,980 --> 01:55:22,680
they're, you know, the things that they, you know, are not

2571
01:55:22,680 --> 01:55:26,880
proud of. Yeah, an interesting variation I heard about the

2572
01:55:26,880 --> 01:55:29,880
alien itself was that the alien could be coming from a

2573
01:55:29,880 --> 01:55:34,080
civilization that is hive mind. So the priorities would

2574
01:55:34,080 --> 01:55:38,380
be different than what we were experiencing individually.

2575
01:55:39,280 --> 01:55:44,680
I actually think hive minds are not as adaptive as what we

2576
01:55:44,680 --> 01:55:49,080
have. Oh, really? Our society is a mess. Because it's a bunch

2577
01:55:49,080 --> 01:55:51,880
of independent agents. And we have, you know, and the cost

2578
01:55:51,880 --> 01:55:54,980
of, you know, dealing with the conflicting interests is very,

2579
01:55:54,980 --> 01:55:58,880
very high. So you know, our society is far from optimal, we

2580
01:55:58,880 --> 01:56:01,480
can make it better, I think, with technology and with being

2581
01:56:01,480 --> 01:56:06,380
more mature. But a hive mind right is actually also it goes

2582
01:56:06,380 --> 01:56:09,080
too far. Right. In machine learning, there's this bias

2583
01:56:09,080 --> 01:56:12,780
variance trade off. Right? I think in human society, we have

2584
01:56:12,780 --> 01:56:16,080
a lot of variance, right? A hive mind has does not have

2585
01:56:16,080 --> 01:56:19,980
enough variance, right? A society that causes more able to

2586
01:56:19,980 --> 01:56:23,480
generate new ideas, and then let the better ideas grow than

2587
01:56:23,480 --> 01:56:26,780
a hive mind is, right? A hive mind is actually not the most

2588
01:56:26,780 --> 01:56:29,680
adaptive. So I think at the end of the day, humans beat the

2589
01:56:29,680 --> 01:56:33,180
board, right? Human society for all its imperfections, right?

2590
01:56:33,280 --> 01:56:36,580
And for all the mess that you know, human life is at the end

2591
01:56:36,580 --> 01:56:40,680
of the day, this is a much more adaptive way to organize

2592
01:56:40,680 --> 01:57:07,680
the species than the board ever could be.

2593
01:57:10,680 --> 01:57:13,680
Transcribed by https://otter.ai

